{
  "hash": "7b4e17493b100efdcc11bb353ab23b6f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Activity39\"\nformat: \n  live-html:\n    theme:\n      light: [lux, theme-light.scss]\n      dark: [superhero, theme-dark.scss] \nengine: knitr\ntoc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Time Series Essentials, install if needed!\nlibrary(feasts)       # Feature extraction & decomposition\nlibrary(fable)        # Forecasting models (ARIMA, ETS, etc.)\nlibrary(fpp3)         # Tidy time series dataseta\nlibrary(astsa)        # Applied statistical TS methods from textbook\nlibrary(tseries)      # Unit root tests & TS diagnostics\nlibrary(tsibbledata)  # Curated TS datasets\nlibrary(quantmod)     # Financial data retrieval\nlibrary(tidyquant)    # Financial analysis in tidyverse\nlibrary(purrr)        # Functional programming for TS pipelines\nlibrary(readr)        # Efficient data import\nlibrary(lubridate)\nlibrary(zoo)\nlibrary(hms)\nlibrary(stringr)\n```\n:::\n\n\n###  Part 1: Parsing Inconsistent Character Dates\n\n::: {.cell}\n```{webr}\nlog_data <- tibble::tibble(\n  user_id = sample(1:100, 10),\n  login_time = c(\"2024-03-05 14:32:11\", \n                 \"2024/03/06 09:10:50\", \n                 \"03-07-2024 12:00:01\", \n                 NA, \n                 \"2024-03-08T15:05:01\", \n                 \"2024.03.09 16:00\", \n                 \"March 10, 2024 17:20\",\n                 \"2024-03-11 18:00:00\", \n                 \"2024-03-12\", \n                 \"20240313\")\n)\n\nglimpse(log_data)\n```\n:::\n\nYour task is to use `parse_date_time()` to convert them into a proper `POSIXct` column. Consider the formats: `ymd HMS`, `mdy HMS`, `B d, Y HM`, etc.\n\n**Key Question:** Which patterns did `lubridate` handle automatically? Which formats needed manual specification?\n\n::: {.cell}\n```{webr}\n# First attempt\nlog_data %>% mutate(login_dt = ymd_hms(login_time))\n```\n:::\n\n::: {.cell}\n```{webr}\nlog_data <- log_data %>% \n  mutate(login_dt = parse_date_time(login_time, orders = c(\"ymd HMS\", \"mdy HMS\", \"dmy HMS\", \"ymd\", \"ymd HM\"))) %>% \n  tidyr::fill(login_dt, .direction=\"downup\")\n\nlog_data\n```\n:::\n\n\n### Part 2: Constructing a Date from Components\n\nSuppose you are given columns: `year = 2022`, `month = 11`, `day = 20`.\n\n**Task:** Combine them into a single date column using `make_date()` or `ymd()`. Here's a more comprehensive data/tibble:\n\n::: {.cell}\n```{webr}\n# Example data with separate components\ncomponent_data <- tibble(\n  year = c(2022, 2023, 2023),\n  month = c(11, 2, 12),\n  day = c(20, 15, 31),\n  event = c(\"A\", \"B\", \"C\")\n)\n\ncomponent_data\n\ncomponent_data_combined <- component_data %>%\n  mutate(\n    full_date = make_date(year, month, day),\n    full_date_alt = ymd(paste(year, month, day, sep = \"-\"))\n  )\n\ncomponent_data_combined\n```\n:::\n\n\nWhy is this useful? Many real-world datasets (especially Kaggle CSVs) separate date parts due to scraping.\n\n\n### Part 3: Real data\n\nYou can download the Air Quality dataset from the UCI repository here: [Air Quality Data Set](https://archive.ics.uci.edu/dataset/360/air+quality) to your working directory for this section. First, we clean the data by parsing dates/times and combining them into a proper datetime column, then convert text-based numbers to numeric values. Next, we aggregate the data into daily averages.\n\n::: {.cell}\n```{webr}\ndat <- read_delim(\"data/AirQualityUCI.csv\", delim = \";\")\n\n# make columns cleaner\nclean_dat <- dat %>% \n  janitor::clean_names() %>% \n  dplyr::select(-x16, -x17) %>% \n  mutate(date = lubridate::dmy(date),\n         time = lubridate::hms(time))\n\nclean_dat <- clean_dat %>%\n  mutate(\n    datetime = date + time, .before = 1\n  )\n\nclean_dat <- clean_dat %>%\n  mutate(across(where(is.character), parse_number))\n\n```\n:::\n\n\n::: {.cell}\n```{webr}\n# aggregate \n# c(\"30 min\", \"1 hour\", \"1 day\", \"1 week\", \"1 quarter\")\nclean_dat %>%\n    mutate(daily = floor_date(datetime, \"1 day\")) %>%\n    group_by(daily) %>%\n    summarise(across(where(is.numeric), \\(x) mean(x, na.rm = TRUE))) -> half\n```\n:::\n\n\n::: {.cell}\n```{webr}\nggplot(half, aes(daily, pt08_s4_no2)) +\n  geom_line(color = \"firebrick\") +\n  labs(title = \"Daily S4_NO2 Concentration\",\n       subtitle = \"Air Quality Data\",\n       x = \"Time\", y = \"S4_NO2\") +\n  scale_x_datetime(date_labels = \"%Y %b %d\") +\n  theme_minimal()\n```\n:::\n\n\n#### Problem Set 1: *Parsing Heterogeneous Timestamps*  \n\n**Data Task:**  \n\n::: {.cell}\n```{webr}\nmessy_logins <- tibble(\n  event_time = c(\"2023-04-05 22:00\", \"2023/04/06 08:30\", \n                \"April 5th 2023, 10pm\", \"2023-04-07\", \"15-04-2023 12:00\")\n)\n```\n:::\n\n1. **Code:** Use `parse_date_time()` with `orders` to handle all formats. Which required explicit format codes? \n\n2. **Real Data (AirQuality):** Column `time` uses `hms()` - what happens if you find entries like \"24:00:00\"? Fix with `str_replace(\"24:\",\"00:\")`.  \n\n3. **Different Period:** Parse `Date` column from 2005 data containing entries like \"15/3/2005\" (day-first).  \n\n<!--\n\n```r\n# data solution\nmessy_logins %>% \n  mutate(event_dt = parse_date_time(\n    event_time, \n    orders = c(\"ymd HM\", \"ymd HMS\", \"Bdy T\", \"dmy HM\")\n  ))\n\n# Real data fix (AirQuality 24:00 edge case)\nclean_dat %>% \n  mutate(time = str_replace(time, \"^24:\", \"00:\")) %>% \n  mutate(time = hms(time))\n\n# Different period (day-first format)\ndf_2005 %>% \n  mutate(date = dmy(date))\n```\n\n-->\n\n\n\n#### Problem Set 2: *Component â†’ Temporal Index*  \n\n**Data Task:**  \n\n::: {.cell}\n```{webr}\nsensor_parts <- tibble(\n  sensor_id = 1:3,\n  yr = c(2023,2023,2023), \n  mth = c(\"Feb\",\"March\",\"April\"), \n  dy = c(28, 15, 1)\n)\n```\n:::\n\n1. **Code:** Use `my()` + `make_date()` to create full dates. Handle text months with `match(mth, month.name)`.  \n\n2. **Real Data (AirQuality):** Create `month_start` using `yearmonth()` + `make_date(year, month, 1)`.  \n\n3. **Different Granularity:** Create quarterly dates from `year` and `quarter` columns using `yearquarter()`.  \n\n<!--\n\n```r\n# data (text months)\nsensor_parts %>% \n  mutate(\n    month_num = match(mth, month.name),\n    full_date = make_date(yr, month_num, dy)\n  )\n\n# Real data (month starts)\nclean_dat %>% \n  mutate(month_start = yearmonth(datetime) %>% make_date())\n\n# Quarterly granularity\nsales_data %>% \n  mutate(qtr = yearquarter(make_date(year, (quarter-1)*3+1, 1))\n```\n\n-->\n\n\n\n#### Problem Set 3: *Gap Imputation*  \n\n**Data Task:**  \n\n::: {.cell}\n```{webr}\ngap_ts <- tsibble(\n  date = ymd(c(\"2023-01-01\", \"2023-01-03\", \"2023-01-04\")),\n  value = c(10, NA, 15), index = date\n)\n```\n:::\n\n1. **Code:** Use `fill_gaps(.full = TRUE)` + `fill(value, .direction = \"down\")`. Why use `.full`? \n\n2. **Real Data (AirQuality):** Convert to `tsibble` with `index = datetime`. Find gaps in CO measurements using `scan_gaps()`.  \n\n3. **Different Frequency:** Resample to daily means using `index_by(date = as_date(datetime))` before imputing.  \n\n<!--\n\n``r\n# data imputation\ngap_ts %>% \n  fill_gaps(.full = TRUE) %>% \n  fill(value, .direction = \"down\")\n\n# Real data gaps (tsibble approach)\nclean_dat %>%\n  as_tsibble(index = datetime) %>% \n  scan_gaps() \n\n# Daily resampling\nclean_dat %>% \n  index_by(date = as_date(datetime)) %>% \n  summarise(co_mean = mean(pt08_s4_no2, na.rm = TRUE)) %>% \n  fill_gaps()\n```\n\n-->\n\n\n### Tsibble Transition \n\n::: {.cell}\n```{webr}\n# Instead of:\nclean_dat %>% group_by(half_hourly = floor_date(datetime, \"1 hour\"))\n\n# Use:\nclean_dat %>% tidyr::drop_na() %>% \n  as_tsibble(index = datetime) %>% \n  index_by(hourly = ~lubridate::floor_date(., \"1 hour\"))\n```\n:::\n\n**Why?** `index_by()` preserves temporal context and enables `fill_gaps()`/`slide()` operations.\n\n<!--\n\n```r\n# Before (naive grouping)\nclean_dat %>% \n  group_by(hour = floor_date(datetime, \"1 hour\"))\n\n# After (temporal-aware)\nclean_dat %>%\n  as_tsibble(index = datetime) %>% \n  index_by(hourly = ~ floor_date(., \"1 hour\")) %>% \n  summarise(co_level = mean(pt08_s4_no2, na.rm = TRUE))\n```\n\n-->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}