{
  "hash": "3801983567f2c1dec5cf56be38a26706",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Activity23\"\nformat: html\nwarning: false\nmessage: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Time Series Essentials, install if needed!\nlibrary(feasts)       # Feature extraction & decomposition\nlibrary(fable)        # Forecasting models (ARIMA, ETS, etc.)\nlibrary(fpp3)         # Tidy time series dataseta\nlibrary(astsa)        # Applied statistical TS methods from textbook\nlibrary(tseries)      # Unit root tests & TS diagnostics\nlibrary(tsibbledata)  # Curated TS datasets\nlibrary(quantmod)     # Financial data retrieval\nlibrary(tidyquant)    # Financial analysis in tidyverse\nlibrary(purrr)        # Functional programming for TS pipelines\nlibrary(readr)        # Efficient data import\n```\n:::\n\n\n\n\n## **1. SARIMA Model Structure**  \n\nA SARIMA$(p,d,q)(P,D,Q)_m$ model combines:  \n- **Regular components**: AR(p), MA(q) terms for short-term patterns  \n- **Seasonal components**: Seasonal AR(P), MA(Q) terms at period $m$  \n- **Differencing**: $d$ regular differences + $D$ seasonal differences  \n\nPrototype equation for monthly data:  \n\n$\\text{Passengers}_t = \\alpha + \\text{Past Values}_{t-1,t-12} + \\text{Errors}_{t-1,t-12} + \\varepsilon_t$  \n\n\n## **2. Strategic Differencing**  \n\n**Key Principle**: Use minimal differencing to stabilize mean/variance  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# AirPassengers dataset\nap_ts <- tsibble::as_tsibble(AirPassengers) %>% \n  index_by(Date = yearmonth(index)) %>% \n  rename(Passengers = value)\n\n# Automated selection\nap_ts %>% \n  features(Passengers, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  kpss_stat kpss_pvalue ndiffs nsdiffs\n      <dbl>       <dbl>  <int>   <int>\n1      2.98        0.01      1       0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visual check\nap_ts %>%\n  gg_tsdisplay(difference(log(Passengers), lag = 12))\n```\n\n::: {.cell-output-display}\n![](activity23_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n**Insight**: Seasonal differencing (lag=12) removes yearly patterns while preserving monthly trends  \n\n\n## **3. Model Building**  \n\n### **3.1 Candidate Models**  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- ap_ts %>%\n  model(\n    Auto = ARIMA(log(Passengers)),\n    Manual1 = ARIMA(log(Passengers) ~ pdq(1,1,1) + PDQ(0,1,1, period=12)),\n    Manual2 = ARIMA(log(Passengers) ~ pdq(2,1,0) + PDQ(1,1,0, period=12))\n  )\n\nglance(models) %>% arrange(AICc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  .model   sigma2 log_lik   AIC  AICc   BIC ar_roots   ma_roots  \n  <chr>     <dbl>   <dbl> <dbl> <dbl> <dbl> <list>     <list>    \n1 Auto    0.00132    250. -489. -489. -475. <cpl [2]>  <cpl [12]>\n2 Manual1 0.00137    245. -482. -482. -470. <cpl [1]>  <cpl [13]>\n3 Manual2 0.00148    241. -474. -473. -462. <cpl [14]> <cpl [0]> \n```\n\n\n:::\n:::\n\n\n\n### **3.2 Coefficient Check**  \n\nFor top model:  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model <- models %>% \n  select(Manual2)\n\ntidy(models) %>% \n  filter(.model == \"Manual2\") %>% \n  mutate(\n    t_stat = estimate / std.error,\n    p_value = 2*pnorm(-abs(t_stat)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  .model  term  estimate std.error statistic      p.value t_stat       p_value\n  <chr>   <chr>    <dbl>     <dbl>     <dbl>        <dbl>  <dbl>         <dbl>\n1 Manual2 ar1    -0.406     0.0876    -4.63  0.00000866   -4.63  0.00000364   \n2 Manual2 ar2    -0.0799    0.0876    -0.913 0.363        -0.913 0.361        \n3 Manual2 sar1   -0.472     0.0806    -5.86  0.0000000353 -5.86  0.00000000460\n```\n\n\n:::\n:::\n\n\n\n\n## **4. Model Refinement Cycle**  \n\n1. Start with automatic differencing  \n2. Compare multiple model specifications  \n3. Validate residuals systematically  \n4. Iterate using PACF patterns  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Final refinement example\nap_ts %>%\n  model(\n    Best = ARIMA(log(Passengers) ~ pdq(1,1,1) + PDQ(0,1,1, period=12))\n  ) %>% \n  report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Passengers \nModel: ARIMA(1,1,1)(0,1,1)[12] \nTransformation: log(Passengers) \n\nCoefficients:\n         ar1      ma1     sma1\n      0.1960  -0.5784  -0.5643\ns.e.  0.2475   0.2132   0.0747\n\nsigma^2 estimated as 0.001375:  log likelihood=244.95\nAIC=-481.9   AICc=-481.58   BIC=-470.4\n```\n\n\n:::\n:::\n\n\n\n\n## **Lab Activity**\n\n## **1. Data Preparation**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpp3)\naccidents <- as_tsibble(USAccDeaths) %>% \n  rename(Deaths = value) %>% \n  mutate(Month = yearmonth(index))\n```\n:::\n\n\n\n**Q1:** Examine the seasonal patterns using `gg_season()`. What type of seasonality dominates this series?  \n\n\n## **2. Model Specification**\n\nFit SARIMA(1,1,1)(0,1,1)₁₂ model with maximum likelihood estimation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsarima_fit <- accidents %>%\n  model(SARIMA = ARIMA(Deaths ~ pdq(1,1,1) + PDQ(0,1,1, period = 12),\n                      stepwise = FALSE, approximation = FALSE))\n```\n:::\n\n\n\n**Q2:** Interpret the model structure:  \n\na) What does the (1,1,1) non-seasonal component represent?  \n\nb) Why do we use PDQ(0,1,1) for seasonal terms?  \n\n\n\n## **3. Parameter Estimation**\n\n**Task:** Extract and interpret coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(sarima_fit) %>% \n  mutate(signif = ifelse(p.value < 0.05, \"***\", \"\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n  .model term  estimate std.error statistic p.value signif\n  <chr>  <chr>    <dbl>     <dbl>     <dbl>   <dbl> <chr> \n1 SARIMA ar1     0.0979     0.311     0.315 0.754   \"\"    \n2 SARIMA ma1    -0.511      0.274    -1.87  0.0668  \"\"    \n3 SARIMA sma1   -0.544      0.178    -3.05  0.00344 \"***\" \n```\n\n\n:::\n:::\n\n\n\n**Q3:** Which coefficients are statistically significant (α=0.05)? What does the MA(1) coefficient suggest?  \n\n## **4. Residual Diagnostics**\n\n### **4.1 Visual Analysis**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsarima_fit %>% \n  augment() %>% \n  ACF(.innov) %>% \n  autoplot() +\n  geom_hline(yintercept = c(-1,1)*1.96/sqrt(nrow(accidents)), linetype=2)\n```\n\n::: {.cell-output-display}\n![](activity23_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n**Q4:** Do residuals show concerning autocorrelation patterns? Justify your answer.  \n\n### **4.2 Formal Tests**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(sarima_fit) %>% \n  features(.innov, list(\n    LB_test = ~ljung_box(.x, lag=24, dof=4)\n  ))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .model LB_test_lb_stat LB_test_lb_pvalue\n  <chr>            <dbl>             <dbl>\n1 SARIMA            25.9             0.170\n```\n\n\n:::\n:::\n\n\n\n**Q5:** Interpret the Ljung-Box (LB) and ARCH test results: \n\na) Can we maintain the white noise assumption? \n\nb) Is there evidence of volatility clustering?  \n\n**A5:**  \n\n\n## **5. Model Validation**\n\n**Task:** Check specification robustness\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccidents %>% \n  model(\n    SARIMA_CSS = ARIMA(Deaths ~ pdq(1,1,1) + PDQ(0,1,1), \n                      method = \"CSS-ML\"),\n    AutoSARIMA = ARIMA(Deaths)\n  ) %>% \n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 8\n  .model      sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots  \n  <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl> <list>    <list>    \n1 SARIMA_CSS 104809.   -425.  859.  860.  867. <cpl [1]> <cpl [13]>\n2 AutoSARIMA 102860.   -425.  857.  857.  863. <cpl [0]> <cpl [13]>\n```\n\n\n:::\n:::\n\n\n\n**Q6:** Compare AICc values across estimation methods. Does our original model remain preferred?  \n",
    "supporting": [
      "activity23_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}