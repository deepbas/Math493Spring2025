[
  {
    "objectID": "RandRStudio.html",
    "href": "RandRStudio.html",
    "title": "Setting Up R and RStudio",
    "section": "",
    "text": "This lab tutorial will guide you through installing R and RStudio on your computer, managing packages, and loading data into R. This setup is essential for performing all kinds of data analysis tasks in R.",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "RandRStudio.html#on-windows",
    "href": "RandRStudio.html#on-windows",
    "title": "Setting Up R and RStudio",
    "section": "On Windows",
    "text": "On Windows\n\n\nDownload R:\n\nVisit CRAN and download the latest version of R for Windows.\nRun the downloaded .exe file and follow the installation instructions.\n\n\n\n\n\nCRAN mirror\nDownload Latest R\nExecutable File in Downloads Folder\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload RStudio:\n\nGo to the RStudio Download page and download the free version of RStudio Desktop for Windows.\nInstall it by running the downloaded .exe file.\n\n\n\n\n\nDownload Latest RStudio\nExecutable File in Downloads Folder",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "RandRStudio.html#on-mac",
    "href": "RandRStudio.html#on-mac",
    "title": "Setting Up R and RStudio",
    "section": "On Mac",
    "text": "On Mac\n\n\nDownload R:\n\nNavigate to CRAN and choose the version for macOS.\nOpen the downloaded .pkg file to start the installation.\n\n\n\n\n\nCRAN mirror\nDownload Latest R\nExecutable File in Downloads Folder\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownload RStudio:\n\nVisit RStudio Download page and select RStudio for macOS.\nOpen the downloaded .dmg file and drag RStudio to your Applications folder.\n\n\n\n\n\nDownload Latest RStudio\nExecutable File in Downloads Folder",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "RandRStudio.html#rmarkdown-interface-components",
    "href": "RandRStudio.html#rmarkdown-interface-components",
    "title": "Setting Up R and RStudio",
    "section": "RMarkdown Interface Components",
    "text": "RMarkdown Interface Components\nRStudio’s interface for working with RMarkdown documents contains four key panes:\n\nSource Editor (Left Pane)\nPrimary workspace for writing .Rmd files with integrated code chunks and narrative text. Features syntax highlighting, chunk controls (run individual code blocks), and document structure navigation.\n\nEnvironment/History (Top-Right Pane)\n\n\nEnvironment: Lists active datasets, variables, and functions\n\n\nHistory: Tracks all executed commands\n\n\nConnections: Manages database links (when used)\n\n\nGit: Version control integration (when configured)\n\n\n\nConsole/Terminal (Bottom-Left Pane)\n\n\nConsole: Direct R code execution environment showing raw output\n\n\nTerminal: System-level command line access\n\n\nBackground Jobs: Monitor long-running processes\n\n\n\nViewer/Help (Bottom-Right Pane)\n\n\nViewer: Preview rendered documents (HTML/PDF/Word)\n\n\nHelp: Access function documentation\n\n\nFiles: Navigate project directory structure\n\n\nPlots: Display graphical output\n\n\nPackages: Manage installed R packages",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "RandRStudio.html#understanding-r-markdown-documents",
    "href": "RandRStudio.html#understanding-r-markdown-documents",
    "title": "Setting Up R and RStudio",
    "section": "Understanding R Markdown Documents",
    "text": "Understanding R Markdown Documents\nR Markdown allows you to integrate code and text in the same document, facilitating reproducible research and interactive data analysis reports. Here’s a quick overview:",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "RandRStudio.html#managing-.rmd-files-and-setting-the-working-directory",
    "href": "RandRStudio.html#managing-.rmd-files-and-setting-the-working-directory",
    "title": "Setting Up R and RStudio",
    "section": "Managing .Rmd Files and Setting the Working Directory",
    "text": "Managing .Rmd Files and Setting the Working Directory\nDownloading and Organizing .Rmd Files\nTo effectively manage your course materials in RStudio, follow these steps:\n\n\nDownload .Rmd File:\n\nNavigate to MyClasses and locate the desired .Rmd file for your course.\nDownload the file to your local machine.\n\n\n\nCreate a Course Folder:\n\nCreate a new folder named math493 in a suitable location on your computer or within your OneDrive account.\n\n\n\nOrganize Files:\n\nMove the downloaded .Rmd file into the math493 folder to keep your course materials organized.",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "RandRStudio.html#text-formatting-in-r-markdown",
    "href": "RandRStudio.html#text-formatting-in-r-markdown",
    "title": "Setting Up R and RStudio",
    "section": "Text Formatting in R Markdown",
    "text": "Text Formatting in R Markdown\nMarkdown is a lightweight markup language that allows you to add formatting elements using plain text. Here’s how you can use different Markdown syntax to format text within your R Markdown documents.\nHeaders\nCreate headers by prefixing the text with one or more hash symbols (#). The number of # symbols before the text determines the level of the header.\n# Header 1\n## Header 2\n### Header 3\nLists\nYou can create bulleted or numbered lists:\n\n\nUnordered Lists (bullets):\n\n- Item 1\n- Item 2\n  - Subitem 1\n  - Subitem 2\n\n\nOrdered Lists:\n\n1. First item\n2. Second item\n   1. Subitem 1\n   2. Subitem 2\nLinks\nInclude hyperlinks using the format [link text](URL):\n[Visit Canvas](https://salisbury.instructure.com/courses/71470)\nEmphasis\nItalicize or bold text using asterisks or underscores:\n\n\nItalic: *Italic* or _Italic_\n\n\nBold: **Bold** or __Bold__\n\nImages\nEmbed images using the format ![Alt text](image_URL \"Optional Title\"):\n![Example Image](path_to_image.jpg \"This is an image\")\nBlockquotes\nCreate blockquotes with &gt;:\n&gt; This is a blockquote.\nCode\nDisplay inline code with backticks and code blocks with triple backticks:\n\nInline code: `code`\n\nCode block:\n\n\n\n\n\n\n\n\n\n\n\n[1] -5\n\n\nHorizontal Rules\nInsert a horizontal line with three or more hyphens:\n---\nCombining Markdown with R Code\nMarkdown can be interleaved with executable R code chunks:\n\n# Calculate the sum of numbers from 1 to 10\nsum(1:10)\n\n[1] 55",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "RandRStudio.html#r-code-chunk-options",
    "href": "RandRStudio.html#r-code-chunk-options",
    "title": "Setting Up R and RStudio",
    "section": "R Code Chunk Options",
    "text": "R Code Chunk Options\nBefore diving into data loading, let’s understand important R chunk options:\n\n\neval: Use eval=FALSE to prevent code from running but still display it.\n\necho: With echo=FALSE, the code is run but not displayed in the document.\n\ninclude: include=FALSE runs the code and hides both the code and output from the final output.\n\nExample to demonstrate:\n\n\n\n\n\n\n\n\n\n# This code will run, and both code and output will appear in the document.\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nSetting the Working Directory\nIt’s crucial to set the working directory in R to the folder where your course files are stored. This can be done in several ways:\n\n\nUsing RStudio:\n\nIn RStudio, you can set the working directory through the menu by navigating to Session &gt; Set Working Directory &gt; Choose Directory... and selecting your math493 folder.\n\n\n\nUsing R Script:\n\nYou can also set the working directory directly in an R script using the setwd() function. Place the following line of code at the beginning of your script:\n\nsetwd(\"~/math493\")\nReplace \"~/math493\" with the actual path to your course folder.\n\n\nUsing R Console:\n\nYou can manually set the working directory in the R console with the same command:\n\nsetwd(\"~/math493\")",
    "crumbs": [
      "Tutorials",
      "R and RStudio"
    ]
  },
  {
    "objectID": "activity14.html",
    "href": "activity14.html",
    "title": "Activity14",
    "section": "",
    "text": "Improve seasonal forecasting models using:\n\nAutoregressive components via lagged variables (\\(y_{t-1}, y_{t-2}\\))\n\nMultivariate diagnostics using correlation matrices and pairwise plots\n\n\nLagged variables capture serial dependence: \\(E[y_t | y_{t-1}]\\)\n\nFourier terms model periodic patterns: \\(S_t = \\sum_{k=1}^K [\\alpha_k\\sin(2\\pi kt/m) + \\beta_k\\cos(2\\pi kt/m)]\\)\n\nMulticollinearity detection via pairwise correlations (|r| &gt; 0.8 indicates potential issues)\n\n\n\n\n\nExtend the NYSE example by including lagged volume values.\nVisualize relationships between the current volume, its lagged values, and Fourier terms.\n\n\n\n\n\n\n\n# Get NYSE Composite index data (volume) from tidyquant and convert to tsibble\nnyse &lt;- tq_get(\"^NYA\", from = \"2021-01-01\", to = Sys.Date(), get = \"stock.prices\") %&gt;% \n  as_tsibble(index = date) %&gt;% \n  fill_gaps() %&gt;% \n  mutate(volume = na.approx(volume),\n         adjusted = na.approx(adjusted),\n         logVolume = log(volume)) %&gt;% \n  select(date, volume, logVolume, adjusted)\n\n# Plot volume series\nnyse %&gt;% \n  autoplot(volume) +\n  labs(title = \"NYSE Composite Index Volume\", y = \"Volume\", x = \"Date\")\n\n\n\n\n\n\n\n# Create extended lag structure (4 weeks backward)\nnyse &lt;- nyse %&gt;% \n  mutate(volume_lag1 = lag(volume, 1),\n         volume_lag2 = lag(volume, 2)) %&gt;% \n  drop_na()\n\n\n\n\n\n# Fit a model including Fourier terms and lagged volume predictors\nnyse_lag_model &lt;- nyse %&gt;% \n  model(\n    LagModel = TSLM(volume ~ fourier(K = 2) + volume_lag1 + volume_lag2)\n  )\n\nreport(nyse_lag_model) %&gt;% \nglance(nyse_lag_model) %&gt;% knitr::kable()\n\nSeries: volume \nModel: TSLM \n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.017e+09 -2.322e+08 -4.401e+07  1.557e+08  4.654e+09 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.120e+08  7.290e+07  12.511  &lt; 2e-16 ***\nfourier(K = 2)C1_7  8.649e+07  2.048e+07   4.223 2.55e-05 ***\nfourier(K = 2)S1_7  1.040e+07  2.049e+07   0.508   0.6118    \nfourier(K = 2)C2_7 -3.684e+07  2.044e+07  -1.803   0.0716 .  \nfourier(K = 2)S2_7  5.220e+07  2.042e+07   2.556   0.0107 *  \nvolume_lag1         7.802e-01  2.549e-02  30.605  &lt; 2e-16 ***\nvolume_lag2         6.441e-03  2.549e-02   0.253   0.8006    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 566900000 on 1539 degrees of freedom\nMultiple R-squared: 0.6194, Adjusted R-squared: 0.6179\nF-statistic: 417.4 on 6 and 1539 DF, p-value: &lt; 2.22e-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\nLagModel\n0.6193707\n0.6178867\n3.213785e+17\n417.384\n0\n7\n-33350.88\n62330.4\n62330.5\n62373.15\n3.234206e+17\n4.946016e+20\n1539\n7\n\n\n\n\n# Residual diagnostics\nnyse_lag_model %&gt;% \n  gg_tsresiduals() +\n  labs(title = \"Residual Diagnostics: NYSE Lag Model\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Prepare a multivariate dataset for ggpairs visualization\nnyse_multi &lt;- nyse %&gt;% \n  mutate(trend = row_number()) %&gt;% \n  select(trend, volume, volume_lag1, volume_lag2) %&gt;% \n  as_tibble()\n\nlibrary(GGally)\nggpairs(nyse_multi, \n        lower = list(continuous = wrap(\"smooth\", alpha = 0.3)),\n        title = \"Multivariate Relationships with Trend Component\")\n\n\n\n\n\n\n\n\n\n\n\n\nTask: Experiment with including additional lags (e.g., lag 3 or lag 4) and higher order Fourier Harmonics in your model. Use ggpairs to visualize how these lagged variables relate to the current volume and discuss potential multicollinearity issues.\n\nSolution:\n\nModel Comparison:\n\n\nnyse &lt;- nyse %&gt;% \n  mutate(volume_lag3 = lag(volume, 3),\n         volume_lag4 = lag(volume, 4)) %&gt;% \n  drop_na()\n\nnyse %&gt;% \n  model(\n    Base = TSLM(volume ~ fourier(K = 2)),\n    Extended = TSLM(volume ~ fourier(K = 3) + volume_lag1 + volume_lag2),\n    Extended1 = TSLM(volume ~ fourier(K = 3) + volume_lag1 + volume_lag2 + volume_lag3 + volume_lag4)\n  ) %&gt;% \n  glance() %&gt;%  # Compare AIC/BIC\n  select(.model, adj_r_squared, AIC, AICc, BIC)\n\n# A tibble: 3 × 5\n  .model    adj_r_squared    AIC   AICc    BIC\n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Base            0.00569 63641. 63641. 63673.\n2 Extended        0.621   62159. 62159. 62213.\n3 Extended1       0.627   62136. 62136. 62200.\n\n# Fit the better mode\nnyse_final &lt;- nyse %&gt;% \n  model(\n    BestModel = TSLM(volume ~ fourier(K = 3) + volume_lag1 + volume_lag2 + volume_lag3 + volume_lag4)\n  )\n  \nnyse_final %&gt;% gg_tsresiduals()\n\n\n\n\n\n\n\nnyse_final %&gt;% \n  augment() %&gt;% \n  ggplot(aes(x = .fitted, y = volume)) + \n  geom_hex() +  # Density-aware plotting\n  geom_abline(slope = 1) +\n  labs(title = \"Actual vs Fitted Values with Density Gradient\")",
    "crumbs": [
      "Week 4",
      "Activity 14"
    ]
  },
  {
    "objectID": "activity14.html#regression-with-lagged-variables-multivariate-eda",
    "href": "activity14.html#regression-with-lagged-variables-multivariate-eda",
    "title": "Activity14",
    "section": "",
    "text": "Improve seasonal forecasting models using:\n\nAutoregressive components via lagged variables (\\(y_{t-1}, y_{t-2}\\))\n\nMultivariate diagnostics using correlation matrices and pairwise plots\n\n\nLagged variables capture serial dependence: \\(E[y_t | y_{t-1}]\\)\n\nFourier terms model periodic patterns: \\(S_t = \\sum_{k=1}^K [\\alpha_k\\sin(2\\pi kt/m) + \\beta_k\\cos(2\\pi kt/m)]\\)\n\nMulticollinearity detection via pairwise correlations (|r| &gt; 0.8 indicates potential issues)\n\n\n\n\n\nExtend the NYSE example by including lagged volume values.\nVisualize relationships between the current volume, its lagged values, and Fourier terms.\n\n\n\n\n\n\n\n# Get NYSE Composite index data (volume) from tidyquant and convert to tsibble\nnyse &lt;- tq_get(\"^NYA\", from = \"2021-01-01\", to = Sys.Date(), get = \"stock.prices\") %&gt;% \n  as_tsibble(index = date) %&gt;% \n  fill_gaps() %&gt;% \n  mutate(volume = na.approx(volume),\n         adjusted = na.approx(adjusted),\n         logVolume = log(volume)) %&gt;% \n  select(date, volume, logVolume, adjusted)\n\n# Plot volume series\nnyse %&gt;% \n  autoplot(volume) +\n  labs(title = \"NYSE Composite Index Volume\", y = \"Volume\", x = \"Date\")\n\n\n\n\n\n\n\n# Create extended lag structure (4 weeks backward)\nnyse &lt;- nyse %&gt;% \n  mutate(volume_lag1 = lag(volume, 1),\n         volume_lag2 = lag(volume, 2)) %&gt;% \n  drop_na()\n\n\n\n\n\n# Fit a model including Fourier terms and lagged volume predictors\nnyse_lag_model &lt;- nyse %&gt;% \n  model(\n    LagModel = TSLM(volume ~ fourier(K = 2) + volume_lag1 + volume_lag2)\n  )\n\nreport(nyse_lag_model) %&gt;% \nglance(nyse_lag_model) %&gt;% knitr::kable()\n\nSeries: volume \nModel: TSLM \n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.017e+09 -2.322e+08 -4.401e+07  1.557e+08  4.654e+09 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         9.120e+08  7.290e+07  12.511  &lt; 2e-16 ***\nfourier(K = 2)C1_7  8.649e+07  2.048e+07   4.223 2.55e-05 ***\nfourier(K = 2)S1_7  1.040e+07  2.049e+07   0.508   0.6118    \nfourier(K = 2)C2_7 -3.684e+07  2.044e+07  -1.803   0.0716 .  \nfourier(K = 2)S2_7  5.220e+07  2.042e+07   2.556   0.0107 *  \nvolume_lag1         7.802e-01  2.549e-02  30.605  &lt; 2e-16 ***\nvolume_lag2         6.441e-03  2.549e-02   0.253   0.8006    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 566900000 on 1539 degrees of freedom\nMultiple R-squared: 0.6194, Adjusted R-squared: 0.6179\nF-statistic: 417.4 on 6 and 1539 DF, p-value: &lt; 2.22e-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\nLagModel\n0.6193707\n0.6178867\n3.213785e+17\n417.384\n0\n7\n-33350.88\n62330.4\n62330.5\n62373.15\n3.234206e+17\n4.946016e+20\n1539\n7\n\n\n\n\n# Residual diagnostics\nnyse_lag_model %&gt;% \n  gg_tsresiduals() +\n  labs(title = \"Residual Diagnostics: NYSE Lag Model\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Prepare a multivariate dataset for ggpairs visualization\nnyse_multi &lt;- nyse %&gt;% \n  mutate(trend = row_number()) %&gt;% \n  select(trend, volume, volume_lag1, volume_lag2) %&gt;% \n  as_tibble()\n\nlibrary(GGally)\nggpairs(nyse_multi, \n        lower = list(continuous = wrap(\"smooth\", alpha = 0.3)),\n        title = \"Multivariate Relationships with Trend Component\")\n\n\n\n\n\n\n\n\n\n\n\n\nTask: Experiment with including additional lags (e.g., lag 3 or lag 4) and higher order Fourier Harmonics in your model. Use ggpairs to visualize how these lagged variables relate to the current volume and discuss potential multicollinearity issues.\n\nSolution:\n\nModel Comparison:\n\n\nnyse &lt;- nyse %&gt;% \n  mutate(volume_lag3 = lag(volume, 3),\n         volume_lag4 = lag(volume, 4)) %&gt;% \n  drop_na()\n\nnyse %&gt;% \n  model(\n    Base = TSLM(volume ~ fourier(K = 2)),\n    Extended = TSLM(volume ~ fourier(K = 3) + volume_lag1 + volume_lag2),\n    Extended1 = TSLM(volume ~ fourier(K = 3) + volume_lag1 + volume_lag2 + volume_lag3 + volume_lag4)\n  ) %&gt;% \n  glance() %&gt;%  # Compare AIC/BIC\n  select(.model, adj_r_squared, AIC, AICc, BIC)\n\n# A tibble: 3 × 5\n  .model    adj_r_squared    AIC   AICc    BIC\n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Base            0.00569 63641. 63641. 63673.\n2 Extended        0.621   62159. 62159. 62213.\n3 Extended1       0.627   62136. 62136. 62200.\n\n# Fit the better mode\nnyse_final &lt;- nyse %&gt;% \n  model(\n    BestModel = TSLM(volume ~ fourier(K = 3) + volume_lag1 + volume_lag2 + volume_lag3 + volume_lag4)\n  )\n  \nnyse_final %&gt;% gg_tsresiduals()\n\n\n\n\n\n\n\nnyse_final %&gt;% \n  augment() %&gt;% \n  ggplot(aes(x = .fitted, y = volume)) + \n  geom_hex() +  # Density-aware plotting\n  geom_abline(slope = 1) +\n  labs(title = \"Actual vs Fitted Values with Density Gradient\")",
    "crumbs": [
      "Week 4",
      "Activity 14"
    ]
  },
  {
    "objectID": "activity29.html",
    "href": "activity29.html",
    "title": "Activity29",
    "section": "",
    "text": "Why State-Space? ARIMA models directly relate past observations to present values. State-space models add a hidden layer that evolves independently:\nSimple Weather Example\n\nObserved: Daily temperature (\\(y_t\\))\n\nHidden State: Atmospheric pressure system (\\(x_t\\))\n\nRelationship:\nPressure evolves: \\(x_t = 0.9x_{t-1} + \\text{weather\\_disturbance}\\)\nWe observe: \\(y_t = x_t + \\text{thermometer\\_error}\\)\n\nModel Equations\nState Transition:\n\\[x_t = \\phi x_{t-1} + w_t \\quad (\\text{Memory factor } \\phi, \\text{ system noise } w_t)\\]\nObservation Process:\n\\[y_t = x_t + v_t \\quad (\\text{Measurement noise } v_t)\\]\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nLevel: Long-term employment capacity\n\nTrend: Damped growth pattern (Ad = additive damped)\n\nNoise: Irregular fluctuations (\\(v_t\\) in our equations)\n\nState Transition Equations:\n\nLevel:\n\\[\nl_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\quad \\Rightarrow \\quad l_t = l_{t-1} + 0.98b_{t-1} + 0.9999\\epsilon_t\n\\]\nDamped Trend:\n\\[\nb_t = \\phi b_{t-1} + \\beta \\epsilon_t \\quad \\Rightarrow \\quad b_t = 0.98b_{t-1} + 0.0162\\epsilon_t\n\\]\n\nObservation Equation:\n\\[\ny_t = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\quad \\Rightarrow \\quad y_t = l_{t-1} + 0.98b_{t-1} + \\epsilon_t\n\\]\n\n\n\nStep 1 - Model Setup\n\n\n\n\n\n\n\n\nStep 2 - Extract States\n\n\n\n\n\n\n\n\nStep 3 - Equation Translation\nBased on the ETS(A,Ad,A) model output with parameters:\n\nα = 0.6488 (level smoothing)\n\nβ = 0.1251 (trend smoothing)\n\nφ = 0.9794 (damping factor)\n\nγ = 0.3298 (seasonal smoothing)\n\n\n\n\nLevel:\n\\(l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t\\)\n\\(\\Rightarrow l_t = l_{t-1} + 0.9794b_{t-1} + 0.6488\\epsilon_t\\)\nDamped Trend:\n\\(b_t = \\phi b_{t-1} + \\beta \\epsilon_t'\\)\n\\(\\Rightarrow b_t = 0.9794b_{t-1} + 0.1251\\epsilon_t'\\)\nSeasonal Component:\n\\(s_t = s_{t-m} + \\gamma \\epsilon_t''\\)\n\\(\\Rightarrow s_t = s_{t-12} + 0.3298\\epsilon_t''\\) (monthly seasonality: \\(m = 12\\))\n\n\n\n\n\\(y_t = l_{t-1} + \\phi b_{t-1} + s_{t-m} + v_t\\)\n\\(\\Rightarrow y_t = l_{t-1} + 0.9794b_{t-1} + s_{t-12} + v_t\\)\n\n\n\n\nProblem 1: Fit damped trend model to US_employment\n\n\n\n\n\n\n\n\nProblem 2: Perform a residual diagnostics vs ARIMA",
    "crumbs": [
      "Week 8",
      "Activity 29"
    ]
  },
  {
    "objectID": "activity29.html#introduction-to-state-space-models",
    "href": "activity29.html#introduction-to-state-space-models",
    "title": "Activity29",
    "section": "",
    "text": "Why State-Space? ARIMA models directly relate past observations to present values. State-space models add a hidden layer that evolves independently:\nSimple Weather Example\n\nObserved: Daily temperature (\\(y_t\\))\n\nHidden State: Atmospheric pressure system (\\(x_t\\))\n\nRelationship:\nPressure evolves: \\(x_t = 0.9x_{t-1} + \\text{weather\\_disturbance}\\)\nWe observe: \\(y_t = x_t + \\text{thermometer\\_error}\\)\n\nModel Equations\nState Transition:\n\\[x_t = \\phi x_{t-1} + w_t \\quad (\\text{Memory factor } \\phi, \\text{ system noise } w_t)\\]\nObservation Process:\n\\[y_t = x_t + v_t \\quad (\\text{Measurement noise } v_t)\\]\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nLevel: Long-term employment capacity\n\nTrend: Damped growth pattern (Ad = additive damped)\n\nNoise: Irregular fluctuations (\\(v_t\\) in our equations)\n\nState Transition Equations:\n\nLevel:\n\\[\nl_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\quad \\Rightarrow \\quad l_t = l_{t-1} + 0.98b_{t-1} + 0.9999\\epsilon_t\n\\]\nDamped Trend:\n\\[\nb_t = \\phi b_{t-1} + \\beta \\epsilon_t \\quad \\Rightarrow \\quad b_t = 0.98b_{t-1} + 0.0162\\epsilon_t\n\\]\n\nObservation Equation:\n\\[\ny_t = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\quad \\Rightarrow \\quad y_t = l_{t-1} + 0.98b_{t-1} + \\epsilon_t\n\\]\n\n\n\nStep 1 - Model Setup\n\n\n\n\n\n\n\n\nStep 2 - Extract States\n\n\n\n\n\n\n\n\nStep 3 - Equation Translation\nBased on the ETS(A,Ad,A) model output with parameters:\n\nα = 0.6488 (level smoothing)\n\nβ = 0.1251 (trend smoothing)\n\nφ = 0.9794 (damping factor)\n\nγ = 0.3298 (seasonal smoothing)\n\n\n\n\nLevel:\n\\(l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t\\)\n\\(\\Rightarrow l_t = l_{t-1} + 0.9794b_{t-1} + 0.6488\\epsilon_t\\)\nDamped Trend:\n\\(b_t = \\phi b_{t-1} + \\beta \\epsilon_t'\\)\n\\(\\Rightarrow b_t = 0.9794b_{t-1} + 0.1251\\epsilon_t'\\)\nSeasonal Component:\n\\(s_t = s_{t-m} + \\gamma \\epsilon_t''\\)\n\\(\\Rightarrow s_t = s_{t-12} + 0.3298\\epsilon_t''\\) (monthly seasonality: \\(m = 12\\))\n\n\n\n\n\\(y_t = l_{t-1} + \\phi b_{t-1} + s_{t-m} + v_t\\)\n\\(\\Rightarrow y_t = l_{t-1} + 0.9794b_{t-1} + s_{t-12} + v_t\\)\n\n\n\n\nProblem 1: Fit damped trend model to US_employment\n\n\n\n\n\n\n\n\nProblem 2: Perform a residual diagnostics vs ARIMA",
    "crumbs": [
      "Week 8",
      "Activity 29"
    ]
  },
  {
    "objectID": "activity3.html",
    "href": "activity3.html",
    "title": "Activity3",
    "section": "",
    "text": "Autocorrelation, also known as serial correlation, measures the correlation of a time series with its own past and future values. Mathematically, the autocorrelation function (ACF) at lag \\(k\\) for a time series \\(\\{X_t\\}\\) is defined as:\n\\[\n\\rho_k = \\frac{\\text{Cov}(X_t, X_{t-k})}{\\sqrt{\\text{Var}(X_t) \\text{Var}(X_{t-k})}}\n\\]\nwhere \\(\\rho_k\\) measures the linear relationship between values \\(k\\) time periods apart.\n\nPositive autocorrelation at lag \\(k\\): high values tend to follow high values and low values tend to follow low values after \\(k\\) periods.\nNegative autocorrelation at lag \\(k\\): high values tend to follow low values and vice versa.\nNo autocorrelation: the series values \\(k\\) periods apart are uncorrelated, suggesting randomness at that lag.",
    "crumbs": [
      "Week 1",
      "Activity 3"
    ]
  },
  {
    "objectID": "activity3.html#autocorrelation-in-time-series",
    "href": "activity3.html#autocorrelation-in-time-series",
    "title": "Activity3",
    "section": "",
    "text": "Autocorrelation, also known as serial correlation, measures the correlation of a time series with its own past and future values. Mathematically, the autocorrelation function (ACF) at lag \\(k\\) for a time series \\(\\{X_t\\}\\) is defined as:\n\\[\n\\rho_k = \\frac{\\text{Cov}(X_t, X_{t-k})}{\\sqrt{\\text{Var}(X_t) \\text{Var}(X_{t-k})}}\n\\]\nwhere \\(\\rho_k\\) measures the linear relationship between values \\(k\\) time periods apart.\n\nPositive autocorrelation at lag \\(k\\): high values tend to follow high values and low values tend to follow low values after \\(k\\) periods.\nNegative autocorrelation at lag \\(k\\): high values tend to follow low values and vice versa.\nNo autocorrelation: the series values \\(k\\) periods apart are uncorrelated, suggesting randomness at that lag.",
    "crumbs": [
      "Week 1",
      "Activity 3"
    ]
  },
  {
    "objectID": "activity3.html#partial-autocorrelation-in-time-series",
    "href": "activity3.html#partial-autocorrelation-in-time-series",
    "title": "Activity3",
    "section": "Partial Autocorrelation in Time Series",
    "text": "Partial Autocorrelation in Time Series\nPartial autocorrelation measures the correlation between a time series and its lagged values, after removing the influence of intermediate lags. In other words, it quantifies the direct relationship between \\(X_t\\) and \\(X_{t-k}\\), controlling for the effects of \\(X_{t-1}, X_{t-2}, \\dots, X_{t-k+1}\\). Mathematically, the partial autocorrelation function (PACF) at lag \\(k\\) for a time series \\(\\{X_t\\}\\) is defined as:\n\\[\n\\phi_{kk} = \\text{Corr}(X_t, X_{t-k} \\mid X_{t-1}, X_{t-2}, \\dots, X_{t-k+1})\n\\]\nwhere \\(\\phi_{kk}\\) represents the partial autocorrelation at lag \\(k\\).\n\nInterpretation of PACF:\n\nA significant partial autocorrelation at lag \\(k\\) suggests a direct relationship between \\(X_t\\) and \\(X_{t-k}\\), independent of the intermediate lags.\nA non-significant partial autocorrelation indicates that the relationship between \\(X_t\\) and \\(X_{t-k}\\) is fully explained by the intermediate lags.\n\nComparison with ACF:\n\nWhile the ACF measures the total correlation between \\(X_t\\) and \\(X_{t-k}\\), the PACF isolates the direct correlation, making it a more precise tool for model identification.\n\n\n\nPractical Illustration with Real Data\nWe will use real datasets from the fpp3 package to compute and visualize autocorrelation. The following code snippets demonstrate how to plot the ACF and PACF for a time series, providing insights into its internal structure.\n\nlibrary(fpp3)\n\naus_airpassengers %&gt;%\n  ACF(Passengers) %&gt;%             # Calculate autocorrelations for the Passengers series\n  autoplot() +                    # Plot the ACF\n  labs(title = \"ACF of Australian Air Passengers\", y = \"ACF\", x = \"Lag\")\n\n\n\n\n\n\n\n# Example 2: Partial Autocorrelation of Australian Air Passengers\naus_airpassengers %&gt;%\n  PACF(Passengers) %&gt;%            # Calculate partial autocorrelations for the Passengers series\n  autoplot() +                    # Plot the PACF\n  labs(title = \"PACF of Australian Air Passengers\", y = \"PACF\", x = \"Lag\")",
    "crumbs": [
      "Week 1",
      "Activity 3"
    ]
  },
  {
    "objectID": "activity3.html#white-noise",
    "href": "activity3.html#white-noise",
    "title": "Activity3",
    "section": "White Noise",
    "text": "White Noise\n\nWhite Noise\n\nNo autocorrelation: \\(\\mathbb{E}[X_t X_{t-k}] = 0, \\quad \\forall k \\neq 0\\)\n\nConstant variance: \\(\\operatorname{Var}(X_t) = \\sigma^2\\)\n\nThe white noise process is defined as:\n\\[\nX_t = \\varepsilon_t, \\quad \\varepsilon_t \\stackrel{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)\n\\]\n\n\n# White Noise Simulation\nwn &lt;- tsibble(time = 1:500, y = rnorm(500), index = time)\n\n# Visualize and test for autocorrelation\nwn %&gt;% autoplot(y) + ggtitle(\"White Noise Process\")\n\n\n\n\n\n\n\nwn %&gt;% ACF(y) %&gt;% autoplot()\n\n\n\n\n\n\n\nwn %&gt;% features(y, ljung_box, lag = 10)  # Should retain H₀ (no autocorrelation)\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    7.71     0.658\n\n\n\nPortmanteau (Ljung-Box) Test\n\nNull hypothesis (\\(H_0\\)): No autocorrelation up to lag \\(h\\)\n\nTest statistic:\n\\[\nQ = n(n+2) \\sum_{k=1}^{h} \\frac{\\hat{\\rho}_k^2}{n-k} \\sim \\chi^2_h\n\\]\n\n\n\nwn %&gt;% features(y, ljung_box, lag = 10)  # p &gt; 0.05 ⇒ retain H₀ (white noise)\n\n# A tibble: 1 × 2\n  lb_stat lb_pvalue\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    7.71     0.658\n\n\n\nPractical Verification\n\nSimulate contaminated white noise to test robustness:\n\n\n\ncontaminated_wn &lt;- wn %&gt;% \n mutate(y = y + 0.3*lag(y, 5) %&gt;% replace_na(0))\n\n\nVisualize and test the contaminated series:\n\n\ncontaminated_wn %&gt;% \n ACF(y) %&gt;% \n autoplot() + \n ggtitle(\"ACF of Contaminated Series\")\n\n\n\n\n\n\n\ncontaminated_wn %&gt;% \n features(y, ljung_box, lag = 10)  # p &lt; 0.05 ⇒ reject H₀\n\n# A tibble: 1 × 2\n  lb_stat  lb_pvalue\n    &lt;dbl&gt;      &lt;dbl&gt;\n1    44.7 0.00000251\n\n\n\nLab Activity: White Noise Series, ACF, and Portmanteau Test\n\nSimulate a White Noise Series\nGenerate a white noise series of length \\(n = 500\\) using the equation: \\[\nX_t = \\varepsilon_t, \\quad \\varepsilon_t \\stackrel{iid}{\\sim} \\mathcal{N}(0, 1)\n\\] Plot the series.\nPlot the ACF\nCompute and plot the autocorrelation function (ACF) of the white noise series. Interpret the results.\nContaminate the Series\nIntroduce contamination into the white noise series by adding a lagged component: \\[\nY_t = X_t + 0.4 \\cdot X_{t-3}\n\\] Replace missing values with 0. Plot the contaminated series.\nPlot the ACF of the Contaminated Series\nCompute and plot the ACF of the contaminated series. Compare it with the ACF of the original white noise series.\nPerform the Portmanteau (Ljung-Box) Test\n\nApply the Ljung-Box test to the original white noise series. Interpret the results.\nApply the Ljung-Box test to the contaminated series. Interpret the results.",
    "crumbs": [
      "Week 1",
      "Activity 3"
    ]
  },
  {
    "objectID": "activity16.html",
    "href": "activity16.html",
    "title": "Activity16",
    "section": "",
    "text": "Investigate whether changes in COVID-19 vaccination numbers help predict changes in confirmed cases using a Granger causality test.\n\n# Retrieve COVID-19 data for the United States and prepare a tsibble\ncovid_data &lt;- covid19(verbose = FALSE) %&gt;% \n  filter(administrative_area_level_1 == \"United States\") %&gt;% \n  mutate(date = ymd(date)) %&gt;% \n  as_tsibble(index = date) %&gt;% \n  select(date, confirmed, vaccines) %&gt;% \n  mutate(vaccines = as.integer(vaccines)) %&gt;% \n  drop_na()\n\n\n# Compute daily changes in confirmed cases and vaccinations\ncovid_data &lt;- covid_data %&gt;% \n  mutate(dConfirmed = difference(confirmed),\n         dVacc = difference(vaccines)) %&gt;% \n  drop_na()\n\n\n# Plot daily changes in confirmed cases and vaccinations\ncovid_data %&gt;% \n  autoplot(vars(dConfirmed, dVacc)) +\n  labs(title = \"Daily Changes: Confirmed Cases & Vaccinations\", x = \"Date\")\n\n\n\n\n\n\n\n\n\n\n\nThe Granger causality test checks whether past values of one variable (here, dVacc) provide statistically significant information for predicting another variable (dConfirmed).\nImplications and Uses:\n\nA significant result (p-value &lt; 0.05) suggests that changes in vaccinations Granger-cause changes in confirmed cases.\nThis does not imply true causality, but indicates that past vaccination data improves forecast accuracy for confirmed cases.\nSuch insights can help inform public health policies by highlighting the predictive value of vaccination trends.\n\nP-value Interpretation:\n\np &lt; 0.05: Reject the null hypothesis. Past vaccination changes significantly improve prediction of confirmed cases.\np ≥ 0.05: Fail to reject the null hypothesis. Vaccination changes do not add significant predictive power.\n\n\n\n\n\n# Test if changes in vaccinations (dVacc) Granger-cause changes in confirmed cases (dConfirmed) using a lag of 1.\nmodel_full &lt;- lm(dConfirmed ~ lag(dVacc, 1), data = covid_data)\nmodel_restricted &lt;- lm(dConfirmed ~ 1, data = covid_data)\nF_stat &lt;- ((deviance(model_restricted) - deviance(model_full)) / 1) / \n          (deviance(model_full) / (nrow(covid_data) - 2 - 1))\ndf1 &lt;- 1\ndf2 &lt;- nrow(covid_data) - 2 - 1\np_value &lt;- pf(F_stat, df1, df2, lower.tail = FALSE)\np_value\n\n[1] 0.260462",
    "crumbs": [
      "Week 4",
      "Activity 16"
    ]
  },
  {
    "objectID": "activity16.html#covid-data-analysis-and-granger-causality-test",
    "href": "activity16.html#covid-data-analysis-and-granger-causality-test",
    "title": "Activity16",
    "section": "",
    "text": "Investigate whether changes in COVID-19 vaccination numbers help predict changes in confirmed cases using a Granger causality test.\n\n# Retrieve COVID-19 data for the United States and prepare a tsibble\ncovid_data &lt;- covid19(verbose = FALSE) %&gt;% \n  filter(administrative_area_level_1 == \"United States\") %&gt;% \n  mutate(date = ymd(date)) %&gt;% \n  as_tsibble(index = date) %&gt;% \n  select(date, confirmed, vaccines) %&gt;% \n  mutate(vaccines = as.integer(vaccines)) %&gt;% \n  drop_na()\n\n\n# Compute daily changes in confirmed cases and vaccinations\ncovid_data &lt;- covid_data %&gt;% \n  mutate(dConfirmed = difference(confirmed),\n         dVacc = difference(vaccines)) %&gt;% \n  drop_na()\n\n\n# Plot daily changes in confirmed cases and vaccinations\ncovid_data %&gt;% \n  autoplot(vars(dConfirmed, dVacc)) +\n  labs(title = \"Daily Changes: Confirmed Cases & Vaccinations\", x = \"Date\")\n\n\n\n\n\n\n\n\n\n\n\nThe Granger causality test checks whether past values of one variable (here, dVacc) provide statistically significant information for predicting another variable (dConfirmed).\nImplications and Uses:\n\nA significant result (p-value &lt; 0.05) suggests that changes in vaccinations Granger-cause changes in confirmed cases.\nThis does not imply true causality, but indicates that past vaccination data improves forecast accuracy for confirmed cases.\nSuch insights can help inform public health policies by highlighting the predictive value of vaccination trends.\n\nP-value Interpretation:\n\np &lt; 0.05: Reject the null hypothesis. Past vaccination changes significantly improve prediction of confirmed cases.\np ≥ 0.05: Fail to reject the null hypothesis. Vaccination changes do not add significant predictive power.\n\n\n\n\n\n# Test if changes in vaccinations (dVacc) Granger-cause changes in confirmed cases (dConfirmed) using a lag of 1.\nmodel_full &lt;- lm(dConfirmed ~ lag(dVacc, 1), data = covid_data)\nmodel_restricted &lt;- lm(dConfirmed ~ 1, data = covid_data)\nF_stat &lt;- ((deviance(model_restricted) - deviance(model_full)) / 1) / \n          (deviance(model_full) / (nrow(covid_data) - 2 - 1))\ndf1 &lt;- 1\ndf2 &lt;- nrow(covid_data) - 2 - 1\np_value &lt;- pf(F_stat, df1, df2, lower.tail = FALSE)\np_value\n\n[1] 0.260462",
    "crumbs": [
      "Week 4",
      "Activity 16"
    ]
  },
  {
    "objectID": "activity16.html#lab-activity-exploring-granger-causality-with-different-lags",
    "href": "activity16.html#lab-activity-exploring-granger-causality-with-different-lags",
    "title": "Activity16",
    "section": "Lab Activity: Exploring Granger Causality with Different Lags",
    "text": "Lab Activity: Exploring Granger Causality with Different Lags\nTask: Using the covid_data, perform Granger causality tests with lag orders 1, 2, 3, 5, and 10.\nFor each lag, compute the p-value to assess whether lagged vaccination changes improve the prediction of confirmed cases. Answer the following:\n\nHow do the p-values change with different lag orders?\nWhich lag order appears most appropriate for capturing the predictive relationship?\n\nWrite a brief discussion on how these findings might inform public health policy.",
    "crumbs": [
      "Week 4",
      "Activity 16"
    ]
  },
  {
    "objectID": "activity6.html#lab-activity",
    "href": "activity6.html#lab-activity",
    "title": "Activity6",
    "section": "Lab Activity",
    "text": "Lab Activity\nRepeat the stationarity and unit root analysis (using ADF and KPSS tests) for NVIDIA (NVDA) stock prices from 2010-01-01 to 2023-01-01. Apply differencing or other transformations as needed to achieve stationarity, and interpret the results.\nSolution:\n\n# Step 1: Get NVIDIA (NVDA) stock data\nnvda &lt;- tq_get(\"NVDA\", \n               from = \"2010-01-01\", \n               to = \"2023-01-01\",\n               get = \"stock.prices\") %&gt;%\n  select(date, adjusted) %&gt;%  # Use adjusted closing prices\n  rename(price = adjusted) %&gt;% \n  as_tsibble(index = date)  # Convert to tsibble with date index\n\n# Step 2: Plot the NVIDIA stock price series\nautoplot(nvda, price) + \n  labs(title = \"NVIDIA (NVDA) Stock Price Series\", \n       x = \"Date\", \n       y = \"Adjusted Closing Price\")\n\n\n\n\n\n\n\n# Step 3: Perform ADF test on the original series\nadf_test_nvda &lt;- adf.test(nvda$price)\nadf_test_nvda\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  nvda$price\nDickey-Fuller = -2.3584, Lag order = 14, p-value = 0.4266\nalternative hypothesis: stationary\n\n# Step 4: Perform KPSS test on the original series\nkpss_test_nvda &lt;- nvda %&gt;%\n  features(price, unitroot_kpss)\nkpss_test_nvda\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      22.2        0.01\n\n# Step 5: Difference the series to achieve stationarity\ndiff_nvda &lt;- nvda %&gt;% mutate(price = difference(price))\n\n# Step 6: Plot the differenced series\nautoplot(diff_nvda, price) + \n  labs(title = \"Differenced NVIDIA (NVDA) Stock Price Series\", \n       x = \"Date\", \n       y = \"Differenced Price\")\n\n\n\n\n\n\n\n# Step 7: Perform ADF test on the differenced series\nadf_test_diff_nvda &lt;- adf.test(na.omit(diff_nvda$price))\nadf_test_diff_nvda\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  na.omit(diff_nvda$price)\nDickey-Fuller = -14.601, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n# Step 8: Perform KPSS test on the differenced series\nkpss_test_diff_nvda &lt;- diff_nvda %&gt;%\n  features(price, unitroot_kpss)\nkpss_test_diff_nvda\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1    0.0934         0.1",
    "crumbs": [
      "Week 2",
      "Activity 6"
    ]
  },
  {
    "objectID": "activity13.html",
    "href": "activity13.html",
    "title": "Activity13",
    "section": "",
    "text": "Seasonality = Repeating patterns with fixed known periods (e.g., daily, weekly, yearly cycles). Differentiated from cycles by strict periodicity.",
    "crumbs": [
      "Week 4",
      "Activity 13"
    ]
  },
  {
    "objectID": "activity13.html#seasonal-modeling",
    "href": "activity13.html#seasonal-modeling",
    "title": "Activity13",
    "section": "",
    "text": "Seasonality = Repeating patterns with fixed known periods (e.g., daily, weekly, yearly cycles). Differentiated from cycles by strict periodicity.",
    "crumbs": [
      "Week 4",
      "Activity 13"
    ]
  },
  {
    "objectID": "activity13.html#fourier-spectral-analysis",
    "href": "activity13.html#fourier-spectral-analysis",
    "title": "Activity13",
    "section": "1. Fourier Spectral Analysis",
    "text": "1. Fourier Spectral Analysis\nFrequency domain decomposition using basis functions:\n\\[S(t) = \\sum_{k=1}^K [a_k\\cos(2\\pi f_k t) + b_k\\sin(2\\pi f_k t)]\\]\nCritical parameters:\n\nNyquist frequency: Max detectable frequency = \\(f_{\\text{max}} = \\frac{1}{2\\Delta t}\\)\nFrequency resolution: \\(\\Delta f = \\frac{1}{N\\Delta t}\\)\nNumber of harmonics (K): Balances complexity vs. overfitting\n\nHarmonics represent pairs of sine/cosine waves that collectively approximate complex seasonal patterns. Think of them as “building blocks” for seasonal shapes. We can capture periodic patterns in time series data using Fourier terms and periodic regressions. Fourier terms in regression models:\n\\[y_t = \\beta_0 + \\sum_{k=1}^K \\beta_k\\cos(2\\pi kt/m) + \\gamma_k\\sin(2\\pi kt/m) + \\epsilon_t\\] where \\(m\\) = seasonal period\nHybrid Approach: Combines Fourier terms (deterministic seasonality) with ARIMA (stochastic patterns):\n\\[y_t = \\underbrace{\\sum_{k=1}^K [\\alpha_k\\sin(2\\pi kt/m) + \\beta_k\\cos(2\\pi kt/m)]}_{\\text{Fourier terms}} + \\underbrace{\\text{ARIMA}(p,d,q)(P,D,Q)_m}_{\\text{Seasonal ARIMA}} + \\epsilon_t\\]\n\nDatasets & Models\n\nFinance Example (Intraday)\n\nDataset: nyse (Stock Volume)\nModel: \\[\ny_t = \\beta_0 + \\sum_{k=1}^2\\Big[\\alpha_k\\sin\\Big(\\frac{2\\pi kt}{78}\\Big) + \\beta_k\\cos\\Big(\\frac{2\\pi kt}{78}\\Big)\\Big]\n\\]\nActivity: Optimize Fourier terms via cross-validation.\n\n\nWe retrieve real NYSE Composite index data (ticker ^NYA), extract the adjusted price, and apply Fourier regression to capture seasonal patterns.\n\n# 1. Get NYSE Composite index data (volume) from tidyquant and convert to tsibble\nnyse &lt;- tq_get(\"^NYA\", from = \"2018-01-01\") %&gt;% \n  as_tsibble(index = date) %&gt;% \n  fill_gaps() %&gt;% \n  mutate(adjusted = imputeTS::na_ma(adjusted, k = 21))\n\n# Plot adjusted series\nnyse %&gt;% \n  autoplot(adjusted) +\n  labs(title = \"NYSE Composite Index Volume\", y = \"Volume\", x = \"Date\")\n\n\n\n\n\n\n\n\nFit seasonal models using Fourier terms (using 1 and 2 harmonics) with cross-validation for model selection.\n\nK=1 → \\(\\cos(2\\pi t/52)\\), \\(\\sin(2\\pi t/52)\\)\nK=2 → Adds \\(\\cos(4\\pi t/52)\\), \\(\\sin(4\\pi t/52)\\)\n\n\n# 2. Define seasonal period (business days/year ≈ 252)\nperiod &lt;- 30  # Annual seasonality in trading days\n\n# 3. K Optimization via AICc\naic_results &lt;- map_dfr(1:10, function(k) {\n  fit &lt;- nyse %&gt;%\n    model(\n      ARIMA(\n        adjusted ~ fourier(K = k, period = period) + \n        pdq(0:5, 0:2, 0:5) + PDQ(0:2, 0:1, 0:2) \n      )\n    )\n  \n  glance(fit) %&gt;%\n    mutate(K = k)\n})\n\noptimal_k &lt;- aic_results %&gt;% \n  filter(AICc == min(AICc)) %&gt;% \n  pull(K)\n\n\n# 4. Final Hybrid Model\nfinal_model &lt;- nyse %&gt;% \n  model(\n    Optimal = ARIMA(\n      adjusted ~ fourier(period = period, K = optimal_k) \n    )\n  )\n\n\n# 5. Cross-validation with optimal K\ncv_results &lt;- nyse %&gt;% \n  stretch_tsibble(.init = 3*period, .step = 21) %&gt;%  # 3-year window, monthly steps\n  model(\n    Optimal = ARIMA(\n      adjusted ~ pdq() + PDQ() + \n      fourier(period = period, K = optimal_k)\n    )\n  ) %&gt;% \n  forecast(h = 21) %&gt;%  # 1-month ahead\n  accuracy(nyse)\n\n\n# 6. Diagnostic Visualization\nfinal_model %&gt;% \n  gg_tsresiduals() + \n  labs(title = paste(\"Residuals for K =\", optimal_k))\n\n\n\n\n\n\n\nfinal_model %&gt;% \n  forecast(h = period) %&gt;%  # 1-year forecast\n  autoplot(nyse %&gt;% filter(year(date) &gt;= 2022)) +\n  labs(title = \"NYSE Adjusted Price Forecast with Optimized Fourier-SARIMA\")\n\n\n\n\n\n\n\n\n\nLab Activities\n\nTask: For the following COVID data, compute daily new confirmed cases, and try fitting the periodic regression with Fourier terms with different seasonal periods (e.g., 7-day vs. 14-day cycles) and compare the residual diagnostics.",
    "crumbs": [
      "Week 4",
      "Activity 13"
    ]
  },
  {
    "objectID": "activity13.html#references",
    "href": "activity13.html#references",
    "title": "Activity13",
    "section": "References",
    "text": "References\nGuidotti, E., Ardia, D., (2020), “COVID-19 Data Hub”, Journal of Open Source Software 5(51):2376, doi: 10.21105/joss.02376",
    "crumbs": [
      "Week 4",
      "Activity 13"
    ]
  },
  {
    "objectID": "activity5.html",
    "href": "activity5.html",
    "title": "Activity5",
    "section": "",
    "text": "A time series is stationary if its statistical properties (mean, variance, autocorrelation) do not change over time. Stationarity is a key assumption for many time series models (e.g., ARIMA). Non-stationary series often exhibit:\n\nTrends: A long-term increase or decrease in the data.\nSeasonality: Periodic fluctuations.\nChanging Variance: Variability that increases or decreases over time.\n\n\n\n\n\nDifferencing: Removes trends by computing the difference between consecutive observations.\n\nFormula: \\(\\nabla X_t = X_t - X_{t-1}\\)\n\nLog Transformation: Stabilizes multiplicative variance by applying the natural logarithm.\n\nFormula: \\(Y_t = \\log(X_t)\\)\n\nBox-Cox Transformation: A generalized power transformation that stabilizes variance and can handle non-linear trends.\n\nFormula: \\(Y_t = \\frac{X_t^\\lambda - 1}{\\lambda}\\)\n\n\n\n\n\nWe start by loading and visualizing a non-stationary time series (Google stock prices).\n\n# Load data\ngoogle &lt;- read_csv(\"~/Desktop/Math493Spring25ClassMaterials/data/google.csv\")\ngoogle_stock &lt;- google |&gt; \n  as_tsibble() |&gt; \n  mutate(LogClose = log(Close))  # Log-transform for later use\n\n# Plot original series\ngoogle_stock |&gt;\n  autoplot(Close) +\n  labs(title = \"Google Stock Price (Non-Stationary)\", y = \"USD\")\n\n\n\n\n\n\n\n\nObservation: The upward trend indicates non-stationarity.\n\n\n\nUse the KPSS test (null hypothesis: stationarity) and ADF test (null hypothesis: unit root) to confirm non-stationarity.\n\n# KPSS test\ngoogle_stock |&gt;\n  features(Close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      10.6        0.01\n\n# Suggested differencing order\ngoogle_stock |&gt;\n  features(Close, unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\nInterpretation:\n\nKPSS p-value &lt; 0.05 ⇒ Reject stationarity.\nunitroot_ndiffs suggests the number of differences required (e.g., 1).\n\n\n\n\n\nDifferencing removes trends by computing the difference between consecutive observations.\n\n# First difference\ngoogle_stationary &lt;- google_stock |&gt;\n  mutate(DiffClose = difference(Close))\n\n# Plot differenced series\ngoogle_stationary |&gt;\n  autoplot(DiffClose) +\n  labs(title = \"Differenced Google Stock Price\", y = \"ΔUSD\")\n\n\n\n\n\n\n\n\nObservation: The differenced series has a stabilized mean.\n\n\n\nLog transformations stabilize multiplicative variance.\n\n# Plot original vs log-transformed series\ngoogle_stock |&gt;\n  pivot_longer(c(Close, LogClose)) |&gt;\n  autoplot(value) +\n  facet_grid(name ~ ., scales = \"free_y\") +\n  labs(title = \"Log Transformation Stabilizes Variance\")\n\n\n\n\n\n\n\n\nObservation: The log-transformed series shows reduced variance amplification.\n\n\n\n\nThe Box-Cox transformation generalizes log and power transformations.\n\n# Estimate optimal lambda\nlambda &lt;- google_stock |&gt;\n  features(Close, features = guerrero) |&gt;\n  pull(lambda_guerrero)\n\n# Apply Box-Cox transformation\ngoogle_stock |&gt;\n  mutate(BoxCoxClose = box_cox(Close, lambda)) |&gt;\n  autoplot(BoxCoxClose) +\n  labs(title = \"Box-Cox Transformed Series\")\n\n\n\n\n\n\n\n\nObservation: The Box-Cox transformation stabilizes both mean and variance.",
    "crumbs": [
      "Week 2",
      "Activity 5"
    ]
  },
  {
    "objectID": "activity5.html#differencing-transformations-and-stationarity",
    "href": "activity5.html#differencing-transformations-and-stationarity",
    "title": "Activity5",
    "section": "",
    "text": "A time series is stationary if its statistical properties (mean, variance, autocorrelation) do not change over time. Stationarity is a key assumption for many time series models (e.g., ARIMA). Non-stationary series often exhibit:\n\nTrends: A long-term increase or decrease in the data.\nSeasonality: Periodic fluctuations.\nChanging Variance: Variability that increases or decreases over time.\n\n\n\n\n\nDifferencing: Removes trends by computing the difference between consecutive observations.\n\nFormula: \\(\\nabla X_t = X_t - X_{t-1}\\)\n\nLog Transformation: Stabilizes multiplicative variance by applying the natural logarithm.\n\nFormula: \\(Y_t = \\log(X_t)\\)\n\nBox-Cox Transformation: A generalized power transformation that stabilizes variance and can handle non-linear trends.\n\nFormula: \\(Y_t = \\frac{X_t^\\lambda - 1}{\\lambda}\\)\n\n\n\n\n\nWe start by loading and visualizing a non-stationary time series (Google stock prices).\n\n# Load data\ngoogle &lt;- read_csv(\"~/Desktop/Math493Spring25ClassMaterials/data/google.csv\")\ngoogle_stock &lt;- google |&gt; \n  as_tsibble() |&gt; \n  mutate(LogClose = log(Close))  # Log-transform for later use\n\n# Plot original series\ngoogle_stock |&gt;\n  autoplot(Close) +\n  labs(title = \"Google Stock Price (Non-Stationary)\", y = \"USD\")\n\n\n\n\n\n\n\n\nObservation: The upward trend indicates non-stationarity.\n\n\n\nUse the KPSS test (null hypothesis: stationarity) and ADF test (null hypothesis: unit root) to confirm non-stationarity.\n\n# KPSS test\ngoogle_stock |&gt;\n  features(Close, unitroot_kpss)\n\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      10.6        0.01\n\n# Suggested differencing order\ngoogle_stock |&gt;\n  features(Close, unitroot_ndiffs)\n\n# A tibble: 1 × 1\n  ndiffs\n   &lt;int&gt;\n1      1\n\n\nInterpretation:\n\nKPSS p-value &lt; 0.05 ⇒ Reject stationarity.\nunitroot_ndiffs suggests the number of differences required (e.g., 1).\n\n\n\n\n\nDifferencing removes trends by computing the difference between consecutive observations.\n\n# First difference\ngoogle_stationary &lt;- google_stock |&gt;\n  mutate(DiffClose = difference(Close))\n\n# Plot differenced series\ngoogle_stationary |&gt;\n  autoplot(DiffClose) +\n  labs(title = \"Differenced Google Stock Price\", y = \"ΔUSD\")\n\n\n\n\n\n\n\n\nObservation: The differenced series has a stabilized mean.\n\n\n\nLog transformations stabilize multiplicative variance.\n\n# Plot original vs log-transformed series\ngoogle_stock |&gt;\n  pivot_longer(c(Close, LogClose)) |&gt;\n  autoplot(value) +\n  facet_grid(name ~ ., scales = \"free_y\") +\n  labs(title = \"Log Transformation Stabilizes Variance\")\n\n\n\n\n\n\n\n\nObservation: The log-transformed series shows reduced variance amplification.\n\n\n\n\nThe Box-Cox transformation generalizes log and power transformations.\n\n# Estimate optimal lambda\nlambda &lt;- google_stock |&gt;\n  features(Close, features = guerrero) |&gt;\n  pull(lambda_guerrero)\n\n# Apply Box-Cox transformation\ngoogle_stock |&gt;\n  mutate(BoxCoxClose = box_cox(Close, lambda)) |&gt;\n  autoplot(BoxCoxClose) +\n  labs(title = \"Box-Cox Transformed Series\")\n\n\n\n\n\n\n\n\nObservation: The Box-Cox transformation stabilizes both mean and variance.",
    "crumbs": [
      "Week 2",
      "Activity 5"
    ]
  },
  {
    "objectID": "activity5.html#lab-activity",
    "href": "activity5.html#lab-activity",
    "title": "Activity5",
    "section": "Lab Activity",
    "text": "Lab Activity\n\nPrompt 1: Load and Visualize Data\n\nLoad the global_economy dataset from the tsibble package.\nPlot GDP for a specific country (e.g., “United States”). Assess stationarity visually.\n\nSolution:\n\n\n\n\n\n\n\n\n\n\nPrompt 2: Test for Stationarity\n\nUse the KPSS test to check for stationarity.\nDetermine the required differencing order using unitroot_ndiffs.\n\nSolution:\n\n\n\n\n\n\n\n\n\n\nPrompt 3: Apply Transformations\n\nApply first differencing to the GDP series.\nApply a Box-Cox transformation using Guerrero’s method to estimate λ.\nRe-test stationarity using the KPSS test.\n\nSolution:",
    "crumbs": [
      "Week 2",
      "Activity 5"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Advanced Topics in Statistics: Applied Time Series! This course focuses on analyzing and forecasting time-dependent data through modern statistical methods. Using real-world case studies and hands-on projects, you will learn to build, evaluate, and compare models like ARIMA and ETS (Error-Trend-Seasonality) to solve problems in economics, finance, public health, and beyond.\nWe will emphasize practical skills, including exploratory time series analysis, model diagnostics, and state-space frameworks, using R for computation and visualization. By the end of the course, you will be able to interpret complex time series patterns, generate reliable forecasts, and communicate insights effectively to diverse audiences.\n\n\n\n\nLearn basic analysis of time series data and exploratory data analysis.\nUnderstand and apply time series regression models.\nMaster autoregressive (AR) and moving average (MA) models.\nMaster ARIMA, ETS, and state-space models for forecasting.\nCompare and select models (e.g., ETS vs ARIMA) using diagnostic tools.\nUtilize R for computation, visualization, and analysis of time series data.\nDevelop skills to interpret and communicate time series results effectively.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#learning-objectives",
    "href": "about.html#learning-objectives",
    "title": "About",
    "section": "",
    "text": "Learn basic analysis of time series data and exploratory data analysis.\nUnderstand and apply time series regression models.\nMaster autoregressive (AR) and moving average (MA) models.\nMaster ARIMA, ETS, and state-space models for forecasting.\nCompare and select models (e.g., ETS vs ARIMA) using diagnostic tools.\nUtilize R for computation, visualization, and analysis of time series data.\nDevelop skills to interpret and communicate time series results effectively.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "activity4.html",
    "href": "activity4.html",
    "title": "Activity4",
    "section": "",
    "text": "The Moving Average of Order 1 (MA(1)) process is defined as:\n\\[\nX_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}\n\\]\nwhere \\(\\varepsilon_t\\) is white noise with variance \\(\\sigma^2\\).\n\n\n\nShort Memory: The autocorrelation function (ACF) cuts off after lag 1.\n\nVariance: \\(\\gamma(0) = \\sigma^2(1 + \\theta^2)\\).\n\nCovariance: \\(\\gamma(1) = \\theta \\sigma^2\\).\n\n\n\n\n\n# MA(1) simulation\nn &lt;- 200\ntheta &lt;- 0.8\neps &lt;- rnorm(n + 1)  # ε_0 to ε_n\nma_values &lt;- numeric(n)\nfor(t in 1:n) {\n  ma_values[t] &lt;- eps[t+1] + theta*eps[t]  # X_t = ε_t + θε_{t-1}\n}\n\nma1 &lt;- tsibble(time = 1:n, y = ma_values, index = time)\n# Plot\nggplot(ma1, aes(x = time, y = y)) +\n  geom_line() +\n  labs(title = \"MA(1) Process\", x = \"Time\", y = \"X_t\") +\n  theme_minimal()",
    "crumbs": [
      "Week 1",
      "Activity 4"
    ]
  },
  {
    "objectID": "activity4.html#q1-ma1-process",
    "href": "activity4.html#q1-ma1-process",
    "title": "Activity4",
    "section": "Q1: MA(1) Process",
    "text": "Q1: MA(1) Process\nPrompt: Simulate and visualize an MA(1) process with parameter θ = 0.9 using tsibble and autoplot.\nSolution:",
    "crumbs": [
      "Week 1",
      "Activity 4"
    ]
  },
  {
    "objectID": "activity4.html#q2-ar1-process",
    "href": "activity4.html#q2-ar1-process",
    "title": "Activity4",
    "section": "Q2: AR(1) Process",
    "text": "Q2: AR(1) Process\nPrompt: Simulate and visualize an AR(1) process with parameter \\(\\phi=0.96\\) using tsibble and autoplot.\nSolution:",
    "crumbs": [
      "Week 1",
      "Activity 4"
    ]
  },
  {
    "objectID": "activity4.html#q3-random-walk-comparison",
    "href": "activity4.html#q3-random-walk-comparison",
    "title": "Activity4",
    "section": "Q3: Random Walk & Comparison",
    "text": "Q3: Random Walk & Comparison\nPrompt: Create a random walk with drift \\((\\delta = 0.2)\\), then visualize all three processes together.\nSolution:",
    "crumbs": [
      "Week 1",
      "Activity 4"
    ]
  },
  {
    "objectID": "activity21.html",
    "href": "activity21.html",
    "title": "Activity21",
    "section": "",
    "text": "Define AR(\\(p\\)): \\(Y_t = \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t\\), \\(\\epsilon_t \\sim WN(0,\\sigma^2)\\)\nAR(p) Properties:\n\nCaptures temporal dependence through lagged terms\nPACF cuts off after lag \\(p\\)\nRequires stationarity for reliable inference\nUnit root non-stationarity occurs when characteristic equation \\(1 - \\phi_1 z - \\cdots - \\phi_p z^p = 0\\) has roots on unit circle\n\n\n\n\n\n\n\n\n\n\n\nlibrary(fable)\nset.seed(123)\nar_data &lt;- tibble(time = 1:100, y = arima.sim(model = list(ar = c(0.7, -0.2)), n = 100)) %&gt;% \n  as_tsibble(index = time)\n\nar_data %&gt;% \n  gg_tsdisplay(y, plot_type = c(\"scatter\")) + # ACF/PACF\n  labs(title = \"AR(2) Process: φ₁=0.7, φ₂=-0.2\")\n\n\n\n\n\n\n\n\nModel Fitting & Diagnostics\n\nfit_ar &lt;- ar_data %&gt;% \n  model(ARIMA(y ~ pdq(2,0,0))) # Explicit AR specification\n\nreport(fit_ar) # Check coefficients & σ²\n\nSeries: y \nModel: ARIMA(2,0,0) \n\nCoefficients:\n         ar1      ar2\n      0.6757  -0.1576\ns.e.  0.0990   0.1002\n\nsigma^2 estimated as 0.8167:  log likelihood=-130.99\nAIC=267.99   AICc=268.24   BIC=275.8\n\nfit_ar %&gt;% residuals() %&gt;% gg_tsdisplay(plot_type = c(\"scatter\")) # Residual diagnostics\n\n\n\n\n\n\n\n\n\n\n\nFrom the global_economy dataset (Ch. 5 of text), we’ll analyze quarterly percentage changes in personal consumption expenditures (stationary series):\n\n\n\n\n\n\n\n\n\n\nExploratory Analysis & Stationarity\n\nVisualize Series:\n\n\n\n\n\n\n\n\n\n\nStationarity Assessment:\n\n\n\n\n\n\n\n\n\nModel Fitting & Forecasting\n\nFit AR Model:\n\n\n\n\n\n\n\n\n\n\nResidual Diagnostics:\n\n\n\n\n\n\n\n\n\n\nForecasting:\n\n\n\n\n\n\n\n\n\n\n\n\n1. Data Preparation & Visualization\n\n\n\n\n\n\n\n\n2. Order Identification\nDetermine appropriate AR order through PACF:\n\n\n\n\n\n\n\n\n3. Model Comparison\nFit competing specifications and evaluate:\n\n\n\n\n\n\n\n\n4. Forecasting & Policy Implications\nUsing the best model, generate forecasts up-to 10 time points into the future and interpret economic meaning:",
    "crumbs": [
      "Week 6",
      "Activity 21"
    ]
  },
  {
    "objectID": "activity21.html#time-series-nature-stationarity",
    "href": "activity21.html#time-series-nature-stationarity",
    "title": "Activity21",
    "section": "",
    "text": "Define AR(\\(p\\)): \\(Y_t = \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t\\), \\(\\epsilon_t \\sim WN(0,\\sigma^2)\\)\nAR(p) Properties:\n\nCaptures temporal dependence through lagged terms\nPACF cuts off after lag \\(p\\)\nRequires stationarity for reliable inference\nUnit root non-stationarity occurs when characteristic equation \\(1 - \\phi_1 z - \\cdots - \\phi_p z^p = 0\\) has roots on unit circle\n\n\n\n\n\n\n\n\n\n\n\nlibrary(fable)\nset.seed(123)\nar_data &lt;- tibble(time = 1:100, y = arima.sim(model = list(ar = c(0.7, -0.2)), n = 100)) %&gt;% \n  as_tsibble(index = time)\n\nar_data %&gt;% \n  gg_tsdisplay(y, plot_type = c(\"scatter\")) + # ACF/PACF\n  labs(title = \"AR(2) Process: φ₁=0.7, φ₂=-0.2\")\n\n\n\n\n\n\n\n\nModel Fitting & Diagnostics\n\nfit_ar &lt;- ar_data %&gt;% \n  model(ARIMA(y ~ pdq(2,0,0))) # Explicit AR specification\n\nreport(fit_ar) # Check coefficients & σ²\n\nSeries: y \nModel: ARIMA(2,0,0) \n\nCoefficients:\n         ar1      ar2\n      0.6757  -0.1576\ns.e.  0.0990   0.1002\n\nsigma^2 estimated as 0.8167:  log likelihood=-130.99\nAIC=267.99   AICc=268.24   BIC=275.8\n\nfit_ar %&gt;% residuals() %&gt;% gg_tsdisplay(plot_type = c(\"scatter\")) # Residual diagnostics",
    "crumbs": [
      "Week 6",
      "Activity 21"
    ]
  },
  {
    "objectID": "activity21.html#real-world-process-us-consumption",
    "href": "activity21.html#real-world-process-us-consumption",
    "title": "Activity21",
    "section": "",
    "text": "From the global_economy dataset (Ch. 5 of text), we’ll analyze quarterly percentage changes in personal consumption expenditures (stationary series):\n\n\n\n\n\n\n\n\n\n\nExploratory Analysis & Stationarity\n\nVisualize Series:\n\n\n\n\n\n\n\n\n\n\nStationarity Assessment:\n\n\n\n\n\n\n\n\n\nModel Fitting & Forecasting\n\nFit AR Model:\n\n\n\n\n\n\n\n\n\n\nResidual Diagnostics:\n\n\n\n\n\n\n\n\n\n\nForecasting:\n\n\n\n\n\n\n\n\n\n\n\n\n1. Data Preparation & Visualization\n\n\n\n\n\n\n\n\n2. Order Identification\nDetermine appropriate AR order through PACF:\n\n\n\n\n\n\n\n\n3. Model Comparison\nFit competing specifications and evaluate:\n\n\n\n\n\n\n\n\n4. Forecasting & Policy Implications\nUsing the best model, generate forecasts up-to 10 time points into the future and interpret economic meaning:",
    "crumbs": [
      "Week 6",
      "Activity 21"
    ]
  },
  {
    "objectID": "activity34.html",
    "href": "activity34.html",
    "title": "Activity34",
    "section": "",
    "text": "Data Prep\n\nUse historical energy demand + temperature\n\nEnsure alignment in timestamps (drop_na())\n\n\n\n\n\n\n\n\n\nStep 1 - Baseline ETS Model\n\n\n\n\n\n\n\n\n\\[\\text{Demand}_t = \\text{Level}_{t-1} + \\text{Error}_t\\]\nThe first model uses ETS (Error-Trend-Seasonal) for demand alone. This is limited because:\n\nIgnores temperature (known predictor)\nAssumes patterns are purely endogenous\n\nStep 2 - ARIMAX with Temperature\n\n\n\n\n\n\n\n\n\\[\\text{Demand}_t = \\phi_1 \\text{Demand}_{t-1} + \\beta \\text{Temperature}_t + \\epsilon_t\\]\nImprovement: Adds temperature as exogenous predictor.\nBut requires temperature forecasts - introduces error propagation if temperature predictions are poor.\n\n\n\nVector Autoregression (VAR)\n\\[\\begin{cases}\n\\text{Demand}_t = \\alpha_1 + \\sum_{i=1}^p \\phi_{1i}\\text{Demand}_{t-i} + \\sum_{i=1}^p \\psi_{1i}\\text{Temp}_{t-i} \\\\\n\\text{Temp}_t = \\alpha_2 + \\sum_{i=1}^p \\phi_{2i}\\text{Demand}_{t-i} + \\sum_{i=1}^p \\psi_{2i}\\text{Temp}_{t-i}\n\\end{cases}\\]\nKey Advantages\n\nHandles bidirectional relationships (temperature ↔︎ demand)\nCaptures lagged cross-effects\n\nBetter for short-term forecasts where system inertia matters\n\n\n3. Critical Implementation Details\n\nDifferencing: Makes series stationary for VAR (difference())\nLag choice: 48 lags = 24hr periodicity (half-hourly data)\nForecast alignment:\n\n\\[\\text{VarForecast}_t = \\text{LastObs} + \\sum \\text{DifferencedForecasts}_t\\]\n(Reverse the differencing through cumulative sums)\n4. Comparison\n\nETS: Good baseline but misses covariates\n\nARIMAX: Better with good temp forecasts\n\nVAR: Best for short-term co-movements, no external forecast needed",
    "crumbs": [
      "Week 9",
      "Activity 34"
    ]
  },
  {
    "objectID": "activity34.html#extending-ets-models-with-predictors",
    "href": "activity34.html#extending-ets-models-with-predictors",
    "title": "Activity34",
    "section": "",
    "text": "Data Prep\n\nUse historical energy demand + temperature\n\nEnsure alignment in timestamps (drop_na())\n\n\n\n\n\n\n\n\n\nStep 1 - Baseline ETS Model\n\n\n\n\n\n\n\n\n\\[\\text{Demand}_t = \\text{Level}_{t-1} + \\text{Error}_t\\]\nThe first model uses ETS (Error-Trend-Seasonal) for demand alone. This is limited because:\n\nIgnores temperature (known predictor)\nAssumes patterns are purely endogenous\n\nStep 2 - ARIMAX with Temperature\n\n\n\n\n\n\n\n\n\\[\\text{Demand}_t = \\phi_1 \\text{Demand}_{t-1} + \\beta \\text{Temperature}_t + \\epsilon_t\\]\nImprovement: Adds temperature as exogenous predictor.\nBut requires temperature forecasts - introduces error propagation if temperature predictions are poor.\n\n\n\nVector Autoregression (VAR)\n\\[\\begin{cases}\n\\text{Demand}_t = \\alpha_1 + \\sum_{i=1}^p \\phi_{1i}\\text{Demand}_{t-i} + \\sum_{i=1}^p \\psi_{1i}\\text{Temp}_{t-i} \\\\\n\\text{Temp}_t = \\alpha_2 + \\sum_{i=1}^p \\phi_{2i}\\text{Demand}_{t-i} + \\sum_{i=1}^p \\psi_{2i}\\text{Temp}_{t-i}\n\\end{cases}\\]\nKey Advantages\n\nHandles bidirectional relationships (temperature ↔︎ demand)\nCaptures lagged cross-effects\n\nBetter for short-term forecasts where system inertia matters\n\n\n3. Critical Implementation Details\n\nDifferencing: Makes series stationary for VAR (difference())\nLag choice: 48 lags = 24hr periodicity (half-hourly data)\nForecast alignment:\n\n\\[\\text{VarForecast}_t = \\text{LastObs} + \\sum \\text{DifferencedForecasts}_t\\]\n(Reverse the differencing through cumulative sums)\n4. Comparison\n\nETS: Good baseline but misses covariates\n\nARIMAX: Better with good temp forecasts\n\nVAR: Best for short-term co-movements, no external forecast needed",
    "crumbs": [
      "Week 9",
      "Activity 34"
    ]
  },
  {
    "objectID": "activity8.html",
    "href": "activity8.html",
    "title": "Activity8",
    "section": "",
    "text": "When a time series exhibits regular, recurring patterns over fixed periods (e.g., 12-month cycles in yearly data), seasonal differencing can help isolate these cyclical components. By subtracting a value from the same season in the previous cycle, you remove much of the repeated seasonal effect:\n\\[\n\\text{SeasonalDiff}_t = Y_t - Y_{t-s},\n\\]\nwhere (s) is the seasonal period (for instance, (s = 12) for monthly data with yearly cycles). This technique is particularly useful for series that show strong periodic fluctuations without requiring other transformations.\n\n\n\nA rolling mean (or moving average) smooths out short-term fluctuations by averaging consecutive observations within a specified window. For a window size (w), the rolling mean at time (t) is:\n\\[\n\\text{RollingMean}_t = \\frac{1}{w} \\sum_{i = t-w+1}^{t} Y_i.\n\\]\nThis technique helps reveal longer-term trends and cyclical behavior in a time series, since each point is now an average of the most recent (w) observations.\n\n\n\nA rolling variance measures how dispersed recent values are around their rolling mean within the same window (w). It is calculated as:\n\\[\n\\text{RollingVar}_t = \\frac{1}{w} \\sum_{i = t-w+1}^{t} \\bigl(Y_i - \\text{RollingMean}_t\\bigr)^2.\n\\]\nObserving the rolling variance over time can highlight periods of increased or decreased fluctuation and help detect heteroscedasticity (changing variance). A more stable rolling variance often indicates that seasonal or cyclic behavior has been accounted for, especially after an appropriate seasonal differencing step.\n\n# Convert AirPassengers to a tsibble\nairpass &lt;- as_tsibble(AirPassengers) %&gt;%\n  rename(Passengers = value)\n\n# Plot the original series\nautoplot(airpass, Passengers) +\n  labs(title = \"Monthly Air Passengers\", \n       subtitle = \"Strong Trend and Seasonality\",\n       y = \"Number of Passengers\", x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Compute and plot ACF\nacf_data &lt;- ACF(airpass, Passengers, lag_max = 36)\nautoplot(acf_data) +\n  labs(title = \"ACF of Air Passengers\", \n       subtitle = \"Slow Decay Indicates Non-Stationarity\",\n       y = \"Autocorrelation\", x = \"Lag\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nRolling statistics (mean and variance) help visualize changes in the series over time. For a stationary series, these should remain roughly constant.\nObservations:\n\nThe rolling mean increases over time, confirming a trend.\nThe rolling variance also increases, indicating heteroscedasticity.\n\n\n# Compute rolling mean and variance\nairpass_roll &lt;- airpass %&gt;%\n  mutate(RollingMean = slider::slide_dbl(Passengers, mean, .before = 11, .complete = TRUE),\n         RollingVar = slider::slide_dbl(Passengers, var, .before = 11, .complete = TRUE))\n\n# Plot rolling statistics\np_roll &lt;- autoplot(airpass_roll, RollingMean) +\n  labs(title = \"Rolling Mean\", y = \"Mean\", x = \"Year\") +\n  theme_minimal()\n\np_var &lt;- autoplot(airpass_roll, RollingVar) +\n  labs(title = \"Rolling Variance\", y = \"Variance\", x = \"Year\") +\n  theme_minimal()\n\ngridExtra::grid.arrange(p_roll, p_var, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nSeasonal differencing (lag = 12 for monthly data) removes seasonality.\n\n# Apply log transformation\nairpass_log &lt;- airpass %&gt;% \n  mutate(LogPassengers = log(Passengers))\n\n# Plot log-transformed series\nautoplot(airpass_log, LogPassengers) +\n  labs(title = \"Log-Transformed Air Passengers\", \n       subtitle = \"Stabilizes Variance\",\n       y = \"Log(Passengers)\", x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Apply seasonal differencing (lag = 12 for monthly data)\nairpass_log_diff &lt;- airpass_log %&gt;%\n  mutate(SeasonalDiff = difference(LogPassengers, lag = 12))\n\n# Plot seasonally differenced series\nautoplot(airpass_log_diff, SeasonalDiff) +\n  labs(title = \"Seasonally Differenced Log-Transformed Air Passengers\", \n       subtitle = \"Removes Seasonality\",\n       y = \"Differenced Log(Passengers)\", x = \"Year\") +\n  theme_minimal()",
    "crumbs": [
      "Week 2",
      "Activity 8"
    ]
  },
  {
    "objectID": "activity8.html#seasonal-differencing-rolling-mean-and-rolling-variance",
    "href": "activity8.html#seasonal-differencing-rolling-mean-and-rolling-variance",
    "title": "Activity8",
    "section": "",
    "text": "When a time series exhibits regular, recurring patterns over fixed periods (e.g., 12-month cycles in yearly data), seasonal differencing can help isolate these cyclical components. By subtracting a value from the same season in the previous cycle, you remove much of the repeated seasonal effect:\n\\[\n\\text{SeasonalDiff}_t = Y_t - Y_{t-s},\n\\]\nwhere (s) is the seasonal period (for instance, (s = 12) for monthly data with yearly cycles). This technique is particularly useful for series that show strong periodic fluctuations without requiring other transformations.\n\n\n\nA rolling mean (or moving average) smooths out short-term fluctuations by averaging consecutive observations within a specified window. For a window size (w), the rolling mean at time (t) is:\n\\[\n\\text{RollingMean}_t = \\frac{1}{w} \\sum_{i = t-w+1}^{t} Y_i.\n\\]\nThis technique helps reveal longer-term trends and cyclical behavior in a time series, since each point is now an average of the most recent (w) observations.\n\n\n\nA rolling variance measures how dispersed recent values are around their rolling mean within the same window (w). It is calculated as:\n\\[\n\\text{RollingVar}_t = \\frac{1}{w} \\sum_{i = t-w+1}^{t} \\bigl(Y_i - \\text{RollingMean}_t\\bigr)^2.\n\\]\nObserving the rolling variance over time can highlight periods of increased or decreased fluctuation and help detect heteroscedasticity (changing variance). A more stable rolling variance often indicates that seasonal or cyclic behavior has been accounted for, especially after an appropriate seasonal differencing step.\n\n# Convert AirPassengers to a tsibble\nairpass &lt;- as_tsibble(AirPassengers) %&gt;%\n  rename(Passengers = value)\n\n# Plot the original series\nautoplot(airpass, Passengers) +\n  labs(title = \"Monthly Air Passengers\", \n       subtitle = \"Strong Trend and Seasonality\",\n       y = \"Number of Passengers\", x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Compute and plot ACF\nacf_data &lt;- ACF(airpass, Passengers, lag_max = 36)\nautoplot(acf_data) +\n  labs(title = \"ACF of Air Passengers\", \n       subtitle = \"Slow Decay Indicates Non-Stationarity\",\n       y = \"Autocorrelation\", x = \"Lag\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nRolling statistics (mean and variance) help visualize changes in the series over time. For a stationary series, these should remain roughly constant.\nObservations:\n\nThe rolling mean increases over time, confirming a trend.\nThe rolling variance also increases, indicating heteroscedasticity.\n\n\n# Compute rolling mean and variance\nairpass_roll &lt;- airpass %&gt;%\n  mutate(RollingMean = slider::slide_dbl(Passengers, mean, .before = 11, .complete = TRUE),\n         RollingVar = slider::slide_dbl(Passengers, var, .before = 11, .complete = TRUE))\n\n# Plot rolling statistics\np_roll &lt;- autoplot(airpass_roll, RollingMean) +\n  labs(title = \"Rolling Mean\", y = \"Mean\", x = \"Year\") +\n  theme_minimal()\n\np_var &lt;- autoplot(airpass_roll, RollingVar) +\n  labs(title = \"Rolling Variance\", y = \"Variance\", x = \"Year\") +\n  theme_minimal()\n\ngridExtra::grid.arrange(p_roll, p_var, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nSeasonal differencing (lag = 12 for monthly data) removes seasonality.\n\n# Apply log transformation\nairpass_log &lt;- airpass %&gt;% \n  mutate(LogPassengers = log(Passengers))\n\n# Plot log-transformed series\nautoplot(airpass_log, LogPassengers) +\n  labs(title = \"Log-Transformed Air Passengers\", \n       subtitle = \"Stabilizes Variance\",\n       y = \"Log(Passengers)\", x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Apply seasonal differencing (lag = 12 for monthly data)\nairpass_log_diff &lt;- airpass_log %&gt;%\n  mutate(SeasonalDiff = difference(LogPassengers, lag = 12))\n\n# Plot seasonally differenced series\nautoplot(airpass_log_diff, SeasonalDiff) +\n  labs(title = \"Seasonally Differenced Log-Transformed Air Passengers\", \n       subtitle = \"Removes Seasonality\",\n       y = \"Differenced Log(Passengers)\", x = \"Year\") +\n  theme_minimal()",
    "crumbs": [
      "Week 2",
      "Activity 8"
    ]
  },
  {
    "objectID": "activity8.html#lab-activity-1-stationarity-analysis-nile-dataset",
    "href": "activity8.html#lab-activity-1-stationarity-analysis-nile-dataset",
    "title": "Activity8",
    "section": "Lab Activity 1: Stationarity Analysis (Nile Dataset)",
    "text": "Lab Activity 1: Stationarity Analysis (Nile Dataset)\n\nPlot the Original Series\nVisualize the Nile dataset to detect any potential seasonality or cyclical components. Even if the Nile data do not exhibit strong seasonal patterns, this initial inspection confirms whether or not seasonal differencing might be warranted.\nCompute Rolling Statistics\n\nRolling Mean: Examine how the mean evolves over a chosen window (e.g., 10 or 12 observations). A stationary series should have a stable rolling mean over time.\n\nRolling Variance: Similarly, assess how the variance changes (or remains constant) across the same rolling window. Stationarity requires a relatively constant variance.\n\nSeasonal Differencing\nIf a seasonal pattern is present (e.g., annual cycles in monthly data), remove it by differencing observations separated by one full season. This step is relevant if the data exhibit cyclical or repeating patterns at fixed intervals.\nInterpretation\n\nEvaluate how rolling mean and variance behave post-seasonal differencing (if performed).\nVerify if stationarity indicators (constant mean, constant variance) are satisfied.\n\n\n\n\n\n\n\n\n\n\nReflection Questions:\n\nDoes the Nile dataset exhibit any trends or seasonality?\nHow do the rolling statistics (mean and variance) behave over time?\nWhat transformations, if any, are necessary to achieve stationarity?",
    "crumbs": [
      "Week 2",
      "Activity 8"
    ]
  },
  {
    "objectID": "activity8.html#lab-activity-2-stationarity-analysis-lynx-dataset",
    "href": "activity8.html#lab-activity-2-stationarity-analysis-lynx-dataset",
    "title": "Activity8",
    "section": "Lab Activity 2: Stationarity Analysis (Lynx Dataset)",
    "text": "Lab Activity 2: Stationarity Analysis (Lynx Dataset)\n\nPlot the Original Series\nInvestigate any repeating or cyclical behavior in the annual Lynx data that might hint at seasonal or quasi-seasonal effects.\nCompute Rolling Statistics\n\nRolling Mean: Check if the mean stays relatively stable over a certain window, which supports stationarity.\nRolling Variance: Verify whether variance remains relatively constant over time.\n\nSeasonal Differencing\nIf the Lynx data show cyclical behavior at fixed intervals (e.g., periodic surges), seasonal differencing with an appropriate lag can reduce these cycles to produce a more stationary series.\nInterpretation\n\nCompare rolling mean and variance before and after seasonal differencing to confirm stationarity improvements.\nDiscuss whether the cyclical patterns are effectively removed or diminished.\n\n\n\n\n\n\n\n\n\n\nReflection Questions\n\nDoes the Lynx dataset exhibit any notable trends or cyclic patterns?\nHow do the rolling mean and variance behave over time?\nAre transformations necessary to ensure stationarity?",
    "crumbs": [
      "Week 2",
      "Activity 8"
    ]
  },
  {
    "objectID": "activity23.html",
    "href": "activity23.html",
    "title": "Activity23",
    "section": "",
    "text": "A SARIMA\\((p,d,q)(P,D,Q)_m\\) model combines:\n\nRegular components: AR(p), MA(q) terms for short-term patterns\n\nSeasonal components: Seasonal AR(P), MA(Q) terms at period \\(m\\)\n\nDifferencing: \\(d\\) regular differences + \\(D\\) seasonal differences",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#sarima-model-structure",
    "href": "activity23.html#sarima-model-structure",
    "title": "Activity23",
    "section": "",
    "text": "A SARIMA\\((p,d,q)(P,D,Q)_m\\) model combines:\n\nRegular components: AR(p), MA(q) terms for short-term patterns\n\nSeasonal components: Seasonal AR(P), MA(Q) terms at period \\(m\\)\n\nDifferencing: \\(d\\) regular differences + \\(D\\) seasonal differences",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#strategic-differencing",
    "href": "activity23.html#strategic-differencing",
    "title": "Activity23",
    "section": "2. Strategic Differencing",
    "text": "2. Strategic Differencing\nKey Principle: Use minimal differencing to stabilize mean/variance\n\n# AirPassengers dataset\nap_ts &lt;- tsibble::as_tsibble(AirPassengers) %&gt;% \n  index_by(Date = yearmonth(index)) %&gt;% \n  rename(Passengers = value)\n\n# Automated selection\nap_ts %&gt;% \n  features(Passengers, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))\n\n# A tibble: 1 × 4\n  kpss_stat kpss_pvalue ndiffs nsdiffs\n      &lt;dbl&gt;       &lt;dbl&gt;  &lt;int&gt;   &lt;int&gt;\n1      2.98        0.01      1       0\n\n# Visual check\nap_ts %&gt;%\n  gg_tsdisplay(difference(log(Passengers), lag = 12))\n\n\n\n\n\n\n\n\nInsight: Seasonal differencing (lag=12) removes yearly patterns while preserving monthly trends",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#model-building",
    "href": "activity23.html#model-building",
    "title": "Activity23",
    "section": "3. Model Building",
    "text": "3. Model Building\n\n3.1 Candidate Models\n\nmodels &lt;- ap_ts %&gt;%\n  model(\n    Auto = ARIMA(log(Passengers)),\n    Manual1 = ARIMA(log(Passengers) ~ pdq(1,1,1) + PDQ(0,1,1, period=12)),\n    Manual2 = ARIMA(log(Passengers) ~ pdq(2,1,0) + PDQ(1,1,0, period=12))\n  )\n\nglance(models) %&gt;% arrange(AICc)\n\n# A tibble: 3 × 8\n  .model   sigma2 log_lik   AIC  AICc   BIC ar_roots   ma_roots  \n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;    \n1 Auto    0.00132    250. -489. -489. -475. &lt;cpl [2]&gt;  &lt;cpl [12]&gt;\n2 Manual1 0.00137    245. -482. -482. -470. &lt;cpl [1]&gt;  &lt;cpl [13]&gt;\n3 Manual2 0.00148    241. -474. -473. -462. &lt;cpl [14]&gt; &lt;cpl [0]&gt; \n\n\n\n\n3.2 Coefficient Check\nFor top model:\n\nfinal_model &lt;- models %&gt;% \n  select(Auto)\n\ntidy(models) %&gt;% \n  filter(.model == \"Auto\") \n\n# A tibble: 4 × 6\n  .model term     estimate std.error statistic  p.value\n  &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Auto   ar1        0.575    0.0843       6.83 2.83e-10\n2 Auto   ar2        0.261    0.0842       3.11 2.33e- 3\n3 Auto   sma1      -0.555    0.0771      -7.21 3.99e-11\n4 Auto   constant   0.0193   0.00149     13.0  2.07e-25",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#model-refinement-cycle",
    "href": "activity23.html#model-refinement-cycle",
    "title": "Activity23",
    "section": "4. Model Refinement Cycle",
    "text": "4. Model Refinement Cycle\n\nStart with automatic differencing\n\nCompare multiple model specifications\n\nValidate residuals systematically\n\nIterate using PACF patterns\n\n\n# Final refinement example\nap_ts %&gt;%\n  model(\n    Best = ARIMA(log(Passengers) ~ pdq(1,1,1) + PDQ(0,1,1, period=12))\n  ) %&gt;% \n  report()\n\nSeries: Passengers \nModel: ARIMA(1,1,1)(0,1,1)[12] \nTransformation: log(Passengers) \n\nCoefficients:\n         ar1      ma1     sma1\n      0.1960  -0.5784  -0.5643\ns.e.  0.2475   0.2132   0.0747\n\nsigma^2 estimated as 0.001375:  log likelihood=244.95\nAIC=-481.9   AICc=-481.58   BIC=-470.4",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#lab-activity",
    "href": "activity23.html#lab-activity",
    "title": "Activity23",
    "section": "Lab Activity",
    "text": "Lab Activity",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#data-preparation",
    "href": "activity23.html#data-preparation",
    "title": "Activity23",
    "section": "1. Data Preparation",
    "text": "1. Data Preparation\n\n\n\n\n\n\n\n\nQ1: Examine the seasonal patterns using gg_season(). What type of seasonality dominates this series?",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#model-specification",
    "href": "activity23.html#model-specification",
    "title": "Activity23",
    "section": "2. Model Specification",
    "text": "2. Model Specification\nFit SARIMA(1,1,1)(0,1,1)₁₂ model with maximum likelihood estimation\n\n\n\n\n\n\n\n\nQ2: Interpret the model structure:\n\nWhat does the (1,1,1) non-seasonal component represent?\nWhy do we use PDQ(0,1,1) for seasonal terms?",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#parameter-estimation",
    "href": "activity23.html#parameter-estimation",
    "title": "Activity23",
    "section": "3. Parameter Estimation",
    "text": "3. Parameter Estimation\nTask: Extract and interpret coefficients\n\n\n\n\n\n\n\n\nQ3: Which coefficients are statistically significant (α=0.05)? What does the MA(1) coefficient suggest?",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#residual-diagnostics",
    "href": "activity23.html#residual-diagnostics",
    "title": "Activity23",
    "section": "4. Residual Diagnostics",
    "text": "4. Residual Diagnostics\n\n4.1 Visual Analysis\n\n\n\n\n\n\n\n\nQ4: Do residuals show concerning autocorrelation patterns? Justify your answer.\n\n\n4.2 Formal Tests\n\n\n\n\n\n\n\n\nQ5: Interpret the Ljung-Box (LB) test results:\n\nCan we maintain the white noise assumption?",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "activity23.html#model-validation",
    "href": "activity23.html#model-validation",
    "title": "Activity23",
    "section": "5. Model Validation",
    "text": "5. Model Validation\nTask: Check specification robustness\n\n\n\n\n\n\n\n\nQ6: Compare AICc values across estimation methods. Does our original model remain preferred?",
    "crumbs": [
      "Week 6",
      "Activity 23"
    ]
  },
  {
    "objectID": "template/ActivityTemplateQuarto.html",
    "href": "template/ActivityTemplateQuarto.html",
    "title": "Activity",
    "section": "",
    "text": "# Time Series Essentials, install if needed!\nlibrary(feasts)       # Feature extraction & decomposition\nlibrary(fable)        # Forecasting models (ARIMA, ETS, etc.)\nlibrary(fpp3)         # Tidy time series dataseta\nlibrary(astsa)        # Applied statistical TS methods from textbook\nlibrary(tseries)      # Unit root tests & TS diagnostics\nlibrary(tsibbledata)  # Curated TS datasets\nlibrary(quantmod)     # Financial data retrieval\nlibrary(tidyquant)    # Financial analysis in tidyverse\nlibrary(purrr)        # Functional programming for TS pipelines\nlibrary(readr)        # Efficient data import\n\n\n\n# Your R-code\n\nYour textual explanations .."
  },
  {
    "objectID": "activity27.html",
    "href": "activity27.html",
    "title": "Activity27",
    "section": "",
    "text": "📌 No Activity\n\n📝 In-Class Midterm",
    "crumbs": [
      "Week 7",
      "Activity 27"
    ]
  },
  {
    "objectID": "activity32.html#fitting-comparable-ets-models",
    "href": "activity32.html#fitting-comparable-ets-models",
    "title": "Activity32",
    "section": "Fitting Comparable ETS Models",
    "text": "Fitting Comparable ETS Models\nLet’s compare some ETS specifications including:\n\nSimple exponential smoothing (no trend/seasonality)\nHolt’s linear trend method\nHolt-Winters seasonal method",
    "crumbs": [
      "Week 8",
      "Activity 32"
    ]
  },
  {
    "objectID": "activity32.html#forecast-evaluation-framework",
    "href": "activity32.html#forecast-evaluation-framework",
    "title": "Activity32",
    "section": "Forecast Evaluation Framework",
    "text": "Forecast Evaluation Framework\nWe’ll evaluate using multiple metrics:\n\nMSE: Mean Squared Error (penalizes large errors)\nMAE: Mean Absolute Error (more robust)\nMAPE: Mean Absolute Percentage Error (scale-independent)\nMASE: Mean Absolute Scaled Error (relative to naive forecast)",
    "crumbs": [
      "Week 8",
      "Activity 32"
    ]
  },
  {
    "objectID": "activity32.html#visual-verification",
    "href": "activity32.html#visual-verification",
    "title": "Activity32",
    "section": "Visual Verification",
    "text": "Visual Verification",
    "crumbs": [
      "Week 8",
      "Activity 32"
    ]
  },
  {
    "objectID": "activity32.html#model-diagnostics",
    "href": "activity32.html#model-diagnostics",
    "title": "Activity32",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\nWe should also check residuals:",
    "crumbs": [
      "Week 8",
      "Activity 32"
    ]
  },
  {
    "objectID": "activity32.html#lab-activities",
    "href": "activity32.html#lab-activities",
    "title": "Activity32",
    "section": "Lab Activities",
    "text": "Lab Activities\nFit an ETS(A,A,M) and ARIMA(0,1,1) model, then compare their residuals\nPart A: Fit both models and compare their accuracy using MASE and RMSE\nPart B: Analyze residual diagnostics to determine which model handles autocorrelation better\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nPrompt: “Which model shows better residual properties for supply chain forecasting? Justify using ACF plots and Ljung-Box test statistics.”\n\n\nActivity 2: Transformation Impact Analysis\nPart A: Implement Box-Cox transformation on the Holt-Winters model using \\(\\lambda = 0.2\\) Part B: (Optional) Evaluate if transformation improves forecast interval coverage at 95% level\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nPrompt: “Does variance stabilization help maintain prediction interval reliability during demand spikes?\n\n\nActivity 3: Ensemble Forecasting Strategy\nPart A: Create equal-weighted average of ETS and ARIMA forecasts\nPart B: Verify if ensemble reduces mean error\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nPrompt: “When would an ensemble be particularly valuable for cement production planning? Consider both average and worst-case performance.”",
    "crumbs": [
      "Week 8",
      "Activity 32"
    ]
  },
  {
    "objectID": "activity30.html",
    "href": "activity30.html",
    "title": "Activity30",
    "section": "",
    "text": "Why ETS? The ETS framework explicitly models three core components—level, trend, and seasonality—using a recursive structure. This differs from ARIMA, which typically uses differencing to remove trend or seasonality.\n\nLevel (\\(l_t\\)): Baseline value\n\nTrend (\\(b_t\\)): Persistent upward/downward movement\n\nSeason (\\(s_t\\)): Regular repeating pattern\n\n\n\nFor an Additive error, trend, and seasonality model, often denoted ETS(A,A,A):\n\\[\n\\begin{align}\ny_t &= l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t, \\\\\nl_t &= l_{t-1} + b_{t-1} + \\alpha \\epsilon_t, \\\\\nb_t &= b_{t-1} + \\beta \\epsilon_t, \\\\\ns_t &= s_{t-m} + \\gamma \\epsilon_t,\n\\end{align}\n\\]\nwhere \\(m\\) is the seasonal period (e.g., \\(m=12\\) for monthly data, \\(m=4\\) for quarterly), and \\(\\alpha, \\beta, \\gamma\\) are smoothing parameters. A small \\(\\beta\\) indicates a very slow‐changing trend, while a small \\(\\gamma\\) indicates very stable seasonality.\nKey Differences from ARIMA\n\nETS: Trend/seasonality are explicitly updated.\n\nARIMA: Trend/seasonality are removed by differencing.\n\nETS: Weighted averages via smoothing.\n\nARIMA: Linear combinations of past values and errors.",
    "crumbs": [
      "Week 8",
      "Activity 30"
    ]
  },
  {
    "objectID": "activity30.html#core-concept-time-series-anatomy-with-ets",
    "href": "activity30.html#core-concept-time-series-anatomy-with-ets",
    "title": "Activity30",
    "section": "",
    "text": "Why ETS? The ETS framework explicitly models three core components—level, trend, and seasonality—using a recursive structure. This differs from ARIMA, which typically uses differencing to remove trend or seasonality.\n\nLevel (\\(l_t\\)): Baseline value\n\nTrend (\\(b_t\\)): Persistent upward/downward movement\n\nSeason (\\(s_t\\)): Regular repeating pattern\n\n\n\nFor an Additive error, trend, and seasonality model, often denoted ETS(A,A,A):\n\\[\n\\begin{align}\ny_t &= l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t, \\\\\nl_t &= l_{t-1} + b_{t-1} + \\alpha \\epsilon_t, \\\\\nb_t &= b_{t-1} + \\beta \\epsilon_t, \\\\\ns_t &= s_{t-m} + \\gamma \\epsilon_t,\n\\end{align}\n\\]\nwhere \\(m\\) is the seasonal period (e.g., \\(m=12\\) for monthly data, \\(m=4\\) for quarterly), and \\(\\alpha, \\beta, \\gamma\\) are smoothing parameters. A small \\(\\beta\\) indicates a very slow‐changing trend, while a small \\(\\gamma\\) indicates very stable seasonality.\nKey Differences from ARIMA\n\nETS: Trend/seasonality are explicitly updated.\n\nARIMA: Trend/seasonality are removed by differencing.\n\nETS: Weighted averages via smoothing.\n\nARIMA: Linear combinations of past values and errors.",
    "crumbs": [
      "Week 8",
      "Activity 30"
    ]
  },
  {
    "objectID": "activity30.html#extended-example-tourism-demand-in-sydney",
    "href": "activity30.html#extended-example-tourism-demand-in-sydney",
    "title": "Activity30",
    "section": "2. Extended Example: Tourism Demand in Sydney",
    "text": "2. Extended Example: Tourism Demand in Sydney\nBelow is a quick demonstration of fitting an ETS model to tourism data in Sydney.",
    "crumbs": [
      "Week 8",
      "Activity 30"
    ]
  },
  {
    "objectID": "activity30.html#activity-us-gdp-forecasting",
    "href": "activity30.html#activity-us-gdp-forecasting",
    "title": "Activity30",
    "section": "3. Activity: US GDP Forecasting",
    "text": "3. Activity: US GDP Forecasting\nHere, we illustrate how to scale GDP by dividing by billions and compare ETS vs. ARIMA approaches in a fair manner. We also consider more sophisticated ETS variants (e.g., damped trend).\n\n3.1 Data Preparation & Scaling\n\n\n\n\n\n\n\n\nWe now have a GDP_billions column that is easier to interpret than raw GDP (which can be in the trillions).\n\n\n3.2 Simple ETS vs. ARIMA\nModel A: Simple Exponential Smoothing (SES)\nThis model handles level only (no trend, no seasonality). In ETS notation: ETS(A,N,N).\n\n\n\n\n\n\n\n\nModel B: ARIMA\nWe compare it with a simple differenced ARIMA(0,1,1). That is:\n\\[\ny_t \\;=\\; y_{t-1} \\;+\\; \\epsilon_t \\;+\\; \\theta\\,\\epsilon_{t-1}.\n\\]\n\n\n\n\n\n\n\n\n\n\n3.3 More Sophisticated ETS Models\nTo capture trend, we might consider a damped trend approach, e.g. ETS(A,Ad,N), where:\n\\[\nb_t = \\phi\\,b_{t-1} + \\beta \\epsilon_t, \\quad 0 &lt; \\phi &lt; 1.\n\\]\nThis damping factor \\(\\phi\\) shrinks the trend over time, preventing runaway forecasts.",
    "crumbs": [
      "Week 8",
      "Activity 30"
    ]
  },
  {
    "objectID": "activity30.html#lab-activities",
    "href": "activity30.html#lab-activities",
    "title": "Activity30",
    "section": "Lab Activities",
    "text": "Lab Activities\n\nActivity 1: Simple Exponential Smoothing (SES)\n\nFit an ETS(A,N,N) model to US GDP (in billions).\n\nExtract the smoothing parameter \\(\\alpha\\).\n\nInterpret what \\(\\alpha\\) implies about how quickly the model reacts to new data.\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\nActivity 2: ARIMA with Automatic Selection\n\nUse ARIMA(GDP_billions) with default auto‐selection.\n\nCompare the chosen order \\((p,d,q)\\) with a manually specified ARIMA(0,2,2).\n\nPlot the forecasts to see which better captures the data trend.\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 3: Damped Trend ETS\n\nFit an ETS(A,Ad,N) model.\n\nInspect the damping parameter \\(\\phi\\).\n\nIf \\(\\phi\\) is close to 1, interpret how that affects the forecast horizon.\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 4: Forecast Accuracy Comparison\n\nGenerate 8‐year forecasts with each model (SES, ARIMA, Damped ETS).\n\nCompare the accuracy metrics (RMSE, MAE, MAPE).\n\nConclude which model best fits US GDP data in billions.\n\n\n\nAnswer",
    "crumbs": [
      "Week 8",
      "Activity 30"
    ]
  },
  {
    "objectID": "activity19.html",
    "href": "activity19.html",
    "title": "Activity19",
    "section": "",
    "text": "Mastering ACF & PACF Diagnostics\nObjective: Identify AR(p)/MA(q) signatures through autocorrelation patterns\nThe ACF reveals how auto correlations persist over lags, indicating the need for differencing (if autocorrelations decay slowly) or the presence of seasonality (periodic spikes). The PACF pinpoints the order of AR terms by showing where partial correlations become negligible. Together, ACF and PACF help diagnose appropriate ARIMA or seasonal ARIMA structures.\n\n1.1 ACF Decay Patterns with gtemp_both\n\n# Trend-dominated series\ngtemp_both |&gt; \n  as_tsibble() |&gt;\n  ACF(lag_max = 50) |&gt; \n  autoplot() + \n  labs(subtitle = \"Slow ACF decay indicates non-stationarity\")\n\n\n\n\n\n\n\n# After differencing\ngtemp_both |&gt; \n  as_tsibble() |&gt;\n  mutate(dTemp = difference(value)) |&gt; \n  ACF(dTemp) |&gt; \n  autoplot() +\n  labs(subtitle = \"ACF cuts off after lag 1 -&gt; MA(1) signature\")\n\n\n\n\n\n\n\n\nKey Concept: Persistent ACF decay suggests differencing needed. Sharp cutoff at lag 1 after differencing implies MA(1) component.\n\n\n1.2 PACF for AR Order Identification with pelt\n\n# Lynx population analysis\npelt |&gt; \n  as_tsibble() |&gt;\n  PACF(Lynx, lag_max = 5) |&gt; \n  autoplot() +\n  labs(title = \"PACF cuts off at lag 2 -&gt; AR(2) process\")\n\n\n\n\n\n\n\n\nEquation: AR(p): \\(X_t = \\phi_1X_{t-1} + ... + \\phi_pX_{t-p} + \\epsilon_t\\)\nPACF spikes within first p lags indicate AR order.\n\n\n1.3 Seasonal ACF in vic_elec\n\nvic_elec |&gt; \n  filter(year(Time)==2013) |&gt; \n  ACF(Demand, lag_max=200) |&gt; \n  autoplot() +\n  labs(subtitle = \"Spikes at 48 lags = daily seasonality\")\n\n\n\n\n\n\n\n\nInterpretation: ACF peaks at multiples of fundamental period (48 half-hours = 1 day) reveal seasonality.\n\n\nLab Activity:\n\nLoad the aus_production dataset from fpp3.\n\n\n\n\n\n\n\n\n\n\nChoose the ‘Gas’ time series. Plot its ACF and PACF to identify any AR/MA components or seasonal effects.\n\n\n\n\n\n\n\n\n\n\nApply differencing if needed, and re-check the ACF/PACF to refine your model choice.",
    "crumbs": [
      "Week 5",
      "Activity 19"
    ]
  },
  {
    "objectID": "activity25.html",
    "href": "activity25.html",
    "title": "Activity25",
    "section": "",
    "text": "Theoretical Background: ARIMA(\\(p,d,q\\))(\\(P,D,Q\\))[\\(s\\)] Model Interpretations",
    "crumbs": [
      "Week 7",
      "Activity 25"
    ]
  },
  {
    "objectID": "activity25.html#example-configurations",
    "href": "activity25.html#example-configurations",
    "title": "Activity25",
    "section": "Example Configurations",
    "text": "Example Configurations\n\nARIMA(1,1,1) with Drift (Nonseasonal)\nIn this configuration, we first remove trends by differencing the series, then model the differenced series with one AR term and one MA term.\n\nNonseasonal Differencing: \\[\n\\begin{aligned}\n\\Delta y_t = y_t - y_{t-1}.\n\\end{aligned}\n\\]\nModeling the Differenced Series: \\[\n\\begin{aligned}\n\\Delta y_t &= c + \\phi_1\\,\\Delta y_{t-1} + \\varepsilon_t + \\theta_1\\,\\varepsilon_{t-1}.\n\\end{aligned}\n\\]\n\nKey Points: - The first difference \\(\\Delta y_t\\) helps stabilize the mean. - \\(\\phi_1\\) captures the autoregressive effect on the differenced data. - \\(\\theta_1\\) models the influence of the previous error.\n\n\n\nARIMA(1,0,1)(0,1,1)[4] with Drift (Seasonal)\nThis model introduces seasonal differencing with period 4 and includes nonseasonal AR(1) and MA(1) terms along with a seasonal MA(1) component.\n\nSeasonal Differencing: \\[\n\\begin{aligned}\n\\Delta_4 y_t = y_t - y_{t-4}.\n\\end{aligned}\n\\]\nSequentially Building the Model: \\[\n\\begin{aligned}\ny_t - y_{t-4} &= c + \\phi_1\\,\\Bigl[(y_{t-1} - y_{t-5})\\Bigr] + \\varepsilon_t + \\theta_1\\,\\varepsilon_{t-1} - \\Theta_1\\,\\varepsilon_{t-4}.\n\\end{aligned}\n\\]\n\nKey Points:\n\nSeasonal differencing (\\(y_t - y_{t-4}\\)) removes periodic effects.\n\\(\\phi_1\\) is the nonseasonal AR term applied to the lagged (but also seasonally adjusted) data.\n\\(\\theta_1\\) is the nonseasonal MA term.\n\\(\\Theta_1\\) is the seasonal MA term that adjusts the error from one season ago.\n\\(c\\) is the drift.",
    "crumbs": [
      "Week 7",
      "Activity 25"
    ]
  },
  {
    "objectID": "activity25.html#practice-configurations",
    "href": "activity25.html#practice-configurations",
    "title": "Activity25",
    "section": "Practice Configurations",
    "text": "Practice Configurations\n\nPractice Problem 1\nProblem: Write the sequential equation for a simple AR(1) model with drift.\nAnswer:\n\\[\n\\begin{aligned}\ny_t &= c + \\phi_1\\,y_{t-1} + \\varepsilon_t.\n\\end{aligned}\n\\]\n\n\nPractice Problem 2\nProblem: Write the sequential equation for an ARIMA(0,1,1) model with drift—that is, first-difference the series and then apply an MA(1) model.\nAnswer:\n\nDifferencing: \\[\n\\begin{aligned}\n\\Delta y_t = y_t - y_{t-1}.\n\\end{aligned}\n\\]\nModel Equation: \\[\n\\begin{aligned}\ny_t - y_{t-1} &= c + \\varepsilon_t + \\theta_1\\,\\varepsilon_{t-1}.\n\\end{aligned}\n\\]\n\n\n\nPractice Problem 3\nProblem: Write the sequential equation for a seasonal ARIMA(0,1,1)(1,0,0)[12] model with drift.\n(Hint: This model applies nonseasonal differencing and includes a seasonal AR(1) component with period 12 along with a nonseasonal MA(1) term.)\nAnswer:\n\nNonseasonal Differencing: \\[\n\\begin{aligned}\n\\Delta y_t = y_t - y_{t-1}.\n\\end{aligned}\n\\]\nIncorporate Seasonal AR(1): \\[\n\\begin{aligned}\n(y_t - y_{t-1}) - \\Phi_1\\,(y_{t-12} - y_{t-13}) &= c + \\varepsilon_t + \\theta_1\\,\\varepsilon_{t-1}.\n\\end{aligned}\n\\]\n\nKey Points:\n\n\\(\\Phi_1\\) is the seasonal AR coefficient with period 12.\n\\(\\theta_1\\) is the nonseasonal MA coefficient.\nThe left-hand side shows the nonseasonally differenced data adjusted by the seasonal AR component.",
    "crumbs": [
      "Week 7",
      "Activity 25"
    ]
  },
  {
    "objectID": "googledata.html",
    "href": "googledata.html",
    "title": "Dataset 1: Google Stock Data",
    "section": "",
    "text": "Download raw data\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nAdj.Close\nVolume\n\n\n\n2019-01-02\n50.82850\n52.61600\n50.78550\n52.29250\n52.29250\n30652000\n\n\n2019-01-03\n52.05000\n52.84900\n50.70350\n50.80300\n50.80300\n36822000\n\n\n2019-01-04\n51.62950\n53.54200\n51.37090\n53.53550\n53.53550\n41878000\n\n\n2019-01-07\n53.57500\n53.70000\n52.73800\n53.41950\n53.41950\n39638000\n\n\n2019-01-08\n53.80550\n54.22800\n53.02650\n53.81400\n53.81400\n35298000\n\n\n2019-01-09\n54.08250\n54.13150\n53.32000\n53.73300\n53.73300\n23986000\n\n\n2019-01-10\n53.38300\n53.55750\n52.88550\n53.51650\n53.51650\n29128000\n\n\n2019-01-11\n53.15900\n53.18875\n52.42400\n52.85950\n52.85950\n30416000\n\n\n2019-01-14\n52.34600\n52.57650\n52.06275\n52.23450\n52.23450\n22886000\n\n\n2019-01-15\n52.50850\n54.00250\n52.36700\n53.85750\n53.85750\n29272000\n\n\n2019-01-16\n54.00000\n54.61875\n53.96700\n54.04850\n54.04850\n26636000\n\n\n2019-01-17\n53.97350\n54.59000\n53.67500\n54.49500\n54.49500\n24854000\n\n\n2019-01-18\n55.00000\n55.41760\n54.54500\n54.91300\n54.91300\n39112000\n\n\n2019-01-22\n54.40000\n54.57550\n53.17350\n53.52600\n53.52600\n32270000\n\n\n2019-01-23\n53.86750\n54.24650\n52.98750\n53.77850\n53.77850\n19340000\n\n\n2019-01-24\n53.82400\n53.97375\n53.03500\n53.69500\n53.69500\n27226000\n\n\n2019-01-25\n54.25000\n54.70000\n54.09100\n54.54950\n54.54950\n22382000\n\n\n2019-01-28\n54.00550\n54.15000\n53.19000\n53.50400\n53.50400\n25686000\n\n\n2019-01-29\n53.63400\n53.75750\n52.79325\n53.03100\n53.03100\n20436000\n\n\n2019-01-30\n53.42150\n54.55000\n53.34250\n54.45300\n54.45300\n25596000\n\n\n2019-01-31\n55.15000\n55.86650\n54.77050\n55.81850\n55.81850\n30766000\n\n\n2019-02-01\n55.62000\n56.25000\n55.24450\n55.53750\n55.53750\n29244000\n\n\n2019-02-04\n55.63300\n56.64000\n55.45100\n56.64000\n56.64000\n51530000\n\n\n2019-02-05\n56.24200\n57.34250\n55.86240\n57.29950\n57.29950\n71044000\n\n\n2019-02-06\n56.97850\n57.35000\n55.63850\n55.76150\n55.76150\n42112000\n\n\n2019-02-07\n55.20800\n55.24200\n54.30000\n54.93550\n54.93550\n40896000\n\n\n2019-02-08\n54.35000\n54.94550\n54.32750\n54.75300\n54.75300\n21516000\n\n\n2019-02-11\n54.84750\n55.29725\n54.64300\n54.75050\n54.75050\n21304000\n\n\n2019-02-12\n55.34000\n56.26475\n55.29250\n56.06850\n56.06850\n32182000\n\n\n2019-02-13\n56.24950\n56.73650\n55.92500\n56.00800\n56.00800\n20996000\n\n\n2019-02-14\n55.90250\n56.41150\n55.52225\n56.08350\n56.08350\n18952000\n\n\n2019-02-15\n56.50400\n56.58350\n55.53250\n55.68250\n55.68250\n28996000\n\n\n2019-02-19\n55.50000\n56.09450\n55.50000\n55.92800\n55.92800\n20928000\n\n\n2019-02-20\n55.99950\n56.17050\n55.26400\n55.69000\n55.69000\n21756000\n\n\n2019-02-21\n55.54200\n55.59700\n54.62600\n54.84850\n54.84850\n28302000\n\n\n2019-02-22\n55.04500\n55.56200\n54.78000\n55.51850\n55.51850\n20990000\n\n\n2019-02-25\n55.80000\n55.92700\n55.36350\n55.47000\n55.47000\n28262000\n\n\n2019-02-26\n55.28750\n55.97550\n54.99600\n55.75650\n55.75650\n29426000\n\n\n2019-02-27\n55.34750\n55.89900\n55.05000\n55.80250\n55.80250\n19368000\n\n\n2019-02-28\n55.56500\n56.38250\n55.55050\n55.99600\n55.99600\n30850000\n\n\n2019-03-01\n56.24500\n57.14850\n56.23750\n57.04950\n57.04950\n29006000\n\n\n2019-03-04\n57.34950\n57.91400\n56.53450\n57.39000\n57.39000\n28920000\n\n\n2019-03-05\n57.50300\n58.48050\n57.30975\n58.10150\n58.10150\n28864000\n\n\n2019-03-06\n58.12450\n58.37830\n57.77450\n57.89300\n57.89300\n21986000\n\n\n2019-03-07\n57.78600\n57.83775\n56.74550\n57.16500\n57.16500\n23332000\n\n\n2019-03-08\n56.33650\n57.35400\n56.16500\n57.11600\n57.11600\n24248000\n\n\n2019-03-11\n57.22250\n58.80950\n57.22250\n58.78800\n58.78800\n34384000\n\n\n2019-03-12\n58.91300\n60.00000\n58.91300\n59.66000\n59.66000\n40262000\n\n\n2019-03-13\n60.03225\n60.04650\n59.59700\n59.66600\n59.66600\n28718000\n\n\n2019-03-14\n59.72550\n59.89400\n59.22400\n59.27750\n59.27750\n23456000\n\n\n2019-03-15\n59.66900\n59.82850\n59.13050\n59.22300\n59.22300\n49236000\n\n\n2019-03-18\n59.16500\n59.50000\n58.87105\n59.21300\n59.21300\n25852000\n\n\n2019-03-19\n59.44050\n60.00000\n59.29350\n59.94250\n59.94250\n30414000\n\n\n2019-03-20\n59.86750\n61.35700\n59.80850\n61.19850\n61.19850\n44548000\n\n\n2019-03-21\n60.80000\n61.58950\n60.65750\n61.57700\n61.57700\n24080000\n\n\n2019-03-22\n61.31600\n61.50000\n60.14125\n60.27500\n60.27500\n34284000\n\n\n2019-03-25\n59.84650\n60.31990\n59.35200\n59.65000\n59.65000\n29936000\n\n\n2019-03-26\n59.92650\n60.14150\n58.83600\n59.23100\n59.23100\n38024000\n\n\n2019-03-27\n59.27500\n59.37795\n57.96850\n58.65100\n58.65100\n28004000\n\n\n2019-03-28\n58.57700\n58.57825\n57.97155\n58.42450\n58.42450\n19234000\n\n\n2019-03-29\n58.74500\n58.94950\n58.14400\n58.66550\n58.66550\n25398000\n\n\n2019-04-01\n59.20500\n59.83300\n59.10000\n59.72150\n59.72150\n25050000\n\n\n2019-04-02\n59.76600\n60.06750\n59.28550\n60.02450\n60.02450\n16558000\n\n\n2019-04-03\n60.37400\n60.81500\n60.02500\n60.29600\n60.29600\n20286000\n\n\n2019-04-04\n60.29700\n60.78350\n60.20650\n60.75000\n60.75000\n19000000\n\n\n2019-04-05\n60.74950\n60.81100\n60.25150\n60.35750\n60.35750\n18144000\n\n\n2019-04-08\n60.39450\n60.43450\n59.99300\n60.19200\n60.19200\n17204000\n\n\n2019-04-09\n59.80000\n60.11450\n59.65400\n59.86250\n59.86250\n17528000\n\n\n2019-04-10\n60.03400\n60.18925\n59.82175\n60.10800\n60.10800\n14492000\n\n\n2019-04-11\n60.19800\n60.39800\n60.00650\n60.23100\n60.23100\n14204000\n\n\n2019-04-12\n60.50000\n60.91750\n60.40550\n60.89350\n60.89350\n18668000\n\n\n2019-04-15\n60.90000\n61.21000\n60.45550\n61.05500\n61.05500\n23748000\n\n\n2019-04-16\n61.25000\n61.54100\n61.00600\n61.35650\n61.35650\n17126000\n\n\n2019-04-17\n61.65000\n62.02800\n61.39100\n61.81700\n61.81700\n24438000\n\n\n2019-04-18\n61.95900\n62.10000\n61.73050\n61.81850\n61.81850\n26636000\n\n\n2019-04-22\n61.79950\n62.45450\n61.41550\n62.44200\n62.44200\n16146000\n\n\n2019-04-23\n62.53450\n63.45000\n62.31900\n63.22750\n63.22750\n26398000\n\n\n2019-04-24\n63.20600\n63.40050\n62.75000\n62.80000\n62.80000\n20376000\n\n\n2019-04-25\n63.23850\n63.37040\n62.60150\n63.17250\n63.17250\n22146000\n\n\n2019-04-26\n63.45000\n63.65350\n63.01600\n63.60900\n63.60900\n24828000\n\n\n2019-04-29\n63.70000\n64.46350\n63.31475\n64.37900\n64.37900\n49988000\n\n\n2019-04-30\n59.25000\n59.64050\n58.75000\n59.42400\n59.42400\n124140000\n\n\n2019-05-01\n59.40250\n59.40250\n58.35900\n58.40400\n58.40400\n52784000\n\n\n2019-05-02\n58.38800\n58.70950\n57.75010\n58.13050\n58.13050\n38896000\n\n\n2019-05-03\n58.68250\n59.34000\n58.45000\n59.27000\n59.27000\n39614000\n\n\n2019-05-06\n58.31300\n59.54250\n58.31300\n59.46950\n59.46950\n31278000\n\n\n2019-05-07\n59.02350\n59.52200\n58.05200\n58.70500\n58.70500\n31028000\n\n\n2019-05-08\n58.60050\n59.02120\n58.28700\n58.31350\n58.31350\n26186000\n\n\n2019-05-09\n57.95150\n58.48300\n57.54250\n58.11900\n58.11900\n23714000\n\n\n2019-05-10\n58.17950\n58.63000\n57.12500\n58.21350\n58.21350\n26290000\n\n\n2019-05-13\n57.09800\n57.39700\n56.10550\n56.60150\n56.60150\n37212000\n\n\n2019-05-14\n56.86050\n57.02100\n55.97750\n56.02200\n56.02200\n36732000\n\n\n2019-05-15\n55.89350\n58.56650\n55.83330\n58.21050\n58.21050\n45786000\n\n\n2019-05-16\n58.22550\n59.40800\n58.14200\n58.94900\n58.94900\n30628000\n\n\n2019-05-17\n58.42350\n59.00750\n58.00050\n58.11500\n58.11500\n24172000\n\n\n2019-05-20\n57.22500\n57.33985\n56.57215\n56.94250\n56.94250\n27066000\n\n\n2019-05-21\n57.42450\n57.63540\n56.89700\n57.48150\n57.48150\n23196000\n\n\n2019-05-22\n57.33750\n57.92600\n57.29450\n57.57100\n57.57100\n18290000\n\n\n2019-05-23\n57.02500\n57.29865\n56.46120\n57.03850\n57.03850\n23978000\n\n\n2019-05-24\n57.36800\n57.48825\n56.58300\n56.67350\n56.67350\n22240000\n\n\n2019-05-28\n56.70000\n57.57935\n56.65600\n56.70750\n56.70750\n27300000\n\n\n2019-05-29\n56.37600\n56.45500\n55.41100\n55.82300\n55.82300\n30764000\n\n\n2019-05-30\n55.77700\n56.15650\n55.60600\n55.89750\n55.89750\n19038000\n\n\n2019-05-31\n55.06450\n55.48000\n55.00900\n55.18150\n55.18150\n30156000\n\n\n2019-06-03\n53.27500\n53.27500\n51.25000\n51.81150\n51.81150\n102612000\n\n\n2019-06-04\n52.14500\n52.80250\n51.68450\n52.65250\n52.65250\n56670000\n\n\n2019-06-05\n52.57700\n52.67750\n51.52450\n52.11100\n52.11100\n43368000\n\n\n2019-06-06\n52.24950\n52.37450\n51.68500\n52.21700\n52.21700\n34064000\n\n\n2019-06-07\n52.53150\n53.54600\n52.42000\n53.30200\n53.30200\n36048000\n\n\n2019-06-10\n53.64900\n54.63300\n53.61610\n54.01900\n54.01900\n29284000\n\n\n2019-06-11\n54.69900\n55.09950\n53.88015\n53.93600\n53.93600\n28734000\n\n\n2019-06-12\n53.90000\n54.04650\n53.37700\n53.85150\n53.85150\n21220000\n\n\n2019-06-13\n54.18200\n54.70850\n54.00750\n54.43850\n54.43850\n21154000\n\n\n2019-06-14\n54.32100\n54.63450\n54.00860\n54.26750\n54.26750\n22230000\n\n\n2019-06-17\n54.31400\n54.95900\n54.31400\n54.62500\n54.62500\n18832000\n\n\n2019-06-18\n55.48450\n55.81950\n54.94950\n55.18000\n55.18000\n27734000\n\n\n2019-06-19\n55.28000\n55.35000\n54.67400\n55.11650\n55.11650\n26776000\n\n\n2019-06-20\n55.99950\n56.00600\n55.23700\n55.57100\n55.57100\n25240000\n\n\n2019-06-21\n55.46200\n56.20550\n55.40400\n56.09400\n56.09400\n38952000\n\n\n2019-06-24\n55.98050\n56.10000\n55.55050\n55.77600\n55.77600\n27912000\n\n\n2019-06-25\n55.63300\n55.71750\n54.19000\n54.31750\n54.31750\n30938000\n\n\n2019-06-26\n54.32500\n54.64850\n53.61200\n53.99000\n53.99000\n36218000\n\n\n2019-06-27\n54.20000\n54.35500\n53.76450\n53.80050\n53.80050\n20086000\n\n\n2019-06-28\n53.81950\n54.05000\n53.66850\n54.04550\n54.04550\n33864000\n\n\n2019-07-01\n54.90000\n55.37900\n54.68515\n54.89750\n54.89750\n28726000\n\n\n2019-07-02\n55.11200\n55.58850\n54.90850\n55.56250\n55.56250\n19832000\n\n\n2019-07-03\n55.87050\n56.33800\n55.69300\n56.07900\n56.07900\n15340000\n\n\n2019-07-05\n55.89000\n56.64400\n55.80700\n56.57950\n56.57950\n25286000\n\n\n2019-07-08\n56.25850\n56.29900\n55.56050\n55.81750\n55.81750\n24728000\n\n\n2019-07-09\n55.59000\n56.40125\n55.35850\n56.24150\n56.24150\n26608000\n\n\n2019-07-10\n56.56100\n57.10250\n56.54850\n57.02400\n57.02400\n24190000\n\n\n2019-07-11\n57.16250\n57.65350\n56.97900\n57.21050\n57.21050\n23910000\n\n\n2019-07-12\n57.19950\n57.36700\n56.93900\n57.24500\n57.24500\n17280000\n\n\n2019-07-15\n57.34300\n57.54100\n56.97000\n57.51700\n57.51700\n18076000\n\n\n2019-07-16\n57.30000\n57.92900\n57.25000\n57.67900\n57.67900\n24776000\n\n\n2019-07-17\n57.54850\n57.91800\n57.28850\n57.31750\n57.31750\n23400000\n\n\n2019-07-18\n57.08700\n57.38025\n56.63650\n57.31650\n57.31650\n25814000\n\n\n2019-07-19\n57.40950\n57.55700\n56.48100\n56.50500\n56.50500\n32944000\n\n\n2019-07-22\n56.67250\n56.96250\n56.21200\n56.90350\n56.90350\n26030000\n\n\n2019-07-23\n57.20000\n57.34500\n56.59000\n57.31050\n57.31050\n21874000\n\n\n2019-07-24\n56.59500\n57.20000\n56.34950\n56.89050\n56.89050\n31796000\n\n\n2019-07-25\n56.89100\n57.08500\n56.04600\n56.60600\n56.60600\n44196000\n\n\n2019-07-26\n61.20200\n63.27750\n61.20000\n62.52050\n62.52050\n96116000\n\n\n2019-07-29\n62.05250\n62.36850\n61.41150\n61.97050\n61.97050\n44474000\n\n\n2019-07-30\n61.27050\n61.74350\n61.16500\n61.25700\n61.25700\n29066000\n\n\n2019-07-31\n61.15000\n61.70000\n60.38820\n60.83400\n60.83400\n34510000\n\n\n2019-08-01\n60.70150\n61.70550\n60.28600\n60.45050\n60.45050\n33970000\n\n\n2019-08-02\n60.03700\n60.34500\n59.44700\n59.69950\n59.69950\n32902000\n\n\n2019-08-05\n58.50200\n58.76200\n57.00700\n57.61600\n57.61600\n51950000\n\n\n2019-08-06\n58.16550\n58.99800\n58.00000\n58.49750\n58.49750\n34188000\n\n\n2019-08-07\n57.80000\n58.92225\n57.48120\n58.69950\n58.69950\n28886000\n\n\n2019-08-08\n59.14150\n60.25050\n58.65100\n60.24000\n60.24000\n29360000\n\n\n2019-08-09\n59.89950\n60.19400\n59.18015\n59.40050\n59.40050\n21314000\n\n\n2019-08-12\n58.96050\n59.24800\n58.38360\n58.73550\n58.73550\n20060000\n\n\n2019-08-13\n58.57300\n60.23900\n58.57300\n59.86350\n59.86350\n25888000\n\n\n2019-08-14\n58.81550\n59.11500\n58.02700\n58.21450\n58.21450\n31574000\n\n\n2019-08-15\n58.17500\n58.79200\n58.10550\n58.36300\n58.36300\n24374000\n\n\n2019-08-16\n58.97750\n59.13600\n58.59050\n58.88000\n58.88000\n26266000\n\n\n2019-08-19\n59.50450\n60.34950\n59.50450\n59.92250\n59.92250\n24632000\n\n\n2019-08-20\n59.76250\n59.80300\n59.10550\n59.13450\n59.13450\n18310000\n\n\n2019-08-21\n59.65750\n59.95000\n59.37150\n59.56250\n59.56250\n14814000\n\n\n2019-08-22\n59.70350\n59.90060\n58.92900\n59.47650\n59.47650\n18950000\n\n\n2019-08-23\n59.09950\n59.70400\n57.38750\n57.56450\n57.56450\n33740000\n\n\n2019-08-26\n57.86300\n58.47350\n57.64800\n58.44450\n58.44450\n24522000\n\n\n2019-08-27\n59.02650\n59.12000\n58.07250\n58.39200\n58.39200\n21544000\n\n\n2019-08-28\n58.08550\n58.82100\n57.86500\n58.55100\n58.55100\n16040000\n\n\n2019-08-29\n59.05600\n59.80300\n59.05600\n59.64250\n59.64250\n21768000\n\n\n2019-08-30\n59.92500\n59.92500\n59.19015\n59.40500\n59.40500\n22596000\n\n\n2019-09-03\n58.85150\n59.34450\n58.16000\n58.41950\n58.41950\n29598000\n\n\n2019-09-04\n58.83550\n59.17400\n58.55000\n59.07050\n59.07050\n21378000\n\n\n2019-09-05\n59.57650\n60.65200\n59.57650\n60.56900\n60.56900\n28162000\n\n\n2019-09-06\n60.40650\n60.60075\n60.12610\n60.24650\n60.24650\n21442000\n\n\n2019-09-09\n60.20000\n61.00000\n59.63100\n60.22050\n60.22050\n29438000\n\n\n2019-09-10\n59.75750\n60.50000\n59.72900\n60.30000\n60.30000\n25202000\n\n\n2019-09-11\n60.17050\n61.13000\n60.11000\n61.00850\n61.00850\n26140000\n\n\n2019-09-12\n61.21500\n62.09300\n61.15100\n61.71250\n61.71250\n34518000\n\n\n2019-09-13\n61.56750\n62.04400\n61.35050\n61.97800\n61.97800\n26028000\n\n\n2019-09-16\n61.47600\n61.97800\n61.28050\n61.56500\n61.56500\n21066000\n\n\n2019-09-17\n61.52000\n61.75000\n61.18450\n61.45750\n61.45750\n19102000\n\n\n2019-09-18\n61.37550\n61.78050\n60.82650\n61.62050\n61.62050\n22702000\n\n\n2019-09-19\n61.60300\n62.22200\n61.60100\n61.93550\n61.93550\n19920000\n\n\n2019-09-20\n61.65600\n62.16600\n61.15400\n61.49650\n61.49650\n45400000\n\n\n2019-09-23\n61.30000\n61.95450\n61.20850\n61.70150\n61.70150\n21248000\n\n\n2019-09-24\n62.00000\n62.33700\n60.53400\n60.93800\n60.93800\n31664000\n\n\n2019-09-25\n60.79100\n62.41500\n60.50450\n62.32600\n62.32600\n29060000\n\n\n2019-09-26\n62.09800\n62.25000\n61.61340\n62.06950\n62.06950\n30760000\n\n\n2019-09-27\n62.15050\n62.20100\n60.72250\n61.25450\n61.25450\n27078000\n\n\n2019-09-30\n61.04850\n61.30000\n60.61500\n60.95000\n60.95000\n28082000\n\n\n2019-10-01\n60.95000\n61.56150\n60.17900\n60.25500\n60.25500\n25470000\n\n\n2019-10-02\n59.84900\n59.84900\n58.56450\n58.83150\n58.83150\n32302000\n\n\n2019-10-03\n59.00000\n59.45300\n58.12150\n59.39150\n59.39150\n32424000\n\n\n2019-10-04\n59.59450\n60.57200\n59.45850\n60.45000\n60.45000\n23248000\n\n\n2019-10-07\n60.22000\n60.91020\n60.18750\n60.38400\n60.38400\n16858000\n\n\n2019-10-08\n59.87950\n60.30400\n59.45050\n59.45650\n59.45650\n20786000\n\n\n2019-10-09\n59.96750\n60.41750\n59.88150\n60.11550\n60.11550\n17354000\n\n\n2019-10-10\n59.92900\n60.75000\n59.86700\n60.43350\n60.43350\n16932000\n\n\n2019-10-11\n61.11050\n61.41950\n60.68700\n60.77250\n60.77250\n25454000\n\n\n2019-10-14\n60.61700\n61.31650\n60.58800\n60.85700\n60.85700\n17350000\n\n\n2019-10-15\n61.02000\n62.36650\n61.02000\n62.15050\n62.15050\n27634000\n\n\n2019-10-16\n62.05850\n62.73700\n61.92250\n62.18200\n62.18200\n21892000\n\n\n2019-10-17\n62.54650\n63.16625\n62.49700\n62.65350\n62.65350\n19048000\n\n\n2019-10-18\n62.67300\n62.94450\n62.05400\n62.27450\n62.27450\n27056000\n\n\n2019-10-21\n62.61300\n62.73145\n62.03000\n62.30750\n62.30750\n20544000\n\n\n2019-10-22\n62.39250\n62.53000\n62.06900\n62.14000\n62.14000\n20476000\n\n\n2019-10-23\n62.11800\n62.99450\n62.11800\n62.95650\n62.95650\n18230000\n\n\n2019-10-24\n63.04500\n63.20000\n62.68575\n63.04950\n63.04950\n20562000\n\n\n2019-10-25\n62.55150\n63.48000\n62.50050\n63.25650\n63.25650\n24262000\n\n\n2019-10-28\n63.77250\n64.96550\n63.62700\n64.50000\n64.50000\n52264000\n\n\n2019-10-29\n63.81150\n64.07950\n62.86060\n63.13100\n63.13100\n37728000\n\n\n2019-10-30\n62.64850\n63.46800\n62.60000\n63.06450\n63.06450\n28178000\n\n\n2019-10-31\n63.06400\n63.38350\n62.54215\n63.00550\n63.00550\n29114000\n\n\n2019-11-01\n63.25000\n63.73100\n63.02500\n63.68700\n63.68700\n33402000\n\n\n2019-11-04\n63.82250\n64.70650\n63.81775\n64.56850\n64.56850\n30020000\n\n\n2019-11-05\n64.64450\n64.94650\n64.56145\n64.60150\n64.60150\n25654000\n\n\n2019-11-06\n64.47300\n64.68650\n64.12500\n64.59000\n64.59000\n23060000\n\n\n2019-11-07\n64.71400\n66.18700\n64.71225\n65.44300\n65.44300\n40600000\n\n\n2019-11-08\n65.26400\n65.90000\n65.21825\n65.56850\n65.56850\n25028000\n\n\n2019-11-11\n65.15900\n65.32125\n64.87050\n64.95950\n64.95950\n20238000\n\n\n2019-11-12\n65.00000\n65.50000\n64.78850\n64.94000\n64.94000\n21718000\n\n\n2019-11-13\n64.70350\n65.21500\n64.67550\n64.90000\n64.90000\n16534000\n\n\n2019-11-14\n64.87500\n65.85000\n64.78250\n65.57300\n65.57300\n23870000\n\n\n2019-11-15\n65.94700\n66.74400\n65.71400\n66.74350\n66.74350\n35652000\n\n\n2019-11-18\n66.61100\n66.77645\n65.87500\n66.03500\n66.03500\n29748000\n\n\n2019-11-19\n66.38500\n66.38500\n65.64000\n65.77300\n65.77300\n25384000\n\n\n2019-11-20\n65.58700\n65.75000\n64.55750\n65.15250\n65.15250\n26172000\n\n\n2019-11-21\n65.07400\n65.62950\n64.65000\n65.06750\n65.06750\n19910000\n\n\n2019-11-22\n65.28100\n65.43650\n64.57050\n64.76700\n64.76700\n27714000\n\n\n2019-11-25\n64.95900\n65.56550\n64.90650\n65.33450\n65.33450\n20724000\n\n\n2019-11-26\n65.49300\n65.74000\n65.25450\n65.67750\n65.67750\n21394000\n\n\n2019-11-27\n65.75000\n65.91800\n65.48150\n65.64950\n65.64950\n19912000\n\n\n2019-11-29\n65.35600\n65.51025\n65.19850\n65.24800\n65.24800\n11740000\n\n\n2019-12-02\n65.05000\n65.29150\n64.05000\n64.49600\n64.49600\n30218000\n\n\n2019-12-03\n63.97850\n64.92305\n63.95000\n64.76400\n64.76400\n22876000\n\n\n2019-12-04\n65.35050\n66.29000\n65.24350\n66.02700\n66.02700\n30750000\n\n\n2019-12-05\n66.40000\n66.46790\n65.82200\n66.40650\n66.40650\n24254000\n\n\n2019-12-06\n66.67200\n67.20000\n66.67200\n67.03100\n67.03100\n26296000\n\n\n2019-12-09\n66.90200\n67.97250\n66.89200\n67.17800\n67.17800\n27086000\n\n\n2019-12-10\n67.07500\n67.49875\n66.80200\n67.23300\n67.23300\n21882000\n\n\n2019-12-11\n67.54200\n67.56000\n67.13350\n67.25100\n67.25100\n17008000\n\n\n2019-12-12\n67.29700\n67.78875\n67.02500\n67.51350\n67.51350\n25620000\n\n\n2019-12-13\n67.39750\n67.65465\n67.19350\n67.39150\n67.39150\n30992000\n\n\n2019-12-16\n67.82500\n68.23400\n67.63350\n68.05850\n68.05850\n27946000\n\n\n2019-12-17\n68.14450\n68.25000\n67.56615\n67.75600\n67.75600\n37080000\n\n\n2019-12-18\n67.83000\n68.02350\n67.55000\n67.63100\n67.63100\n30452000\n\n\n2019-12-19\n67.59100\n67.90500\n67.44925\n67.80200\n67.80200\n29398000\n\n\n2019-12-20\n68.16750\n68.18200\n67.45000\n67.47950\n67.47950\n66300000\n\n\n2019-12-23\n67.79350\n67.99000\n67.32550\n67.44200\n67.44200\n17662000\n\n\n2019-12-24\n67.42500\n67.51300\n67.13900\n67.17800\n67.17800\n6950000\n\n\n2019-12-26\n67.30850\n68.06635\n67.22350\n68.02000\n68.02000\n13350000\n\n\n2019-12-27\n68.14950\n68.22650\n67.46550\n67.59450\n67.59450\n20768000\n\n\n2019-12-30\n67.50000\n67.65000\n66.70100\n66.80700\n66.80700\n21018000\n\n\n2019-12-31\n66.50550\n66.90000\n66.45425\n66.85100\n66.85100\n19236000\n\n\n2020-01-02\n67.07750\n68.40700\n67.07750\n68.36850\n68.36850\n28132000\n\n\n2020-01-03\n67.39300\n68.62500\n67.27720\n68.03300\n68.03300\n23728000\n\n\n2020-01-06\n67.50000\n69.82500\n67.50000\n69.71050\n69.71050\n34646000\n\n\n2020-01-07\n69.89700\n70.14950\n69.51900\n69.66700\n69.66700\n30054000\n\n\n2020-01-08\n69.60400\n70.57900\n69.54200\n70.21600\n70.21600\n30560000\n\n\n2020-01-09\n71.02850\n71.36650\n70.51350\n70.99150\n70.99150\n30018000\n\n\n2020-01-10\n71.37800\n71.74645\n70.91750\n71.48650\n71.48650\n36414000\n\n\n2020-01-13\n71.80650\n72.02600\n71.30100\n71.96150\n71.96150\n33046000\n\n\n2020-01-14\n71.95050\n72.09000\n71.41850\n71.54400\n71.54400\n31178000\n\n\n2020-01-15\n71.51050\n72.06975\n71.51050\n71.96000\n71.96000\n25654000\n\n\n2020-01-16\n72.37200\n72.59950\n72.04600\n72.58500\n72.58500\n23474000\n\n\n2020-01-17\n73.14550\n74.06475\n72.91100\n74.01950\n74.01950\n47924000\n\n\n2020-01-21\n73.95600\n74.59250\n73.56000\n74.22000\n74.22000\n40734000\n\n\n2020-01-22\n74.55000\n75.16070\n74.24650\n74.29750\n74.29750\n32216000\n\n\n2020-01-23\n74.38200\n74.77600\n74.10500\n74.33250\n74.33250\n27024000\n\n\n2020-01-24\n74.67950\n74.77475\n73.26250\n73.33550\n73.33550\n35692000\n\n\n2020-01-27\n71.55000\n71.90350\n71.06000\n71.69500\n71.69500\n35104000\n\n\n2020-01-28\n72.15000\n72.80000\n71.62350\n72.62800\n72.62800\n31548000\n\n\n2020-01-29\n72.94000\n73.27150\n72.33700\n72.93150\n72.93150\n21554000\n\n\n2020-01-30\n71.99800\n72.86400\n71.82000\n72.79200\n72.79200\n26788000\n\n\n2020-01-31\n73.44500\n73.50650\n71.42650\n71.71150\n71.71150\n48344000\n\n\n2020-02-03\n73.10000\n74.50000\n72.94950\n74.29700\n74.29700\n60736000\n\n\n2020-02-04\n72.85350\n73.47500\n71.31500\n72.35350\n72.35350\n78660000\n\n\n2020-02-05\n73.12100\n73.19200\n71.52800\n72.41150\n72.41150\n39724000\n\n\n2020-02-06\n72.51650\n74.10000\n72.47850\n73.81150\n73.81150\n33588000\n\n\n2020-02-07\n73.36500\n74.29200\n73.31750\n73.96150\n73.96150\n23446000\n\n\n2020-02-10\n73.71600\n75.47500\n73.71600\n75.43400\n75.43400\n28398000\n\n\n2020-02-11\n75.59050\n76.48150\n75.28190\n75.43950\n75.43950\n26892000\n\n\n2020-02-12\n75.72400\n76.03475\n75.40550\n75.91350\n75.91350\n23352000\n\n\n2020-02-13\n75.63450\n76.35900\n75.23000\n75.73300\n75.73300\n18590000\n\n\n2020-02-14\n75.78000\n76.03700\n75.36700\n76.03700\n76.03700\n23956000\n\n\n2020-02-18\n75.75000\n76.58150\n75.62950\n75.98350\n75.98350\n22414000\n\n\n2020-02-19\n76.25350\n76.60530\n76.07000\n76.33450\n76.33450\n18986000\n\n\n2020-02-20\n76.10000\n76.48200\n75.34100\n75.90750\n75.90750\n21932000\n\n\n2020-02-21\n75.40150\n75.61075\n74.02200\n74.25550\n74.25550\n34634000\n\n\n2020-02-24\n71.30550\n71.84850\n70.56950\n71.07950\n71.07950\n57342000\n\n\n2020-02-25\n71.65000\n71.90700\n69.12000\n69.42250\n69.42250\n49566000\n\n\n2020-02-26\n69.80700\n70.78500\n68.95000\n69.65900\n69.65900\n44048000\n\n\n2020-02-27\n68.10300\n68.58520\n65.85850\n65.90450\n65.90450\n59566000\n\n\n2020-02-28\n63.87500\n67.05700\n63.55000\n66.96650\n66.96650\n75782000\n\n\n2020-03-02\n67.58050\n69.54350\n66.34075\n69.45550\n69.45550\n48630000\n\n\n2020-03-03\n69.97100\n70.50750\n66.60000\n67.06950\n67.06950\n48046000\n\n\n2020-03-04\n67.96150\n69.40450\n67.15550\n69.32600\n69.32600\n38266000\n\n\n2020-03-05\n67.51000\n67.94550\n65.25500\n65.95200\n65.95200\n51226000\n\n\n2020-03-06\n63.85300\n65.31100\n63.05250\n64.92050\n64.92050\n53212000\n\n\n2020-03-09\n60.26500\n62.73800\n60.00000\n60.77800\n60.77800\n67308000\n\n\n2020-03-10\n63.00000\n64.05750\n60.93850\n64.01950\n64.01950\n52228000\n\n\n2020-03-11\n62.48500\n63.04800\n59.80350\n60.77050\n60.77050\n52170000\n\n\n2020-03-12\n56.30000\n59.69350\n55.66500\n55.74550\n55.74550\n84534000\n\n\n2020-03-13\n58.95000\n60.98800\n55.85715\n60.98650\n60.98650\n74002000\n\n\n2020-03-16\n54.80000\n57.61335\n53.72200\n54.21650\n54.21650\n85048000\n\n\n2020-03-17\n54.65550\n56.54300\n52.80050\n55.99000\n55.99000\n77230000\n\n\n2020-03-18\n52.82550\n55.32500\n51.86400\n54.84000\n54.84000\n84668000\n\n\n2020-03-19\n54.65250\n57.89850\n53.00540\n55.76450\n55.76450\n73022000\n\n\n2020-03-20\n56.78600\n57.19950\n53.27450\n53.61600\n53.61600\n72036000\n\n\n2020-03-23\n53.06600\n53.56600\n50.67680\n52.83100\n52.83100\n80882000\n\n\n2020-03-24\n55.18850\n56.75000\n54.53100\n56.72300\n56.72300\n66890000\n\n\n2020-03-25\n56.32350\n57.44500\n54.30050\n55.12450\n55.12450\n81630000\n\n\n2020-03-26\n55.59000\n58.49850\n54.67650\n58.08750\n58.08750\n71434000\n\n\n2020-03-27\n56.28350\n57.53350\n55.29550\n55.53550\n55.53550\n64170000\n\n\n2020-03-30\n56.25200\n57.58150\n54.82400\n57.34100\n57.34100\n51482000\n\n\n2020-03-31\n57.36500\n58.76550\n56.90700\n58.14050\n58.14050\n49728000\n\n\n2020-04-01\n56.10000\n56.48450\n54.87250\n55.28100\n55.28100\n46884000\n\n\n2020-04-02\n54.91300\n56.34300\n54.82000\n56.04200\n56.04200\n39298000\n\n\n2020-04-03\n55.95075\n56.17700\n53.99050\n54.89400\n54.89400\n46268000\n\n\n2020-04-06\n56.90000\n59.73300\n56.54700\n59.34600\n59.34600\n53294000\n\n\n2020-04-07\n61.05000\n61.25000\n59.11150\n59.32550\n59.32550\n47746000\n\n\n2020-04-08\n60.32500\n60.95350\n59.40800\n60.51400\n60.51400\n39502000\n\n\n2020-04-09\n61.20400\n61.27850\n59.83675\n60.57250\n60.57250\n43508000\n\n\n2020-04-13\n60.45900\n61.02550\n59.37990\n60.87800\n60.87800\n34796000\n\n\n2020-04-14\n62.25450\n64.10350\n61.84650\n63.46150\n63.46150\n49408000\n\n\n2020-04-15\n62.28050\n64.02300\n62.02000\n63.12350\n63.12350\n33434000\n\n\n2020-04-16\n63.70500\n63.95000\n62.13100\n63.17350\n63.17350\n50362000\n\n\n2020-04-17\n64.24250\n64.72150\n63.56150\n64.16250\n64.16250\n38980000\n\n\n2020-04-20\n63.55000\n64.08000\n63.06850\n63.33050\n63.33050\n33910000\n\n\n2020-04-21\n62.35000\n62.71350\n60.48550\n60.81700\n60.81700\n43060000\n\n\n2020-04-22\n62.27700\n64.28065\n62.10000\n63.16050\n63.16050\n41848000\n\n\n2020-04-23\n63.57750\n64.66550\n63.28350\n63.81550\n63.81550\n31324000\n\n\n2020-04-24\n63.05850\n64.02000\n62.47250\n63.96550\n63.96550\n32792000\n\n\n2020-04-27\n64.80000\n64.80750\n63.45000\n63.79400\n63.79400\n32012000\n\n\n2020-04-28\n64.39650\n64.40250\n61.61000\n61.68350\n61.68350\n59026000\n\n\n2020-04-29\n67.07300\n67.99950\n66.26700\n67.07400\n67.07400\n75872000\n\n\n2020-04-30\n66.24400\n67.64100\n66.12450\n67.43300\n67.43300\n53378000\n\n\n2020-05-01\n66.42500\n67.60350\n65.55000\n66.03050\n66.03050\n41450000\n\n\n2020-05-04\n65.41150\n66.38300\n64.95000\n66.34000\n66.34000\n30080000\n\n\n2020-05-05\n66.89600\n68.69700\n66.87300\n67.55550\n67.55550\n33030000\n\n\n2020-05-06\n68.08450\n68.55600\n67.36450\n67.36500\n67.36500\n24308000\n\n\n2020-05-07\n68.29700\n68.88000\n67.76350\n68.62800\n68.62800\n27952000\n\n\n2020-05-08\n69.15650\n69.93800\n68.77400\n69.41850\n69.41850\n27738000\n\n\n2020-05-11\n68.91400\n70.82650\n68.85760\n70.16300\n70.16300\n28242000\n\n\n2020-05-12\n70.35600\n70.75000\n68.73850\n68.78700\n68.78700\n27812000\n\n\n2020-05-13\n68.85250\n69.27410\n66.42000\n67.46650\n67.46650\n36252000\n\n\n2020-05-14\n66.75100\n67.87100\n66.19550\n67.80650\n67.80650\n32062000\n\n\n2020-05-15\n67.50000\n68.72400\n66.95000\n68.65950\n68.65950\n34154000\n\n\n2020-05-18\n68.08750\n69.61625\n67.71250\n69.19700\n69.19700\n36448000\n\n\n2020-05-19\n69.34985\n69.60000\n68.67425\n68.67425\n68.67425\n25612000\n\n\n2020-05-20\n69.47900\n70.52100\n69.36250\n70.33600\n70.33600\n33108000\n\n\n2020-05-21\n70.40000\n70.77450\n69.67250\n70.14000\n70.14000\n27700000\n\n\n2020-05-22\n69.83550\n70.63800\n69.59150\n70.52100\n70.52100\n26188000\n\n\n2020-05-26\n71.86350\n72.05000\n70.60650\n70.85100\n70.85100\n41212000\n\n\n2020-05-27\n70.86250\n71.08700\n69.56450\n70.89200\n70.89200\n33716000\n\n\n2020-05-28\n69.84300\n72.04200\n69.80000\n70.83650\n70.83650\n33844000\n\n\n2020-05-29\n70.84700\n71.62850\n70.66750\n71.44600\n71.44600\n36418000\n\n\n2020-06-01\n70.91950\n71.89800\n70.90000\n71.59100\n71.59100\n24342000\n\n\n2020-06-02\n71.52750\n71.98050\n70.94150\n71.96100\n71.96100\n25562000\n\n\n2020-06-03\n71.91500\n72.32760\n71.48885\n71.81900\n71.81900\n25124000\n\n\n2020-06-04\n71.52000\n71.94800\n70.23650\n70.60900\n70.60900\n29686000\n\n\n2020-06-05\n70.65850\n72.25250\n70.30000\n71.91950\n71.91950\n34698000\n\n\n2020-06-08\n71.11700\n72.39950\n71.11700\n72.33050\n72.33050\n28084000\n\n\n2020-06-09\n72.26800\n73.40000\n72.16050\n72.80800\n72.80800\n28184000\n\n\n2020-06-10\n72.97700\n73.71295\n72.81350\n73.29250\n73.29250\n30504000\n\n\n2020-06-11\n72.12400\n72.72375\n70.10000\n70.19200\n70.19200\n39826000\n\n\n2020-06-12\n71.42450\n71.85000\n69.30100\n70.65900\n70.65900\n38928000\n\n\n2020-06-15\n69.54000\n71.24000\n69.39600\n70.99250\n70.99250\n30078000\n\n\n2020-06-16\n72.26100\n72.75100\n71.29500\n72.13600\n72.13600\n34184000\n\n\n2020-06-17\n72.35800\n73.00000\n71.56900\n72.55600\n72.55600\n30992000\n\n\n2020-06-18\n72.45800\n72.57050\n71.35050\n71.79800\n71.79800\n31638000\n\n\n2020-06-19\n72.20000\n72.39000\n71.06750\n71.58600\n71.58600\n63158000\n\n\n2020-06-22\n71.45000\n72.63750\n71.16050\n72.59300\n72.59300\n30848000\n\n\n2020-06-23\n72.78200\n73.79705\n72.26200\n73.22050\n73.22050\n28596000\n\n\n2020-06-24\n73.07550\n73.77100\n71.48750\n71.59850\n71.59850\n35120000\n\n\n2020-06-25\n71.49500\n72.14500\n71.00000\n72.06650\n72.06650\n24610000\n\n\n2020-06-26\n71.56950\n71.67250\n67.59950\n67.99500\n67.99500\n85354000\n\n\n2020-06-29\n67.90900\n69.78000\n67.35050\n69.74850\n69.74850\n36204000\n\n\n2020-06-30\n69.52200\n70.93250\n69.19800\n70.68050\n70.68050\n40848000\n\n\n2020-07-01\n70.55500\n72.15000\n70.49100\n71.90200\n71.90200\n35504000\n\n\n2020-07-02\n72.34700\n74.14750\n72.32100\n73.23500\n73.23500\n37182000\n\n\n2020-07-06\n74.00300\n75.32950\n73.64300\n74.78500\n74.78500\n31280000\n\n\n2020-07-07\n74.50000\n75.84000\n74.17750\n74.25900\n74.25900\n29164000\n\n\n2020-07-08\n74.71600\n75.29400\n74.28150\n74.80000\n74.80000\n24994000\n\n\n2020-07-09\n75.32250\n76.13600\n74.40425\n75.54950\n75.54950\n28466000\n\n\n2020-07-10\n75.30750\n77.19150\n74.82700\n77.08700\n77.08700\n37126000\n\n\n2020-07-13\n77.50000\n78.85660\n75.26215\n75.56700\n75.56700\n36928000\n\n\n2020-07-14\n74.51550\n76.14750\n74.17500\n76.02900\n76.02900\n31700000\n\n\n2020-07-15\n76.15650\n76.76650\n74.90000\n75.68200\n75.68200\n32214000\n\n\n2020-07-16\n75.00000\n75.93450\n74.31550\n75.90000\n75.90000\n30386000\n\n\n2020-07-17\n76.08100\n76.17200\n74.92100\n75.77750\n75.77750\n29134000\n\n\n2020-07-20\n75.76300\n78.51450\n75.18000\n78.28600\n78.28600\n31146000\n\n\n2020-07-21\n79.34950\n79.34950\n77.71400\n77.92100\n77.92100\n24324000\n\n\n2020-07-22\n78.02500\n78.50000\n77.30500\n78.42450\n78.42450\n18640000\n\n\n2020-07-23\n78.34850\n78.59350\n75.36960\n75.78400\n75.78400\n32552000\n\n\n2020-07-24\n74.94650\n75.88180\n74.42000\n75.59350\n75.59350\n30880000\n\n\n2020-07-27\n75.78000\n77.04850\n75.76050\n76.51000\n76.51000\n24920000\n\n\n2020-07-28\n76.25900\n76.32400\n74.88300\n75.01700\n75.01700\n34044000\n\n\n2020-07-29\n75.31600\n76.56260\n75.06650\n76.10100\n76.10100\n22130000\n\n\n2020-07-30\n74.85000\n76.89350\n74.61100\n76.57250\n76.57250\n33428000\n\n\n2020-07-31\n75.25050\n75.44750\n72.70150\n74.14800\n74.14800\n68798000\n\n\n2020-08-03\n74.33200\n74.52350\n73.28200\n73.72250\n73.72250\n46604000\n\n\n2020-08-04\n73.82850\n74.27800\n72.93250\n73.24850\n73.24850\n38070000\n\n\n2020-08-05\n73.46500\n74.12050\n73.17300\n73.68050\n73.68050\n39590000\n\n\n2020-08-06\n73.58750\n75.11950\n73.30000\n75.00500\n75.00500\n39908000\n\n\n2020-08-07\n75.00000\n75.84225\n74.08200\n74.72450\n74.72450\n31556000\n\n\n2020-08-10\n74.35900\n75.20375\n73.65400\n74.80500\n74.80500\n25786000\n\n\n2020-08-11\n74.62200\n75.50000\n73.90000\n74.01600\n74.01600\n29088000\n\n\n2020-08-12\n74.27900\n75.61930\n74.26250\n75.33100\n75.33100\n28740000\n\n\n2020-08-13\n75.51700\n76.86250\n75.40025\n75.92250\n75.92250\n29104000\n\n\n2020-08-14\n75.78300\n76.09500\n75.14400\n75.38650\n75.38650\n27096000\n\n\n2020-08-17\n75.73350\n76.28050\n75.39850\n75.89900\n75.89900\n27566000\n\n\n2020-08-18\n76.30900\n78.12350\n76.18550\n77.93000\n77.93000\n40542000\n\n\n2020-08-19\n77.66550\n78.68400\n77.19750\n77.37650\n77.37650\n33212000\n\n\n2020-08-20\n77.17250\n79.29350\n76.91000\n79.08750\n79.08750\n34138000\n\n\n2020-08-21\n78.85150\n79.88600\n78.40025\n79.02100\n79.02100\n28930000\n\n\n2020-08-24\n79.69900\n80.70850\n79.02850\n79.41000\n79.41000\n28198000\n\n\n2020-08-25\n79.10350\n80.58100\n79.10350\n80.41100\n80.41100\n44942000\n\n\n2020-08-26\n80.40000\n82.96100\n80.18000\n82.61900\n82.61900\n79868000\n\n\n2020-08-27\n82.68400\n82.75000\n81.28750\n81.71650\n81.71650\n37232000\n\n\n2020-08-28\n81.67450\n82.35850\n81.53750\n82.22050\n82.22050\n29980000\n\n\n2020-08-31\n82.39450\n82.39825\n81.51550\n81.70900\n81.70900\n36468000\n\n\n2020-09-01\n81.83150\n83.28650\n81.61100\n83.03550\n83.03550\n36506000\n\n\n2020-09-02\n83.68875\n86.65900\n83.31650\n86.41400\n86.41400\n50224000\n\n\n2020-09-03\n85.48570\n85.48570\n80.75300\n82.09200\n82.09200\n62156000\n\n\n2020-09-04\n81.21300\n82.25550\n77.38065\n79.55200\n79.55200\n52172000\n\n\n2020-09-08\n76.67550\n78.19325\n76.40050\n76.61950\n76.61950\n52218000\n\n\n2020-09-09\n77.87650\n78.45000\n76.80255\n77.84800\n77.84800\n35494000\n\n\n2020-09-10\n78.03200\n79.20405\n76.29025\n76.60100\n76.60100\n32372000\n\n\n2020-09-11\n76.80000\n78.76000\n74.86800\n76.03600\n76.03600\n31942000\n\n\n2020-09-14\n76.95025\n78.20000\n75.78700\n75.96400\n75.96400\n33932000\n\n\n2020-09-15\n76.80000\n77.97850\n76.59175\n77.07200\n77.07200\n26622000\n\n\n2020-09-16\n77.77700\n78.10000\n75.99100\n76.04500\n76.04500\n26234000\n\n\n2020-09-17\n74.80000\n75.41490\n73.50000\n74.77650\n74.77650\n37596000\n\n\n2020-09-18\n74.90050\n75.15015\n71.85650\n72.99950\n72.99950\n62078000\n\n\n2020-09-21\n72.00300\n72.41800\n70.32750\n71.55800\n71.55800\n57776000\n\n\n2020-09-22\n72.50450\n73.47600\n71.72650\n73.27300\n73.27300\n31664000\n\n\n2020-09-23\n72.93900\n73.04800\n70.38500\n70.76050\n70.76050\n33148000\n\n\n2020-09-24\n70.55150\n72.18545\n70.49250\n71.41450\n71.41450\n29004000\n\n\n2020-09-25\n71.63150\n72.50000\n70.66700\n72.24800\n72.24800\n26460000\n\n\n2020-09-28\n73.71050\n73.84000\n72.46505\n73.22600\n73.22600\n40158000\n\n\n2020-09-29\n73.51950\n73.83315\n72.94025\n73.46650\n73.46650\n19564000\n\n\n2020-09-30\n73.34000\n74.48750\n72.99400\n73.48000\n73.48000\n34032000\n\n\n2020-10-01\n74.21350\n74.95200\n73.96050\n74.50450\n74.50450\n35590000\n\n\n2020-10-02\n73.10150\n74.16000\n72.54600\n72.92100\n72.92100\n25682000\n\n\n2020-10-05\n73.31050\n74.41050\n73.21350\n74.30100\n74.30100\n22266000\n\n\n2020-10-06\n73.77900\n74.33800\n72.42950\n72.67200\n72.67200\n24908000\n\n\n2020-10-07\n73.21450\n73.44800\n71.80000\n73.01450\n73.01450\n34924000\n\n\n2020-10-08\n73.25450\n74.50000\n73.25450\n74.29650\n74.29650\n23756000\n\n\n2020-10-09\n74.73500\n75.82600\n74.47250\n75.76100\n75.76100\n28706000\n\n\n2020-10-12\n77.15000\n79.69300\n76.62850\n78.45750\n78.45750\n49652000\n\n\n2020-10-13\n79.18650\n79.50000\n78.16000\n78.58400\n78.58400\n32020000\n\n\n2020-10-14\n78.92950\n79.38420\n77.52650\n78.40400\n78.40400\n38586000\n\n\n2020-10-15\n77.35750\n78.75525\n77.25150\n77.95650\n77.95650\n30800000\n\n\n2020-10-16\n78.29250\n79.05650\n78.15000\n78.65050\n78.65050\n28694000\n\n\n2020-10-19\n79.02300\n79.40750\n76.40000\n76.73050\n76.73050\n32142000\n\n\n2020-10-20\n76.35250\n78.87500\n76.28350\n77.79650\n77.79650\n44834000\n\n\n2020-10-21\n78.66650\n80.93650\n78.58150\n79.66550\n79.66550\n51366000\n\n\n2020-10-22\n79.65250\n81.09950\n79.25000\n80.76650\n80.76650\n28672000\n\n\n2020-10-23\n81.30350\n82.11800\n81.02550\n82.05000\n82.05000\n27516000\n\n\n2020-10-26\n81.25050\n81.91200\n78.82500\n79.52250\n79.52250\n37066000\n\n\n2020-10-27\n79.78350\n80.34225\n79.13900\n80.21300\n80.21300\n24580000\n\n\n2020-10-28\n77.98700\n78.06750\n75.73100\n75.83100\n75.83100\n36680000\n\n\n2020-10-29\n76.11800\n79.68550\n76.11200\n78.36200\n78.36200\n40062000\n\n\n2020-10-30\n83.60550\n84.35000\n80.22300\n81.05050\n81.05050\n86582000\n\n\n2020-11-02\n81.40800\n83.03850\n80.80150\n81.30150\n81.30150\n50708000\n\n\n2020-11-03\n81.58900\n83.08500\n80.83100\n82.51050\n82.51050\n33234000\n\n\n2020-11-04\n85.51400\n88.56825\n85.30150\n87.45650\n87.45650\n71418000\n\n\n2020-11-05\n89.05000\n89.68200\n87.52550\n88.16850\n88.16850\n41316000\n\n\n2020-11-06\n87.69750\n88.62150\n87.01750\n88.08750\n88.08750\n33218000\n\n\n2020-11-09\n89.54500\n90.90300\n88.00100\n88.15000\n88.15000\n45366000\n\n\n2020-11-10\n86.55450\n88.15000\n85.86500\n87.01950\n87.01950\n52722000\n\n\n2020-11-11\n87.50000\n88.21100\n87.36825\n87.63550\n87.63550\n25280000\n\n\n2020-11-12\n87.38150\n88.41350\n87.28000\n87.49200\n87.49200\n24950000\n\n\n2020-11-13\n87.88150\n89.05200\n87.22750\n88.85100\n88.85100\n29998000\n\n\n2020-11-16\n88.58500\n89.95350\n88.38450\n89.06900\n89.06900\n24936000\n\n\n2020-11-17\n88.84700\n89.25000\n88.35000\n88.50750\n88.50750\n22942000\n\n\n2020-11-18\n88.26150\n88.67350\n87.30700\n87.33900\n87.33900\n23470000\n\n\n2020-11-19\n86.91900\n88.47950\n86.85025\n88.19600\n88.19600\n24998000\n\n\n2020-11-20\n88.26050\n88.70000\n87.09300\n87.10950\n87.10950\n46270000\n\n\n2020-11-23\n87.48000\n87.69500\n85.88600\n86.74300\n86.74300\n43232000\n\n\n2020-11-24\n86.52500\n88.58000\n86.38450\n88.44400\n88.44400\n31560000\n\n\n2020-11-25\n88.64450\n88.92700\n87.82700\n88.57150\n88.57150\n20916000\n\n\n2020-11-27\n88.65450\n90.20000\n88.62200\n89.65950\n89.65950\n17698000\n\n\n2020-11-30\n89.05920\n89.40325\n87.75000\n88.03700\n88.03700\n36476000\n\n\n2020-12-01\n88.71850\n91.24150\n88.46850\n89.90500\n89.90500\n34738000\n\n\n2020-12-02\n89.90500\n91.78250\n89.47365\n91.39750\n91.39750\n24440000\n\n\n2020-12-03\n91.20050\n92.36000\n91.13250\n91.33850\n91.33850\n24546000\n\n\n2020-12-04\n91.22600\n91.65800\n90.84950\n91.39950\n91.39950\n27564000\n\n\n2020-12-07\n90.95000\n91.61850\n90.28900\n90.97400\n90.97400\n26418000\n\n\n2020-12-08\n90.50500\n91.09500\n89.81025\n90.92750\n90.92750\n21926000\n\n\n2020-12-09\n90.60050\n91.71350\n88.39050\n89.20650\n89.20650\n30152000\n\n\n2020-12-10\n88.49000\n89.06550\n87.01600\n88.76650\n88.76650\n27256000\n\n\n2020-12-11\n88.15300\n89.22250\n88.00000\n89.08850\n89.08850\n24414000\n\n\n2020-12-14\n88.75000\n89.86950\n87.86075\n88.00300\n88.00300\n32004000\n\n\n2020-12-15\n88.22100\n88.57100\n87.49750\n88.38850\n88.38850\n29646000\n\n\n2020-12-16\n88.64400\n88.65000\n87.80400\n88.15000\n88.15000\n30270000\n\n\n2020-12-17\n88.42550\n88.58920\n86.93300\n87.39500\n87.39500\n32494000\n\n\n2020-12-18\n87.70900\n87.75550\n86.01100\n86.55050\n86.55050\n80328000\n\n\n2020-12-21\n85.67550\n87.04250\n84.95000\n86.96850\n86.96850\n36568000\n\n\n2020-12-22\n86.72150\n86.87025\n85.62875\n86.17500\n86.17500\n18734000\n\n\n2020-12-23\n86.40550\n87.39950\n86.25200\n86.61900\n86.61900\n20676000\n\n\n2020-12-24\n86.75000\n87.30000\n86.45550\n86.94250\n86.94250\n6936000\n\n\n2020-12-28\n87.58175\n89.53640\n87.31675\n88.80450\n88.80450\n27860000\n\n\n2020-12-29\n89.38950\n89.62200\n87.80450\n87.93600\n87.93600\n25988000\n\n\n2020-12-30\n88.10050\n88.25475\n86.28000\n86.97600\n86.97600\n26122000\n\n\n2020-12-31\n86.77100\n87.94650\n86.77100\n87.59400\n87.59400\n20238000\n\n\n2021-01-04\n87.87700\n88.03250\n85.39250\n86.41200\n86.41200\n38038000\n\n\n2021-01-05\n86.25000\n87.38350\n85.90075\n87.04600\n87.04600\n22906000\n\n\n2021-01-06\n85.13150\n87.40000\n84.95000\n86.76450\n86.76450\n52042000\n\n\n2021-01-07\n87.00300\n89.42000\n86.85250\n89.36250\n89.36250\n45300000\n\n\n2021-01-08\n89.39900\n90.49200\n88.67675\n90.36050\n90.36050\n41012000\n\n\n2021-01-11\n89.30350\n89.71575\n88.02600\n88.33600\n88.33600\n24194000\n\n\n2021-01-12\n87.69600\n88.90200\n86.26550\n87.32750\n87.32750\n27140000\n\n\n2021-01-13\n86.92900\n88.25175\n86.90050\n87.72000\n87.72000\n21882000\n\n\n2021-01-14\n87.68100\n88.75050\n86.67000\n87.00900\n87.00900\n23590000\n\n\n2021-01-15\n86.90950\n87.80000\n86.07750\n86.80950\n86.80950\n26844000\n\n\n2021-01-19\n87.61250\n90.46375\n87.07300\n89.54300\n89.54300\n34692000\n\n\n2021-01-20\n91.57300\n95.18550\n91.27650\n94.34500\n94.34500\n49806000\n\n\n2021-01-21\n94.90000\n96.74300\n94.35550\n94.56250\n94.56250\n41278000\n\n\n2021-01-22\n94.78400\n95.54750\n94.08800\n95.05250\n95.05250\n25442000\n\n\n2021-01-25\n96.03350\n96.47900\n93.37650\n94.97000\n94.97000\n38546000\n\n\n2021-01-26\n94.44200\n96.25000\n94.21225\n95.86200\n95.86200\n26262000\n\n\n2021-01-27\n94.12650\n94.50000\n90.45000\n91.53950\n91.53950\n54966000\n\n\n2021-01-28\n92.19700\n94.94150\n92.13200\n93.15550\n93.15550\n35462000\n\n\n2021-01-29\n92.30850\n92.86400\n90.51000\n91.78700\n91.78700\n32252000\n\n\n2021-02-01\n92.67850\n96.11960\n92.54650\n95.06750\n95.06750\n32044000\n\n\n2021-02-02\n96.12800\n97.78800\n95.72450\n96.37550\n96.37550\n45474000\n\n\n2021-02-03\n103.65000\n105.82500\n100.91900\n103.50350\n103.50350\n82364000\n\n\n2021-02-04\n103.44450\n103.92750\n102.12950\n103.11850\n103.11850\n37046000\n\n\n2021-02-05\n103.50000\n105.12550\n102.96650\n104.90000\n104.90000\n30702000\n\n\n2021-02-08\n105.29550\n106.17735\n103.60000\n104.64550\n104.64550\n24838000\n\n\n2021-02-09\n103.92700\n105.25650\n103.92700\n104.17550\n104.17550\n17798000\n\n\n2021-02-10\n104.71050\n105.41850\n103.15450\n104.76900\n104.76900\n22710000\n\n\n2021-02-11\n104.97550\n105.10150\n103.86600\n104.79450\n104.79450\n18914000\n\n\n2021-02-12\n104.51250\n105.44100\n104.15650\n105.20550\n105.20550\n17114000\n\n\n2021-02-16\n105.21800\n107.63400\n105.21800\n106.09500\n106.09500\n22676000\n\n\n2021-02-17\n105.00000\n106.68300\n104.94600\n106.41550\n106.41550\n21418000\n\n\n2021-02-18\n105.51950\n106.63675\n105.18550\n105.86000\n105.86000\n22432000\n\n\n2021-02-19\n105.96350\n106.52650\n104.87050\n105.05700\n105.05700\n29148000\n\n\n2021-02-22\n103.35000\n104.57100\n103.10700\n103.24400\n103.24400\n27350000\n\n\n2021-02-23\n101.25050\n104.10050\n100.10100\n103.54300\n103.54300\n33348000\n\n\n2021-02-24\n102.09150\n105.03900\n101.90650\n104.75850\n104.75850\n24966000\n\n\n2021-02-25\n103.37250\n104.74400\n101.06450\n101.56800\n101.56800\n36568000\n\n\n2021-02-26\n102.52600\n103.55050\n100.80300\n101.84300\n101.84300\n41670000\n\n\n2021-03-01\n102.82600\n104.32600\n102.30500\n104.07550\n104.07550\n28090000\n\n\n2021-03-02\n103.80950\n105.21850\n103.56300\n103.79200\n103.79200\n22692000\n\n\n2021-03-03\n103.36050\n104.42590\n100.50000\n101.33550\n101.33550\n29684000\n\n\n2021-03-04\n101.16850\n104.46200\n101.01350\n102.45450\n102.45450\n42360000\n\n\n2021-03-05\n103.65600\n105.90550\n102.32075\n105.42700\n105.42700\n43904000\n\n\n2021-03-08\n105.05650\n106.44050\n101.08050\n101.20850\n101.20850\n32948000\n\n\n2021-03-09\n103.50000\n103.90200\n102.39150\n102.63500\n102.63500\n33946000\n\n\n2021-03-10\n103.58800\n103.75000\n101.66850\n102.75150\n102.75150\n25372000\n\n\n2021-03-11\n103.70300\n106.28500\n103.61900\n105.73850\n105.73850\n24782000\n\n\n2021-03-12\n104.25000\n104.51300\n102.37750\n103.09600\n103.09600\n34516000\n\n\n2021-03-15\n103.11500\n103.35300\n102.17550\n103.32450\n103.32450\n25962000\n\n\n2021-03-16\n103.94950\n106.17800\n103.50000\n104.62600\n104.62600\n29832000\n\n\n2021-03-17\n103.80150\n105.48900\n102.70000\n104.55400\n104.55400\n25980000\n\n\n2021-03-18\n103.05000\n103.77500\n101.67750\n101.81100\n101.81100\n27228000\n\n\n2021-03-19\n102.10250\n102.65500\n100.88400\n102.16000\n102.16000\n46298000\n\n\n2021-03-22\n102.09200\n102.89950\n101.30350\n101.92950\n101.92950\n39096000\n\n\n2021-03-23\n102.58500\n103.61510\n101.96100\n102.64800\n102.64800\n27340000\n\n\n2021-03-24\n103.26850\n103.91050\n102.07775\n102.25300\n102.25300\n23080000\n\n\n2021-03-25\n102.24050\n102.94350\n100.53650\n102.21800\n102.21800\n28378000\n\n\n2021-03-26\n101.94300\n102.54950\n100.70100\n101.77750\n101.77750\n29870000\n\n\n2021-03-29\n101.39400\n102.92150\n100.78100\n102.79750\n102.79750\n24596000\n\n\n2021-03-30\n102.88150\n103.53900\n102.20150\n102.77700\n102.77700\n20732000\n\n\n2021-03-31\n102.95600\n104.66635\n102.83725\n103.43150\n103.43150\n29198000\n\n\n2021-04-01\n104.89750\n107.14700\n104.84450\n106.88750\n106.88750\n33980000\n\n\n2021-04-05\n107.64700\n111.86550\n107.58100\n111.27750\n111.27750\n43298000\n\n\n2021-04-06\n111.12500\n111.88300\n110.74000\n111.23750\n111.23750\n27060000\n\n\n2021-04-07\n111.30650\n112.75000\n111.26650\n112.48400\n112.48400\n25798000\n\n\n2021-04-08\n113.89800\n114.20025\n112.88400\n113.27200\n113.27200\n27166000\n\n\n2021-04-09\n112.83500\n114.45200\n112.68570\n114.29400\n114.29400\n20888000\n\n\n2021-04-12\n113.31250\n113.76600\n111.92325\n112.73950\n112.73950\n31318000\n\n\n2021-04-13\n113.07350\n113.86050\n112.80450\n113.36350\n113.36350\n23310000\n\n\n2021-04-14\n113.75800\n113.89950\n112.45950\n112.74200\n112.74200\n20220000\n\n\n2021-04-15\n113.84900\n115.32985\n113.30000\n114.83300\n114.83300\n27472000\n\n\n2021-04-16\n115.15000\n115.32200\n114.22250\n114.88800\n114.88800\n22596000\n\n\n2021-04-19\n114.59900\n115.92250\n114.39225\n115.12000\n115.12000\n24688000\n\n\n2021-04-20\n115.39450\n115.48000\n113.58550\n114.68150\n114.68150\n21774000\n\n\n2021-04-21\n114.26250\n114.76600\n112.92850\n114.66450\n114.66450\n23930000\n\n\n2021-04-22\n114.66150\n115.18810\n112.82250\n113.39600\n113.39600\n21096000\n\n\n2021-04-23\n114.17350\n116.29100\n113.91050\n115.76500\n115.76500\n28670000\n\n\n2021-04-26\n115.99650\n117.06300\n115.69200\n116.33700\n116.33700\n20834000\n\n\n2021-04-27\n116.80000\n116.87250\n115.21350\n115.35600\n115.35600\n31972000\n\n\n2021-04-28\n120.35725\n122.61890\n118.74250\n118.99550\n118.99550\n59728000\n\n\n2021-04-29\n120.51650\n121.82600\n120.11400\n121.49450\n121.49450\n39554000\n\n\n2021-04-30\n120.22450\n121.35700\n120.10800\n120.50600\n120.50600\n39142000\n\n\n2021-05-03\n120.13600\n120.98500\n119.22500\n119.75850\n119.75850\n33788000\n\n\n2021-05-04\n118.48700\n118.96300\n115.58500\n117.71250\n117.71250\n35120000\n\n\n2021-05-05\n118.42100\n119.11000\n117.57050\n117.83700\n117.83700\n21806000\n\n\n2021-05-06\n117.53200\n119.13550\n117.11690\n119.06750\n119.06750\n20618000\n\n\n2021-05-07\n120.00000\n120.82050\n119.50000\n119.93450\n119.93450\n23272000\n\n\n2021-05-10\n118.74450\n118.90000\n116.73650\n117.08300\n117.08300\n26006000\n\n\n2021-05-11\n114.59300\n116.10000\n114.15000\n115.43800\n115.43800\n32110000\n\n\n2021-05-12\n113.08550\n114.26850\n111.50250\n111.95400\n111.95400\n34934000\n\n\n2021-05-13\n113.05450\n113.83005\n112.13600\n113.09850\n113.09850\n26670000\n\n\n2021-05-14\n114.59150\n116.05700\n114.16600\n115.80800\n115.80800\n26624000\n\n\n2021-05-17\n115.46600\n116.16700\n114.75000\n116.07050\n116.07050\n19842000\n\n\n2021-05-18\n116.84530\n117.15750\n115.15800\n115.17150\n115.17150\n17302000\n\n\n2021-05-19\n113.22000\n115.83800\n113.17600\n115.43550\n115.43550\n19350000\n\n\n2021-05-20\n116.40200\n118.01700\n116.05450\n117.80450\n117.80450\n23832000\n\n\n2021-05-21\n118.29950\n118.45000\n117.11850\n117.25500\n117.25500\n22832000\n\n\n2021-05-24\n118.35000\n120.92400\n118.00550\n120.33350\n120.33350\n21244000\n\n\n2021-05-25\n121.00000\n121.64450\n120.14950\n120.45350\n120.45350\n18838000\n\n\n2021-05-26\n120.64175\n122.14720\n120.62575\n121.67650\n121.67650\n21856000\n\n\n2021-05-27\n121.84700\n122.00000\n120.10000\n120.12550\n120.12550\n38962000\n\n\n2021-05-28\n121.09800\n121.40700\n120.38450\n120.57800\n120.57800\n24108000\n\n\n2021-06-01\n121.10000\n121.89855\n120.24400\n121.49050\n121.49050\n19166000\n\n\n2021-06-02\n121.76550\n122.10000\n120.21000\n121.06400\n121.06400\n17158000\n\n\n2021-06-03\n119.75100\n120.48725\n119.14150\n120.23050\n120.23050\n18346000\n\n\n2021-06-04\n121.12600\n122.69295\n120.88850\n122.58800\n122.58800\n25948000\n\n\n2021-06-07\n122.56600\n123.40000\n122.05365\n123.30450\n123.30450\n23850000\n\n\n2021-06-08\n123.99500\n124.72475\n123.41200\n124.14250\n124.14250\n25060000\n\n\n2021-06-09\n124.97500\n125.25000\n124.36650\n124.57000\n124.57000\n20126000\n\n\n2021-06-10\n124.70050\n126.16300\n124.70000\n126.08000\n126.08000\n31234000\n\n\n2021-06-11\n126.24600\n126.34950\n124.91450\n125.69650\n125.69650\n25240000\n\n\n2021-06-14\n125.66950\n126.41150\n125.04700\n126.35200\n126.35200\n22550000\n\n\n2021-06-15\n126.52200\n126.86200\n125.64850\n126.03300\n126.03300\n22182000\n\n\n2021-06-16\n126.24750\n126.52350\n124.14995\n125.69650\n125.69650\n26316000\n\n\n2021-06-17\n125.52300\n127.19650\n125.51500\n126.37100\n126.37100\n25756000\n\n\n2021-06-18\n125.70550\n126.38900\n124.60300\n125.56750\n125.56750\n53306000\n\n\n2021-06-21\n125.74000\n127.03675\n125.13425\n126.45500\n126.45500\n26256000\n\n\n2021-06-22\n126.45000\n127.27000\n126.02650\n126.99950\n126.99950\n20984000\n\n\n2021-06-23\n126.55000\n127.79600\n126.25200\n126.46150\n126.46150\n19694000\n\n\n2021-06-24\n127.05350\n127.53550\n126.96000\n127.28200\n127.28200\n18934000\n\n\n2021-06-25\n126.95700\n127.50500\n126.44400\n126.99500\n126.99500\n33484000\n\n\n2021-06-28\n127.00000\n127.33700\n125.97380\n126.81950\n126.81950\n28110000\n\n\n2021-06-29\n126.77250\n127.00000\n125.43700\n126.01850\n126.01850\n20950000\n\n\n2021-06-30\n125.65355\n125.80000\n124.75750\n125.31600\n125.31600\n24006000\n\n\n2021-07-01\n124.84975\n126.46250\n124.84975\n126.36850\n126.36850\n17120000\n\n\n2021-07-02\n126.83950\n128.84801\n126.76900\n128.71899\n128.71899\n21160000\n\n\n2021-07-06\n129.44949\n129.88451\n128.40900\n129.77100\n129.77100\n21350000\n\n\n2021-07-07\n130.34100\n130.63989\n129.76000\n130.07750\n130.07750\n16680000\n\n\n2021-07-08\n128.25000\n130.03250\n128.03999\n129.17700\n129.17700\n19780000\n\n\n2021-07-09\n128.94450\n129.84950\n128.94350\n129.57449\n129.57449\n15106000\n\n\n2021-07-12\n129.83350\n130.77000\n129.60001\n130.56400\n130.56400\n16944000\n\n\n2021-07-13\n130.88150\n132.04201\n130.63699\n130.99451\n130.99451\n16618000\n\n\n2021-07-14\n131.90150\n132.99600\n131.89800\n132.08250\n132.08250\n17912000\n\n\n2021-07-15\n132.50000\n132.59500\n130.59801\n131.26649\n131.26649\n16586000\n\n\n2021-07-16\n131.64101\n132.18300\n130.82150\n131.84551\n131.84551\n14856000\n\n\n2021-07-19\n131.15550\n131.24699\n128.53700\n129.25400\n129.25400\n25710000\n\n\n2021-07-20\n130.00400\n132.00134\n129.18840\n131.10150\n131.10150\n19084000\n\n\n2021-07-21\n130.78700\n132.61725\n130.60150\n132.60049\n132.60049\n14742000\n\n\n2021-07-22\n132.64999\n133.50450\n132.39999\n133.32851\n133.32851\n13608000\n\n\n2021-07-23\n135.26000\n138.80850\n134.70050\n137.81599\n137.81599\n26378000\n\n\n2021-07-26\n138.25000\n139.71300\n137.65100\n139.64450\n139.64450\n23052000\n\n\n2021-07-27\n140.01100\n140.01100\n135.10001\n136.79649\n136.79649\n42164000\n\n\n2021-07-28\n138.56200\n139.67600\n136.35001\n136.38150\n136.38150\n54688000\n\n\n2021-07-29\n136.38060\n137.15150\n136.13800\n136.54050\n136.54050\n19284000\n\n\n2021-07-30\n135.51100\n135.77135\n134.81419\n135.22099\n135.22099\n23954000\n\n\n2021-08-02\n135.48450\n136.02049\n134.66950\n135.98950\n135.98950\n20140000\n\n\n2021-08-03\n136.00000\n136.33550\n134.18350\n136.28000\n136.28000\n19064000\n\n\n2021-08-04\n136.24950\n136.53799\n135.41499\n136.02850\n136.02850\n16528000\n\n\n2021-08-05\n136.02850\n136.95000\n135.60001\n136.94000\n136.94000\n11866000\n\n\n2021-08-06\n136.29500\n137.08005\n136.04649\n137.03600\n137.03600\n13560000\n\n\n2021-08-09\n136.94900\n138.32150\n136.42876\n138.00200\n138.00200\n12374000\n\n\n2021-08-10\n138.07950\n138.55150\n137.22650\n138.09650\n138.09650\n16034000\n\n\n2021-08-11\n138.28301\n138.84775\n137.35001\n137.68950\n137.68950\n15204000\n\n\n2021-08-12\n137.71300\n138.41800\n136.77901\n138.38950\n138.38950\n14646000\n\n\n2021-08-13\n138.35750\n138.67400\n138.00501\n138.40601\n138.40601\n12580000\n\n\n2021-08-16\n138.00000\n138.99049\n136.16576\n138.91600\n138.91600\n18040000\n\n\n2021-08-17\n138.19099\n138.71851\n136.78751\n137.30051\n137.30051\n21272000\n\n\n2021-08-18\n137.11549\n138.29401\n136.42101\n136.57001\n136.57001\n14934000\n\n\n2021-08-19\n135.46750\n137.45200\n135.35600\n136.91350\n136.91350\n18296000\n\n\n2021-08-20\n137.08299\n138.61450\n136.46699\n138.43700\n138.43700\n15564000\n\n\n2021-08-23\n138.99850\n142.17700\n138.74795\n141.09950\n141.09950\n21090000\n\n\n2021-08-24\n141.54350\n143.00751\n141.35350\n142.39850\n142.39850\n15126000\n\n\n2021-08-25\n142.88300\n143.31300\n142.43950\n142.95000\n142.95000\n12838000\n\n\n2021-08-26\n142.61850\n143.13480\n142.09151\n142.12300\n142.12300\n14922000\n\n\n2021-08-27\n142.11250\n145.01100\n142.02000\n144.55051\n144.55051\n24562000\n\n\n2021-08-30\n144.70450\n146.48950\n144.60001\n145.46950\n145.46950\n16916000\n\n\n2021-08-31\n145.88451\n146.11200\n145.00000\n145.46201\n145.46201\n26756000\n\n\n2021-09-01\n145.64999\n146.82050\n145.61450\n145.84199\n145.84199\n15824000\n\n\n2021-09-02\n145.94949\n146.32500\n144.10651\n144.21899\n144.21899\n21844000\n\n\n2021-09-03\n144.14600\n145.37700\n143.50501\n144.77499\n144.77499\n19104000\n\n\n2021-09-07\n144.74950\n145.82400\n144.54100\n145.51900\n145.51900\n15170000\n\n\n2021-09-08\n145.39349\n145.55100\n144.20000\n144.88350\n144.88350\n15486000\n\n\n2021-09-09\n144.88350\n145.66950\n144.43401\n144.91350\n144.91350\n14798000\n\n\n2021-09-10\n145.44350\n146.01900\n141.74150\n141.92101\n141.92101\n32896000\n\n\n2021-09-13\n143.20100\n144.19099\n142.28250\n143.46500\n143.46500\n20176000\n\n\n2021-09-14\n144.16100\n144.72749\n142.90550\n143.40601\n143.40601\n18916000\n\n\n2021-09-15\n143.75900\n145.58150\n142.25600\n145.20599\n145.20599\n20648000\n\n\n2021-09-16\n145.12100\n145.20000\n143.41635\n144.37350\n144.37350\n20292000\n\n\n2021-09-17\n143.79849\n144.24950\n141.06149\n141.46350\n141.46350\n60040000\n\n\n2021-09-20\n139.00020\n139.36250\n137.05299\n139.01700\n139.01700\n34918000\n\n\n2021-09-21\n140.11700\n140.81160\n138.90550\n139.64650\n139.64650\n18130000\n\n\n2021-09-22\n140.05051\n141.58350\n139.47176\n140.93851\n140.93851\n22068000\n\n\n2021-09-23\n141.60950\n142.25246\n141.09650\n141.82651\n141.82651\n17272000\n\n\n2021-09-24\n140.94600\n142.90350\n140.85049\n142.63300\n142.63300\n14950000\n\n\n2021-09-27\n141.58550\n142.50000\n140.50000\n141.50101\n141.50101\n18844000\n\n\n2021-09-28\n139.08850\n139.60651\n135.70000\n136.18401\n136.18401\n42190000\n\n\n2021-09-29\n137.10969\n137.39850\n134.25000\n134.52100\n134.52100\n26338000\n\n\n2021-09-30\n134.32500\n135.59000\n133.00000\n133.26550\n133.26550\n35294000\n\n\n2021-10-01\n133.55450\n137.07100\n133.37750\n136.46249\n136.46249\n28388000\n\n\n2021-10-04\n135.69949\n135.70000\n131.16650\n133.76500\n133.76500\n31530000\n\n\n2021-10-05\n134.00000\n137.36200\n134.00000\n136.17700\n136.17700\n24126000\n\n\n2021-10-06\n134.62550\n137.85200\n134.48650\n137.35400\n137.35400\n19764000\n\n\n2021-10-07\n138.86301\n140.15445\n138.56300\n139.18550\n139.18550\n18250000\n\n\n2021-10-08\n139.90601\n140.31700\n139.42935\n140.05600\n140.05600\n18924000\n\n\n2021-10-11\n139.80000\n140.76366\n138.80650\n138.84750\n138.84750\n16584000\n\n\n2021-10-12\n139.63750\n139.69501\n136.25000\n136.71300\n136.71300\n22536000\n\n\n2021-10-13\n137.75000\n138.55000\n136.97650\n137.89999\n137.89999\n16380000\n\n\n2021-10-14\n139.95200\n141.65150\n139.33900\n141.41200\n141.41200\n21426000\n\n\n2021-10-15\n142.20000\n142.20000\n141.06450\n141.67500\n141.67500\n21250000\n\n\n2021-10-18\n141.21350\n142.99875\n141.21350\n142.96050\n142.96050\n16564000\n\n\n2021-10-19\n143.29150\n144.10700\n143.09599\n143.82201\n143.82201\n15316000\n\n\n2021-10-20\n144.22250\n144.24776\n141.91200\n142.41499\n142.41499\n17940000\n\n\n2021-10-21\n142.19200\n142.84950\n141.63699\n142.78050\n142.78050\n14850000\n\n\n2021-10-22\n140.35100\n141.55850\n137.17050\n138.62500\n138.62500\n30182000\n\n\n2021-10-25\n138.81050\n139.20579\n136.74850\n138.77300\n138.77300\n21082000\n\n\n2021-10-26\n140.60600\n140.83949\n139.00549\n139.67200\n139.67200\n28258000\n\n\n2021-10-27\n139.90250\n149.11800\n139.90250\n146.42750\n146.42750\n51850000\n\n\n2021-10-28\n147.29900\n147.42450\n144.76350\n146.12900\n146.12900\n32418000\n\n\n2021-10-29\n145.52000\n148.61301\n145.16650\n148.27049\n148.27049\n28954000\n\n\n2021-11-01\n148.16499\n148.39951\n143.57950\n143.77400\n143.77400\n32272000\n\n\n2021-11-02\n144.80940\n146.92050\n144.64101\n145.86301\n145.86301\n21150000\n\n\n2021-11-03\n146.27499\n146.91051\n145.05350\n146.78999\n146.78999\n17886000\n\n\n2021-11-04\n147.20000\n149.95000\n146.63500\n148.68300\n148.68300\n24700000\n\n\n2021-11-05\n149.35350\n150.57075\n148.65199\n149.24100\n149.24100\n20408000\n\n\n2021-11-08\n150.00000\n151.03450\n149.11999\n149.35150\n149.35150\n18388000\n\n\n2021-11-09\n149.74600\n150.37849\n147.50700\n149.24850\n149.24850\n16876000\n\n\n2021-11-10\n148.00975\n148.70000\n145.32500\n146.62601\n146.62601\n22708000\n\n\n2021-11-11\n147.10700\n148.50224\n146.69450\n146.74800\n146.74800\n12464000\n\n\n2021-11-12\n147.83150\n149.85950\n146.45399\n149.64549\n149.64549\n17048000\n\n\n2021-11-15\n150.00000\n150.47700\n148.65250\n149.38800\n149.38800\n16248000\n\n\n2021-11-16\n149.17050\n149.83250\n148.35001\n149.07600\n149.07600\n17254000\n\n\n2021-11-17\n149.22900\n149.62601\n148.56300\n149.06200\n149.06200\n15290000\n\n\n2021-11-18\n149.14600\n151.61000\n148.99850\n150.70900\n150.70900\n26658000\n\n\n2021-11-19\n151.00000\n151.85001\n149.88750\n149.95250\n149.95250\n19766000\n\n\n2021-11-22\n150.14175\n150.74451\n147.00549\n147.07851\n147.07851\n24608000\n\n\n2021-11-23\n147.11301\n147.69400\n144.88950\n146.75700\n146.75700\n18126000\n\n\n2021-11-24\n146.35001\n147.00000\n145.19900\n146.71750\n146.71750\n16464000\n\n\n2021-11-26\n145.01550\n145.29700\n142.48550\n142.80600\n142.80600\n16992000\n\n\n2021-11-29\n144.29849\n146.86200\n144.29849\n146.11400\n146.11400\n26276000\n\n\n2021-11-30\n145.45026\n146.62849\n142.06599\n142.45200\n142.45200\n41590000\n\n\n2021-12-01\n144.21249\n146.49915\n141.50000\n141.61800\n141.61800\n28476000\n\n\n2021-12-02\n141.82400\n144.67500\n140.98200\n143.77650\n143.77650\n21250000\n\n\n2021-12-03\n144.49550\n145.21300\n141.14999\n142.52049\n142.52049\n26688000\n\n\n2021-12-06\n143.57400\n144.35150\n140.64700\n143.79649\n143.79649\n22198000\n\n\n2021-12-07\n145.95000\n148.30000\n145.70250\n148.03650\n148.03650\n23258000\n\n\n2021-12-08\n148.33150\n149.15649\n147.20000\n148.72051\n148.72051\n18964000\n\n\n2021-12-09\n148.17600\n149.60500\n147.52901\n148.10600\n148.10600\n18580000\n\n\n2021-12-10\n149.10001\n149.39999\n147.35750\n148.67500\n148.67500\n21634000\n\n\n2021-12-13\n148.44400\n148.56250\n146.36000\n146.70450\n146.70450\n24104000\n\n\n2021-12-14\n144.77000\n145.44200\n142.24249\n144.97051\n144.97051\n24778000\n\n\n2021-12-15\n144.36600\n147.51724\n142.70550\n147.36850\n147.36850\n27280000\n\n\n2021-12-16\n148.07700\n148.55150\n144.09250\n144.83850\n144.83850\n27400000\n\n\n2021-12-17\n142.71449\n144.46010\n141.78799\n142.80299\n142.80299\n43404000\n\n\n2021-12-20\n140.67960\n142.61050\n140.25000\n142.40150\n142.40150\n20264000\n\n\n2021-12-21\n143.14999\n144.69205\n141.73500\n144.22051\n144.22051\n19548000\n\n\n2021-12-22\n144.10001\n147.30299\n143.96300\n146.94900\n146.94900\n18438000\n\n\n2021-12-23\n147.08949\n148.57260\n146.95085\n147.14250\n147.14250\n13818000\n\n\n2021-12-27\n147.46350\n148.42650\n147.25000\n148.06400\n148.06400\n13256000\n\n\n2021-12-28\n148.37450\n148.37450\n145.93550\n146.44800\n146.44800\n18624000\n\n\n2021-12-29\n146.42950\n147.18375\n145.50450\n146.50450\n146.50450\n17022000\n\n\n2021-12-30\n146.45000\n147.06250\n145.75850\n146.00250\n146.00250\n12978000\n\n\n2021-12-31\n145.54401\n146.36500\n144.67750\n144.67950\n144.67950\n17298000\n\n\n2022-01-03\n144.47549\n145.55000\n143.50250\n145.07449\n145.07449\n25214000\n\n\n2022-01-04\n145.55051\n146.61000\n143.81615\n144.41650\n144.41650\n22928000\n\n\n2022-01-05\n144.18100\n144.29800\n137.52350\n137.65350\n137.65350\n49642000\n\n\n2022-01-06\n137.49750\n139.68600\n136.76350\n137.55100\n137.55100\n29050000\n\n\n2022-01-07\n137.90500\n138.25475\n135.78900\n137.00450\n137.00450\n19408000\n\n\n2022-01-10\n135.09900\n138.64000\n133.14050\n138.57400\n138.57400\n34096000\n\n\n2022-01-11\n138.18050\n140.32950\n136.81351\n140.01750\n140.01750\n23502000\n\n\n2022-01-12\n141.55450\n142.81426\n141.11200\n141.64800\n141.64800\n23642000\n\n\n2022-01-13\n141.84050\n143.18550\n138.91400\n139.13100\n139.13100\n26566000\n\n\n2022-01-14\n137.50000\n141.20050\n137.50000\n139.78650\n139.78650\n23826000\n\n\n2022-01-18\n136.60001\n137.39149\n135.61700\n136.29050\n136.29050\n27382000\n\n\n2022-01-19\n136.93851\n138.39951\n135.50000\n135.65199\n135.65199\n20796000\n\n\n2022-01-20\n136.51401\n137.91200\n133.14450\n133.50650\n133.50650\n21930000\n\n\n2022-01-21\n133.01199\n134.76050\n130.00101\n130.09199\n130.09199\n41920000\n\n\n2022-01-24\n126.02750\n130.77850\n124.64195\n130.37199\n130.37199\n55148000\n\n\n2022-01-25\n128.43550\n129.33850\n126.37800\n126.73550\n126.73550\n36008000\n\n\n2022-01-26\n130.59250\n132.80749\n127.15350\n129.24000\n129.24000\n39630000\n\n\n2022-01-27\n131.36099\n132.60996\n128.94501\n129.12100\n129.12100\n30248000\n\n\n2022-01-28\n130.00000\n133.37050\n128.69450\n133.28950\n133.28950\n30518000\n\n\n2022-01-31\n134.19800\n135.84351\n132.27400\n135.69850\n135.69850\n34056000\n\n\n2022-02-01\n137.83501\n138.20000\n134.56825\n137.87849\n137.87849\n51204000\n\n\n2022-02-02\n151.86350\n152.10001\n145.55749\n148.03650\n148.03650\n89750000\n\n\n2022-02-03\n145.29500\n149.11771\n142.20500\n142.65050\n142.65050\n56930000\n\n\n2022-02-04\n143.01700\n144.53525\n139.81751\n143.01601\n143.01601\n49224000\n\n\n2022-02-07\n143.70900\n143.84650\n138.69900\n138.93800\n138.93800\n44610000\n\n\n2022-02-08\n138.99126\n139.83710\n136.87300\n139.21300\n139.21300\n34256000\n\n\n2022-02-09\n140.84975\n142.17551\n140.37700\n141.45300\n141.45300\n28628000\n\n\n2022-02-10\n139.50000\n141.43100\n138.05000\n138.60249\n138.60249\n33018000\n\n\n2022-02-11\n138.75000\n139.28325\n133.28850\n134.13001\n134.13001\n38808000\n\n\n2022-02-14\n133.36549\n136.16650\n133.30200\n135.30000\n135.30000\n26792000\n\n\n2022-02-15\n137.47150\n137.89999\n135.53950\n136.42551\n136.42551\n26578000\n\n\n2022-02-16\n136.43050\n137.94600\n134.82365\n137.48750\n137.48750\n25610000\n\n\n2022-02-17\n136.14999\n136.83949\n132.20200\n132.30850\n132.30850\n30968000\n\n\n2022-02-18\n133.03751\n133.82400\n130.30705\n130.46750\n130.46750\n31858000\n\n\n2022-02-22\n129.98500\n131.90076\n127.74100\n129.40250\n129.40250\n38906000\n\n\n2022-02-23\n131.07851\n131.74899\n127.50350\n127.58500\n127.58500\n26432000\n\n\n2022-02-24\n125.00000\n133.03700\n124.76450\n132.67349\n132.67349\n43166000\n\n\n2022-02-25\n133.52550\n135.38901\n131.76500\n134.51950\n134.51950\n26236000\n\n\n2022-02-28\n133.28450\n135.64050\n132.82526\n134.89101\n134.89101\n29676000\n\n\n2022-03-01\n134.48000\n136.11099\n133.37849\n134.16800\n134.16800\n24640000\n\n\n2022-03-02\n134.60825\n135.61549\n133.43249\n134.75150\n134.75150\n23966000\n\n\n2022-03-03\n135.97850\n136.71381\n133.43100\n134.30800\n134.30800\n19780000\n\n\n2022-03-04\n133.38251\n134.19900\n130.40849\n132.12199\n132.12199\n24446000\n\n\n2022-03-07\n131.90401\n131.90401\n126.41000\n126.46450\n126.46450\n39178000\n\n\n2022-03-08\n126.25050\n131.24651\n125.86075\n127.27850\n127.27850\n35250000\n\n\n2022-03-09\n131.39999\n134.19850\n130.08800\n133.86600\n133.86600\n32258000\n\n\n2022-03-10\n131.46249\n133.53850\n131.40100\n132.68201\n132.68201\n24266000\n\n\n2022-03-11\n133.99950\n134.20000\n130.29649\n130.47549\n130.47549\n26600000\n\n\n2022-03-14\n130.57300\n131.02600\n126.41300\n126.74100\n126.74100\n30254000\n\n\n2022-03-15\n127.74150\n130.51724\n126.56800\n129.66051\n129.66051\n30292000\n\n\n2022-03-16\n131.00000\n133.77100\n129.20100\n133.69051\n133.69051\n32058000\n\n\n2022-03-17\n133.32100\n134.73950\n132.71899\n134.60049\n134.60049\n23994000\n\n\n2022-03-18\n133.88400\n136.91350\n132.93201\n136.80150\n136.80150\n45900000\n\n\n2022-03-21\n136.84750\n137.58250\n134.61150\n136.47850\n136.47850\n26632000\n\n\n2022-03-22\n136.50000\n141.50000\n136.50000\n140.27750\n140.27750\n29776000\n\n\n2022-03-23\n139.13850\n140.02499\n138.16650\n138.50349\n138.50349\n25302000\n\n\n2022-03-24\n139.27251\n141.39650\n138.03940\n141.31200\n141.31200\n20544000\n\n\n2022-03-25\n141.75400\n141.95950\n139.69949\n141.52150\n141.52150\n19270000\n\n\n2022-03-28\n140.68449\n141.97650\n139.82816\n141.95000\n141.95000\n23774000\n\n\n2022-03-29\n143.16051\n144.16251\n142.48399\n143.25000\n143.25000\n28678000\n\n\n2022-03-30\n142.86999\n143.48050\n142.16800\n142.64450\n142.64450\n21046000\n\n\n2022-03-31\n142.44850\n142.64450\n139.61900\n139.64951\n139.64951\n29516000\n\n\n2022-04-01\n140.01000\n140.95000\n138.79700\n140.70000\n140.70000\n23480000\n\n\n2022-04-04\n140.82449\n144.04375\n140.82449\n143.64250\n143.64250\n19076000\n\n\n2022-04-05\n143.39951\n143.59000\n140.94350\n141.06300\n141.06300\n19256000\n\n\n2022-04-06\n139.16150\n139.84850\n136.41811\n137.17600\n137.17600\n23574000\n\n\n2022-04-07\n136.61800\n137.70151\n134.85725\n136.46500\n136.46500\n19448000\n\n\n2022-04-08\n136.25000\n136.25000\n133.75250\n134.01050\n134.01050\n16434000\n\n\n2022-04-11\n132.89999\n132.93919\n129.61749\n129.79649\n129.79649\n24188000\n\n\n2022-04-12\n132.42349\n132.42349\n127.57600\n128.37450\n128.37450\n23004000\n\n\n2022-04-13\n128.62650\n130.65575\n128.43860\n130.28600\n130.28600\n19542000\n\n\n2022-04-14\n130.64951\n130.71025\n127.11150\n127.25300\n127.25300\n23484000\n\n\n2022-04-18\n127.41000\n128.71201\n126.57845\n127.96100\n127.96100\n14918000\n\n\n2022-04-19\n128.07700\n130.90375\n127.45150\n130.53101\n130.53101\n22720000\n\n\n2022-04-20\n131.28400\n131.92349\n127.89405\n128.24550\n128.24550\n22610000\n\n\n2022-04-21\n129.35001\n130.30749\n124.65000\n124.93750\n124.93750\n30158000\n\n\n2022-04-22\n125.00000\n125.45200\n119.14050\n119.61400\n119.61400\n46410000\n\n\n2022-04-25\n119.42950\n123.27800\n118.76925\n123.25000\n123.25000\n34522000\n\n\n2022-04-26\n122.75000\n122.75000\n119.16185\n119.50600\n119.50600\n49394000\n\n\n2022-04-27\n114.37300\n117.50000\n113.12425\n115.02050\n115.02050\n62238000\n\n\n2022-04-28\n117.11500\n120.43850\n115.14390\n119.41150\n119.41150\n36790000\n\n\n2022-04-29\n117.57800\n118.96000\n114.69400\n114.96650\n114.96650\n33694000\n\n\n2022-05-02\n113.90650\n117.33950\n113.39950\n117.15700\n117.15700\n30280000\n\n\n2022-05-03\n116.76500\n119.30000\n116.62700\n118.12950\n118.12950\n21216000\n\n\n2022-05-04\n118.00350\n123.14300\n115.73850\n122.57500\n122.57500\n33232000\n\n\n2022-05-05\n120.22050\n121.23325\n115.18250\n116.74650\n116.74650\n43090000\n\n\n2022-05-06\n115.51900\n117.49850\n114.14300\n115.66000\n115.66000\n35310000\n\n\n2022-05-09\n113.30350\n115.56290\n112.55150\n113.08400\n113.08400\n34520000\n\n\n2022-05-10\n116.04050\n116.69100\n113.38330\n114.58450\n114.58450\n31158000\n\n\n2022-05-11\n113.71050\n116.67100\n113.65000\n113.96100\n113.96100\n36502000\n\n\n2022-05-12\n111.93800\n114.85650\n110.11350\n113.16100\n113.16100\n41464000\n\n\n2022-05-13\n114.84550\n118.08500\n114.00000\n116.51550\n116.51550\n29738000\n\n\n2022-05-16\n115.38400\n116.60750\n114.33500\n114.79250\n114.79250\n23282000\n\n\n2022-05-17\n117.22750\n117.22750\n115.33750\n116.70150\n116.70150\n21576000\n\n\n2022-05-18\n115.23750\n115.69565\n112.14200\n112.40100\n112.40100\n27982000\n\n\n2022-05-19\n111.84100\n113.58750\n110.46800\n110.74550\n110.74550\n29192000\n\n\n2022-05-20\n112.08550\n112.55000\n106.37300\n109.31300\n109.31300\n37586000\n\n\n2022-05-23\n110.10400\n112.00550\n109.15425\n111.66650\n111.66650\n31558000\n\n\n2022-05-24\n106.37750\n106.39500\n102.20800\n105.92600\n105.92600\n60386000\n\n\n2022-05-25\n105.14200\n106.54470\n104.21125\n105.83950\n105.83950\n37900000\n\n\n2022-05-26\n106.05050\n108.95525\n105.48800\n108.29600\n108.29600\n30288000\n\n\n2022-05-27\n109.78850\n112.86800\n109.55000\n112.79900\n112.79900\n29924000\n\n\n2022-05-31\n113.07900\n116.43350\n112.57250\n114.03900\n114.03900\n51302000\n\n\n2022-06-01\n114.93150\n117.39900\n113.55050\n114.13700\n114.13700\n28630000\n\n\n2022-06-02\n114.18800\n117.89800\n113.30800\n117.74600\n117.74600\n27472000\n\n\n2022-06-03\n115.99250\n116.36450\n113.66800\n114.56400\n114.56400\n25052000\n\n\n2022-06-06\n116.74250\n119.39850\n116.52830\n117.01050\n117.01050\n23786000\n\n\n2022-06-07\n115.64800\n117.74865\n115.12550\n117.22950\n117.22950\n26414000\n\n\n2022-06-08\n116.87650\n118.64600\n116.69675\n117.23800\n117.23800\n22544000\n\n\n2022-06-09\n116.34150\n118.35000\n114.86700\n114.91800\n114.91800\n23142000\n\n\n2022-06-10\n112.78125\n113.49700\n110.86100\n111.42750\n111.42750\n31324000\n\n\n2022-06-13\n107.44600\n109.21850\n106.58805\n106.87650\n106.87650\n36756000\n\n\n2022-06-14\n106.89000\n108.45750\n106.35200\n107.19400\n107.19400\n25480000\n\n\n2022-06-15\n108.89950\n112.06300\n108.11875\n110.39050\n110.39050\n33192000\n\n\n2022-06-16\n108.14950\n109.29050\n105.79250\n106.63600\n106.63600\n35314000\n\n\n2022-06-17\n106.53500\n109.24950\n105.62855\n107.86550\n107.86550\n43516000\n\n\n2022-06-21\n109.70200\n112.67300\n109.29350\n112.01500\n112.01500\n39010000\n\n\n2022-06-22\n111.16300\n113.76950\n110.72430\n112.03400\n112.03400\n23922000\n\n\n2022-06-23\n112.95000\n113.19650\n111.02900\n112.68450\n112.68450\n24710000\n\n\n2022-06-24\n113.60300\n118.63750\n113.60300\n118.53800\n118.53800\n39122000\n\n\n2022-06-27\n118.93500\n119.25000\n116.00075\n116.62250\n116.62250\n32840000\n\n\n2022-06-28\n116.35100\n117.85650\n112.44400\n112.57150\n112.57150\n28232000\n\n\n2022-06-29\n112.14850\n113.66450\n111.55400\n112.25650\n112.25650\n18628000\n\n\n2022-06-30\n110.49950\n111.32980\n107.31000\n109.37250\n109.37250\n38046000\n\n\n2022-07-01\n108.33700\n109.80635\n107.10500\n109.08100\n109.08100\n31028000\n\n\n2022-07-05\n107.51450\n114.05260\n106.24950\n113.88700\n113.88700\n36398000\n\n\n2022-07-06\n114.09200\n116.35200\n112.25050\n115.21350\n115.21350\n28852000\n\n\n2022-07-07\n116.00800\n119.86200\n115.53400\n119.30600\n119.30600\n32184000\n\n\n2022-07-08\n117.55000\n120.43500\n117.51400\n120.16850\n120.16850\n29082000\n\n\n2022-07-11\n118.65000\n118.79450\n116.23450\n116.52250\n116.52250\n26718000\n\n\n2022-07-12\n116.83850\n117.84950\n114.61500\n114.84950\n114.84950\n24970000\n\n\n2022-07-13\n112.63900\n115.15700\n111.82300\n112.18700\n112.18700\n38958000\n\n\n2022-07-14\n110.82600\n111.98750\n109.32550\n111.44000\n111.44000\n32366000\n\n\n2022-07-15\n112.96300\n114.00050\n111.82250\n112.76700\n112.76700\n34330000\n\n\n2022-07-18\n113.44000\n114.80000\n109.30000\n109.91000\n109.91000\n33354000\n\n\n2022-07-19\n111.73000\n114.81000\n110.50000\n114.62000\n114.62000\n30992300\n\n\n2022-07-20\n114.06000\n116.33000\n113.26000\n114.70000\n114.70000\n26780100\n\n\n2022-07-21\n115.09000\n115.21000\n111.91000\n115.04000\n115.04000\n27267800\n\n\n2022-07-22\n111.81000\n113.18000\n107.60000\n108.36000\n108.36000\n44455300\n\n\n2022-07-25\n108.88000\n110.58000\n107.01000\n108.21000\n108.21000\n28289900\n\n\n2022-07-26\n107.43000\n107.74000\n104.76000\n105.44000\n105.44000\n36626600\n\n\n2022-07-27\n109.60000\n114.40000\n108.42000\n113.60000\n113.60000\n41474600\n\n\n2022-07-28\n112.80000\n114.70000\n111.85100\n114.59000\n114.59000\n23303800\n\n\n2022-07-29\n113.40000\n116.90000\n113.23000\n116.64000\n116.64000\n31336200\n\n\n2022-08-01\n115.53000\n117.12000\n114.69000\n115.48000\n115.48000\n22856200\n\n\n2022-08-02\n114.43000\n117.08000\n114.26000\n115.90000\n115.90000\n17911000\n\n\n2022-08-03\n116.34000\n119.42000\n116.15000\n118.78000\n118.78000\n25302800\n\n\n2022-08-04\n118.30000\n119.50000\n117.71000\n118.87000\n118.87000\n15757700\n\n\n2022-08-05\n116.93000\n118.86000\n116.71000\n118.22000\n118.22000\n15615700\n\n\n2022-08-08\n119.12000\n120.86000\n117.83000\n118.14000\n118.14000\n17061100\n\n\n2022-08-09\n117.99000\n118.20000\n116.56000\n117.50000\n117.50000\n15424300\n\n\n2022-08-10\n119.59000\n121.78000\n119.36000\n120.65000\n120.65000\n20497000\n\n\n2022-08-11\n122.08000\n122.34000\n119.55000\n119.82000\n119.82000\n16671600\n\n\n2022-08-12\n121.16000\n122.65000\n120.40000\n122.65000\n122.65000\n16121100\n\n\n2022-08-15\n122.21000\n123.26000\n121.57000\n122.88000\n122.88000\n15525000\n\n\n2022-08-16\n122.32000\n123.22800\n121.53500\n122.51000\n122.51000\n15626200\n\n\n2022-08-17\n120.93000\n122.15000\n120.20000\n120.32000\n120.32000\n17589200\n\n\n2022-08-18\n120.23000\n121.69000\n119.55000\n120.86000\n120.86000\n15652000\n\n\n2022-08-19\n119.87000\n120.00000\n117.67000\n118.12000\n118.12000\n20187000\n\n\n2022-08-22\n116.10000\n116.50000\n114.67000\n115.07000\n115.07000\n19316000\n\n\n2022-08-23\n114.32000\n115.93000\n114.30000\n114.77000\n114.77000\n14390700\n\n\n2022-08-24\n114.45000\n115.71700\n113.78000\n114.70000\n114.70000\n16051200\n\n\n2022-08-25\n115.15000\n117.78000\n115.05000\n117.70000\n117.70000\n14874700\n\n\n2022-08-26\n115.81000\n116.60000\n111.22000\n111.30000\n111.30000\n31698700\n\n\n2022-08-29\n110.78000\n111.96000\n109.81000\n110.34000\n110.34000\n20386100\n\n\n2022-08-30\n111.03000\n111.37000\n108.80000\n109.91000\n109.91000\n20548200\n\n\n2022-08-31\n111.63000\n111.77000\n109.05000\n109.15000\n109.15000\n25898000\n\n\n2022-09-01\n109.20000\n111.22000\n108.19000\n110.55000\n110.55000\n22784400\n\n\n2022-09-02\n111.34000\n111.67500\n108.13000\n108.68000\n108.68000\n20618100\n\n\n2022-09-06\n108.13500\n108.88000\n106.51000\n107.48000\n107.48000\n20565100\n\n\n2022-09-07\n107.76000\n110.99000\n107.61500\n110.48000\n110.48000\n22987200\n\n\n2022-09-08\n109.18000\n110.58000\n108.06000\n109.42000\n109.42000\n21660700\n\n\n2022-09-09\n110.05000\n112.00000\n110.00000\n111.78000\n111.78000\n21732900\n\n\n2022-09-12\n111.99000\n112.64000\n110.93000\n111.87000\n111.87000\n19732900\n\n\n2022-09-13\n108.89000\n109.37000\n105.00000\n105.31000\n105.31000\n33015000\n\n\n2022-09-14\n105.44000\n106.10000\n104.50000\n105.87000\n105.87000\n22115800\n\n\n2022-09-15\n105.01000\n106.21000\n103.31000\n103.90000\n103.90000\n26494900\n\n\n2022-09-16\n102.97000\n104.03000\n101.85500\n103.63000\n103.63000\n64540100\n\n\n2022-09-19\n102.54000\n104.02000\n102.37000\n103.85000\n103.85000\n19738600\n\n\n2022-09-20\n102.88000\n103.17000\n101.12000\n101.83000\n101.83000\n24001700\n\n\n2022-09-21\n102.24000\n103.49000\n99.99000\n100.01000\n100.01000\n26596800\n\n\n2022-09-22\n99.45000\n101.68000\n99.41000\n100.57000\n100.57000\n21272700\n\n\n2022-09-23\n100.06000\n100.11000\n98.01000\n99.17000\n99.17000\n25657000\n\n\n2022-09-26\n98.61000\n100.44000\n98.38000\n98.81000\n98.81000\n22437900\n\n\n2022-09-27\n99.91000\n100.46000\n97.34000\n98.09000\n98.09000\n24225000\n\n\n2022-09-28\n98.02000\n101.40000\n97.80000\n100.74000\n100.74000\n24617000\n\n\n2022-09-29\n99.30000\n99.30000\n96.52000\n98.09000\n98.09000\n21921500\n\n\n2022-09-30\n97.73000\n99.49400\n96.03000\n96.15000\n96.15000\n26277800\n\n\n2022-10-03\n97.22000\n99.97000\n97.02000\n99.30000\n99.30000\n24840000\n\n\n2022-10-04\n101.04000\n102.72000\n101.04000\n102.41000\n102.41000\n22580900\n\n\n2022-10-05\n100.69000\n102.74000\n99.74000\n102.22000\n102.22000\n18475500\n\n\n2022-10-06\n101.50000\n103.73000\n101.50000\n102.24000\n102.24000\n17156200\n\n\n2022-10-07\n100.65000\n101.42000\n99.21000\n99.57000\n99.57000\n24249900\n\n\n2022-10-10\n99.85000\n99.99000\n97.87000\n98.71000\n98.71000\n16529900\n\n\n2022-10-11\n98.25000\n100.12000\n97.25000\n98.05000\n98.05000\n21617700\n\n\n2022-10-12\n98.27000\n99.64800\n97.67000\n98.30000\n98.30000\n17343400\n\n\n2022-10-13\n95.93000\n100.53000\n95.27000\n99.71000\n99.71000\n32812200\n\n\n2022-10-14\n100.62500\n101.29000\n97.03000\n97.18000\n97.18000\n22624800\n\n\n2022-10-17\n99.52000\n101.77000\n99.51000\n100.78000\n100.78000\n23311600\n\n\n2022-10-18\n103.94000\n104.22000\n100.65000\n101.39000\n101.39000\n21610500\n\n\n2022-10-19\n100.70000\n101.65900\n99.63500\n100.29000\n100.29000\n21573700\n\n\n2022-10-20\n100.82000\n103.00000\n99.97000\n100.53000\n100.53000\n25125100\n\n\n2022-10-21\n98.46000\n101.62000\n98.23000\n101.48000\n101.48000\n28988700\n\n\n2022-10-24\n102.09000\n103.10000\n100.30000\n102.97000\n102.97000\n24680800\n\n\n2022-10-25\n103.30000\n105.10000\n103.02000\n104.93000\n104.93000\n29910200\n\n\n2022-10-26\n96.76000\n98.54000\n94.57000\n94.82000\n94.82000\n71504300\n\n\n2022-10-27\n94.31000\n95.17000\n91.90000\n92.60000\n92.60000\n54036500\n\n\n2022-10-28\n92.53000\n96.86000\n92.32300\n96.58000\n96.58000\n35696900\n\n\n2022-10-31\n95.78000\n96.35000\n94.38000\n94.66000\n94.66000\n29868700\n\n\n2022-11-01\n95.59000\n96.16500\n90.43000\n90.50000\n90.50000\n43220600\n\n\n2022-11-02\n90.91000\n91.30000\n87.01000\n87.07000\n87.07000\n43553600\n\n\n2022-11-03\n86.34500\n86.55000\n83.45000\n83.49000\n83.49000\n48510400\n\n\n2022-11-04\n85.51000\n86.73000\n83.88000\n86.70000\n86.70000\n40173300\n\n\n2022-11-07\n87.34000\n88.94000\n86.96000\n88.65000\n88.65000\n26899900\n\n\n2022-11-08\n89.16000\n90.40500\n87.65000\n88.91000\n88.91000\n30172000\n\n\n2022-11-09\n88.54500\n89.49000\n87.36000\n87.40000\n87.40000\n26743900\n\n\n2022-11-10\n92.34000\n94.55000\n91.65000\n94.17000\n94.17000\n42371200\n\n\n2022-11-11\n94.71000\n97.36000\n94.16000\n96.73000\n96.73000\n30569100\n\n\n2022-11-14\n95.50000\n97.18000\n95.11300\n96.03000\n96.03000\n24170100\n\n\n2022-11-15\n98.67000\n100.42000\n97.02000\n98.72000\n98.72000\n31831000\n\n\n2022-11-16\n98.02000\n99.85000\n97.90200\n98.99000\n98.99000\n24660200\n\n\n2022-11-17\n97.18000\n99.48000\n97.10000\n98.50000\n98.50000\n21818700\n\n\n2022-11-18\n99.01000\n99.16000\n96.74000\n97.80000\n97.80000\n24969900\n\n\n2022-11-21\n97.56000\n98.72000\n95.67000\n95.83000\n95.83000\n18696900\n\n\n2022-11-22\n96.16000\n97.54800\n94.41000\n97.33000\n97.33000\n18868900\n\n\n2022-11-23\n97.34000\n99.06900\n97.34000\n98.82000\n98.82000\n17568900\n\n\n2022-11-25\n98.46500\n98.94000\n97.53000\n97.60000\n97.60000\n8567800\n\n\n2022-11-28\n97.20000\n97.83000\n95.89000\n96.25000\n96.25000\n19974500\n\n\n2022-11-29\n96.00000\n96.39000\n94.39000\n95.44000\n95.44000\n20220000\n\n\n2022-11-30\n95.12000\n101.45000\n94.67000\n101.45000\n101.45000\n39888100\n\n\n2022-12-01\n101.40000\n102.59000\n100.67000\n101.28000\n101.28000\n21771500\n\n\n2022-12-02\n99.37000\n101.15000\n99.17000\n100.83000\n100.83000\n18821500\n\n\n2022-12-05\n99.81500\n101.75000\n99.35500\n99.87000\n99.87000\n19955500\n\n\n2022-12-06\n99.67000\n100.21000\n96.76000\n97.31000\n97.31000\n20877600\n\n\n2022-12-07\n96.77000\n97.31000\n95.02500\n95.15000\n95.15000\n26647900\n\n\n2022-12-08\n95.69000\n95.87000\n93.80000\n93.95000\n93.95000\n25593200\n\n\n2022-12-09\n93.90000\n94.49000\n93.02000\n93.07000\n93.07000\n21885300\n\n\n2022-12-12\n93.09000\n93.87500\n91.90000\n93.56000\n93.56000\n27380900\n\n\n2022-12-13\n98.07000\n99.80000\n95.38000\n95.85000\n95.85000\n34788500\n\n\n2022-12-14\n95.54000\n97.22000\n93.94000\n95.31000\n95.31000\n26452900\n\n\n2022-12-15\n93.54000\n94.03000\n90.43000\n91.20000\n91.20000\n28298800\n\n\n2022-12-16\n91.20000\n91.75000\n90.01000\n90.86000\n90.86000\n48485500\n\n\n2022-12-19\n90.88000\n91.20000\n88.92500\n89.15000\n89.15000\n23020500\n\n\n2022-12-20\n88.73000\n89.78000\n88.04000\n89.63000\n89.63000\n21976800\n\n\n2022-12-21\n89.73000\n90.91500\n88.91000\n90.25000\n90.25000\n20336400\n\n\n2022-12-22\n88.93000\n89.18000\n86.94000\n88.26000\n88.26000\n23656100\n\n\n2022-12-23\n87.62000\n90.10000\n87.62000\n89.81000\n89.81000\n17815000\n\n\n2022-12-27\n89.31000\n89.50000\n87.53500\n87.93000\n87.93000\n15470900\n\n\n2022-12-28\n87.50000\n88.52000\n86.37000\n86.46000\n86.46000\n17879600\n\n\n2022-12-29\n87.03000\n89.36500\n86.99000\n88.95000\n88.95000\n18280700\n\n\n2022-12-30\n87.36500\n88.83000\n87.03000\n88.73000\n88.73000\n19190300\n\n\n2023-01-03\n89.83000\n91.55000\n89.02000\n89.70000\n89.70000\n20738500\n\n\n2023-01-04\n91.01000\n91.24000\n87.80000\n88.71000\n88.71000\n27046500\n\n\n2023-01-05\n88.07000\n88.21000\n86.56000\n86.77000\n86.77000\n23136100\n\n\n2023-01-06\n87.36000\n88.47000\n85.57000\n88.16000\n88.16000\n26612600\n\n\n2023-01-09\n89.19500\n90.83000\n88.58000\n88.80000\n88.80000\n22996700\n\n\n2023-01-10\n86.72000\n89.47500\n86.70000\n89.24000\n89.24000\n22855600\n\n\n2023-01-11\n90.06000\n92.45000\n89.74000\n92.26000\n92.26000\n25998800\n\n\n2023-01-12\n92.40000\n92.62000\n90.57000\n91.91000\n91.91000\n22754200\n\n\n2023-01-13\n91.52800\n92.98000\n90.93000\n92.80000\n92.80000\n18630700\n\n\n2023-01-17\n92.78000\n92.97000\n90.84000\n92.16000\n92.16000\n22935800\n\n\n2023-01-18\n92.94000\n93.58800\n91.40000\n91.78000\n91.78000\n19641600\n\n\n2023-01-19\n91.39000\n94.40000\n91.38000\n93.91000\n93.91000\n28707700\n\n\n2023-01-20\n95.95000\n99.42000\n95.91000\n99.28000\n99.28000\n53704800\n\n\n2023-01-23\n99.13000\n101.40000\n98.75000\n101.21000\n101.21000\n31791800\n\n\n2023-01-24\n99.55000\n101.09000\n98.70000\n99.21000\n99.21000\n27391400\n\n\n2023-01-25\n97.20000\n97.72000\n95.26200\n96.73000\n96.73000\n31000900\n\n\n2023-01-26\n98.28000\n99.21000\n96.82000\n99.16000\n99.16000\n24542100\n\n\n2023-01-27\n99.05000\n101.58000\n98.97000\n100.71000\n100.71000\n29020400\n\n\n2023-01-30\n98.74500\n99.40900\n97.52000\n97.95000\n97.95000\n24365100\n\n\n2023-01-31\n97.86000\n99.91000\n97.79000\n99.87000\n99.87000\n22306800\n\n\n2023-02-01\n99.74000\n102.19000\n98.42000\n101.43000\n101.43000\n26392600\n\n\n2023-02-02\n106.79000\n108.82000\n106.54000\n108.80000\n108.80000\n46622600\n\n\n2023-02-03\n103.51000\n108.02000\n103.30000\n105.22000\n105.22000\n36823400\n\n\n2023-02-06\n102.68500\n104.70000\n102.21000\n103.47000\n103.47000\n25573000\n\n\n2023-02-07\n103.63000\n108.67000\n103.54800\n108.04000\n108.04000\n33738800\n\n\n2023-02-08\n102.69000\n103.58000\n98.45500\n100.00000\n100.00000\n73546000\n\n\n2023-02-09\n100.54000\n100.61000\n93.86000\n95.46000\n95.46000\n97798600\n\n\n2023-02-10\n95.74000\n97.02000\n94.53000\n94.86000\n94.86000\n49325300\n\n\n2023-02-13\n95.01000\n95.35000\n94.05000\n95.00000\n95.00000\n43116600\n\n\n2023-02-14\n94.66000\n95.17500\n92.65000\n94.95000\n94.95000\n42513100\n\n\n2023-02-15\n94.74000\n97.34000\n94.36000\n97.10000\n97.10000\n36964500\n\n\n2023-02-16\n95.54000\n97.88000\n94.97000\n95.78000\n95.78000\n35642100\n\n\n2023-02-17\n95.07000\n95.75000\n93.45000\n94.59000\n94.59000\n31095100\n\n\n2023-02-21\n93.24000\n93.41500\n92.00000\n92.05000\n92.05000\n28367200\n\n\n2023-02-22\n91.93400\n92.36000\n90.87000\n91.80000\n91.80000\n29891100\n\n\n2023-02-23\n92.13000\n92.13000\n90.01000\n91.07000\n91.07000\n32423700\n\n\n2023-02-24\n89.63000\n90.13000\n88.86000\n89.35000\n89.35000\n31295600\n\n\n2023-02-27\n90.09000\n90.45000\n89.61000\n90.10000\n90.10000\n22724300\n\n\n2023-02-28\n89.54000\n91.45000\n89.52000\n90.30000\n90.30000\n30546900\n\n\n2023-03-01\n90.16000\n91.20000\n89.85000\n90.51000\n90.51000\n26323900\n\n\n2023-03-02\n89.86000\n92.48000\n89.77000\n92.31000\n92.31000\n23328600\n\n\n2023-03-03\n92.74000\n94.11000\n92.66000\n94.02000\n94.02000\n30242500\n\n\n2023-03-06\n94.36000\n96.30000\n94.30000\n95.58000\n95.58000\n28288200\n\n\n2023-03-07\n95.42000\n96.09000\n93.84400\n94.17000\n94.17000\n24101500\n\n\n2023-03-08\n94.40500\n96.24000\n94.40500\n94.65000\n94.65000\n25395200\n\n\n2023-03-09\n94.49000\n95.92000\n92.35500\n92.66000\n92.66000\n24438900\n\n\n2023-03-10\n92.50000\n93.18000\n90.80000\n91.01000\n91.01000\n32850100\n\n\n2023-03-13\n90.56500\n93.08000\n89.94000\n91.66000\n91.66000\n31508600\n\n\n2023-03-14\n93.07000\n94.83000\n92.78000\n94.25000\n94.25000\n32303900\n\n\n2023-03-15\n93.54000\n97.25000\n93.04000\n96.55000\n96.55000\n38367300\n\n\n2023-03-16\n96.57000\n101.97000\n95.87000\n101.07000\n101.07000\n54499500\n\n\n2023-03-17\n100.84000\n103.49000\n100.75000\n102.46000\n102.46000\n76140300\n\n\n2023-03-20\n101.06000\n102.58000\n100.79000\n101.93000\n101.93000\n26033900\n\n\n2023-03-21\n101.98000\n105.96000\n101.86000\n105.84000\n105.84000\n33122800\n\n\n2023-03-22\n105.14000\n107.51000\n104.21000\n104.22000\n104.22000\n32336900\n\n\n2023-03-23\n105.89000\n107.10100\n105.41000\n106.26000\n106.26000\n31385800\n\n\n2023-03-24\n105.74000\n106.16000\n104.74000\n106.06000\n106.06000\n25236200\n\n\n2023-03-27\n105.32000\n105.40000\n102.63000\n103.06000\n103.06000\n25393400\n\n\n2023-03-28\n103.00000\n103.00000\n100.28000\n101.36000\n101.36000\n24913500\n\n\n2023-03-29\n102.72000\n102.82000\n101.03000\n101.90000\n101.90000\n26148300\n\n\n2023-03-30\n101.44000\n101.61000\n100.29000\n101.32000\n101.32000\n25009800\n\n\n2023-03-31\n101.71000\n104.19000\n101.44000\n104.00000\n104.00000\n28108000\n\n\n2023-04-03\n102.67000\n104.95000\n102.38000\n104.91000\n104.91000\n20719900\n\n\n2023-04-04\n104.84000\n106.10000\n104.60000\n105.12000\n105.12000\n20377200\n\n\n2023-04-05\n106.12000\n106.54000\n104.10200\n104.95000\n104.95000\n21864200\n\n\n2023-04-06\n105.77000\n109.63000\n104.81500\n108.90000\n108.90000\n34684200\n\n\n2023-04-10\n107.39000\n107.97000\n105.60000\n106.95000\n106.95000\n19741500\n\n\n2023-04-11\n106.92000\n107.22000\n105.28000\n106.12000\n106.12000\n18721300\n\n\n2023-04-12\n107.39000\n107.58700\n104.97000\n105.22000\n105.22000\n22761600\n\n\n2023-04-13\n106.47000\n108.26500\n106.44000\n108.19000\n108.19000\n21650700\n\n\n2023-04-14\n107.69000\n109.58000\n107.59000\n109.46000\n109.46000\n20758700\n\n\n2023-04-17\n105.43000\n106.71000\n105.32000\n106.42000\n106.42000\n29043400\n\n\n2023-04-18\n107.00000\n107.05000\n104.78000\n105.12000\n105.12000\n17641400\n\n\n2023-04-19\n104.21500\n105.72500\n103.80000\n105.02000\n105.02000\n16732000\n\n\n2023-04-20\n104.65000\n106.88800\n104.64000\n105.90000\n105.90000\n22515300\n\n\n2023-04-21\n106.09000\n106.64000\n105.48500\n105.91000\n105.91000\n22379000\n\n\n2023-04-24\n106.05000\n107.32000\n105.36000\n106.78000\n106.78000\n21410900\n\n\n2023-04-25\n106.61000\n107.44000\n104.56000\n104.61000\n104.61000\n31408100\n\n\n2023-04-26\n105.56000\n107.02000\n103.27000\n104.45000\n104.45000\n37068200\n\n\n2023-04-27\n105.23000\n109.15000\n104.42000\n108.37000\n108.37000\n38235200\n\n\n2023-04-28\n107.80000\n108.29000\n106.04000\n108.22000\n108.22000\n23957900\n\n\n2023-05-01\n107.72000\n108.68000\n107.50000\n107.71000\n107.71000\n20926300\n\n\n2023-05-02\n107.66000\n107.73000\n104.50000\n105.98000\n105.98000\n20343100\n\n\n2023-05-03\n106.22000\n108.13000\n105.62000\n106.12000\n106.12000\n17116300\n\n\n2023-05-04\n106.16000\n106.30000\n104.70000\n105.21000\n105.21000\n19780600\n\n\n2023-05-05\n105.32000\n106.44000\n104.73900\n106.21500\n106.21500\n20705300\n\n\n2023-05-08\n105.79500\n108.42000\n105.79000\n108.24000\n108.24000\n17266000\n\n\n2023-05-09\n108.78000\n110.59500\n107.72500\n107.94000\n107.94000\n24782400\n\n\n2023-05-10\n108.55000\n113.51000\n108.48000\n112.28000\n112.28000\n47533500\n\n\n2023-05-11\n115.86000\n118.44000\n114.93000\n116.90000\n116.90000\n57115100\n\n\n2023-05-12\n117.00000\n118.26000\n116.55000\n117.92000\n117.92000\n31272500\n\n\n2023-05-15\n116.49000\n118.79500\n116.48000\n116.96000\n116.96000\n22107900\n\n\n2023-05-16\n116.83000\n121.20000\n116.83000\n120.09000\n120.09000\n32370100\n\n\n2023-05-17\n120.18000\n122.28000\n119.46000\n121.48000\n121.48000\n26659600\n\n\n2023-05-18\n121.56000\n123.90000\n121.49000\n123.52000\n123.52000\n27014500\n\n\n2023-05-19\n124.20000\n126.47900\n122.72000\n123.25000\n123.25000\n30251300\n\n\n2023-05-22\n123.51000\n127.05000\n123.45000\n125.87000\n125.87000\n29760200\n\n\n2023-05-23\n124.93000\n125.42000\n123.05000\n123.29000\n123.29000\n24477900\n\n\n2023-05-24\n121.88000\n122.75000\n120.75000\n121.64000\n121.64000\n23087900\n\n\n2023-05-25\n125.21000\n125.98000\n122.90000\n124.35000\n124.35000\n33812700\n\n\n2023-05-26\n124.06500\n126.00000\n123.29000\n125.43000\n125.43000\n25154700\n\n\n2023-05-30\n126.29000\n126.38000\n122.89000\n124.64000\n124.64000\n27230700\n\n\n2023-05-31\n123.70000\n124.90000\n123.10000\n123.37000\n123.37000\n41548800\n\n\n2023-06-01\n123.50000\n125.04000\n123.30000\n124.37000\n124.37000\n25017700\n\n\n2023-06-02\n124.49000\n126.74500\n124.35000\n125.23000\n125.23000\n19362400\n\n\n2023-06-05\n124.61000\n127.99000\n124.38000\n126.63000\n126.63000\n22672500\n\n\n2023-06-06\n126.60000\n128.88001\n125.97000\n127.91000\n127.91000\n19450100\n\n\n2023-06-07\n127.57500\n129.55000\n122.63000\n122.94000\n122.94000\n34179300\n\n\n2023-06-08\n122.58500\n123.73000\n122.01000\n122.67000\n122.67000\n24815000\n\n\n2023-06-09\n122.56000\n124.28500\n122.42000\n122.87000\n122.87000\n20304500\n\n\n2023-06-12\n123.39500\n124.75000\n122.35000\n124.35000\n124.35000\n22255700\n\n\n2023-06-13\n125.65000\n125.86000\n123.84500\n124.43000\n124.43000\n19287700\n\n\n2023-06-14\n123.80000\n124.79000\n122.16000\n124.38000\n124.38000\n24659600\n\n\n2023-06-15\n123.88000\n126.16000\n123.14000\n125.79000\n125.79000\n24517100\n\n\n2023-06-16\n126.70000\n126.70000\n123.79000\n124.06000\n124.06000\n56686800\n\n\n2023-06-20\n123.53500\n125.17500\n122.83000\n123.85000\n123.85000\n22698000\n\n\n2023-06-21\n123.23500\n123.41000\n120.86000\n121.26000\n121.26000\n22612000\n\n\n2023-06-22\n120.66000\n123.93500\n119.60000\n123.87000\n123.87000\n20781900\n\n\n2023-06-23\n122.04000\n123.44000\n121.86000\n123.02000\n123.02000\n29542900\n\n\n2023-06-26\n121.46600\n122.72000\n118.99000\n119.09000\n119.09000\n23185000\n\n\n2023-06-27\n117.84000\n119.89500\n116.91000\n119.01000\n119.01000\n27221700\n\n\n2023-06-28\n117.96000\n121.27000\n117.60000\n121.08000\n121.08000\n19753100\n\n\n2023-06-29\n120.09000\n120.91000\n119.21000\n120.01000\n120.01000\n18517500\n\n\n2023-06-30\n121.10000\n122.03000\n120.88000\n120.97000\n120.97000\n23865800\n\n\n2023-07-03\n120.32000\n121.02000\n119.70500\n120.56000\n120.56000\n13888300\n\n\n2023-07-05\n120.06000\n123.37000\n120.06000\n122.63000\n122.63000\n17830300\n\n\n2023-07-06\n120.64000\n121.15000\n119.25000\n120.93000\n120.93000\n17732500\n\n\n2023-07-07\n120.89000\n121.75000\n120.09000\n120.14000\n120.14000\n20982400\n\n\n2023-07-10\n119.07000\n119.07000\n116.64000\n116.87000\n116.87000\n32960100\n\n\n2023-07-11\n116.76000\n118.22500\n115.83000\n117.71000\n117.71000\n18286600\n\n\n2023-07-12\n119.30000\n120.96000\n119.00000\n119.62000\n119.62000\n22059600\n\n\n2023-07-13\n121.54000\n125.33500\n121.06000\n124.83000\n124.83000\n31535900\n\n\n2023-07-14\n125.13000\n127.09000\n124.90000\n125.70000\n125.70000\n20482800\n\n\n2023-07-17\n126.06000\n127.28000\n124.50000\n125.06000\n125.06000\n20675300\n\n\n2023-07-18\n124.90500\n124.99000\n123.30000\n124.08000\n124.08000\n21071200\n\n\n2023-07-19\n124.79000\n125.47000\n122.47000\n122.78000\n122.78000\n22313800\n\n\n2023-07-20\n122.12000\n124.70000\n118.68500\n119.53000\n119.53000\n27541700\n\n\n2023-07-21\n120.87000\n121.30000\n119.07000\n120.31000\n120.31000\n56498100\n\n\n2023-07-24\n121.92600\n123.35000\n121.38000\n121.88000\n121.88000\n22276100\n\n\n2023-07-25\n121.88000\n123.69000\n121.53000\n122.79000\n122.79000\n31820800\n\n\n2023-07-26\n130.36000\n131.36999\n128.71001\n129.66000\n129.66000\n46216900\n\n\n2023-07-27\n131.80000\n133.60001\n129.17999\n129.86999\n129.86999\n35931600\n\n\n2023-07-28\n130.97000\n134.07001\n130.92000\n133.01000\n133.01000\n26971000\n\n\n2023-07-31\n133.01000\n133.83000\n132.13001\n133.11000\n133.11000\n18381900\n\n\n2023-08-01\n130.85500\n132.92000\n130.75000\n131.89000\n131.89000\n22154300\n\n\n2023-08-02\n129.84000\n130.42000\n127.85000\n128.64000\n128.64000\n22705800\n\n\n2023-08-03\n128.36999\n129.77000\n127.77500\n128.77000\n128.77000\n15018100\n\n\n2023-08-04\n129.60001\n131.92999\n128.31500\n128.53999\n128.53999\n20509500\n\n\n2023-08-07\n129.51000\n132.06000\n129.42999\n131.94000\n131.94000\n17621000\n\n\n2023-08-08\n130.98000\n131.94000\n130.13001\n131.84000\n131.84000\n16836000\n\n\n2023-08-09\n132.19000\n132.47000\n129.50501\n130.14999\n130.14999\n17745200\n\n\n2023-08-10\n131.97000\n132.64700\n130.03500\n130.21001\n130.21001\n17855700\n\n\n2023-08-11\n129.20200\n130.44000\n128.75000\n130.17000\n130.17000\n15191500\n\n\n2023-08-14\n129.85001\n131.91000\n129.59000\n131.83000\n131.83000\n17526200\n\n\n2023-08-15\n131.59000\n131.99000\n129.81900\n130.27000\n130.27000\n14769200\n\n\n2023-08-16\n129.28000\n130.89800\n128.46001\n129.11000\n129.11000\n17548400\n\n\n2023-08-17\n130.45000\n132.49100\n129.85001\n130.46001\n130.46001\n23665600\n\n\n2023-08-18\n129.06000\n129.83000\n127.00000\n128.11000\n128.11000\n23619400\n\n\n2023-08-21\n127.85000\n129.26000\n127.16000\n128.92999\n128.92999\n21851100\n\n\n2023-08-22\n129.13001\n130.95000\n128.92500\n129.69000\n129.69000\n15569400\n\n\n2023-08-23\n130.85001\n134.07001\n130.51000\n133.21001\n133.21001\n26497000\n\n\n2023-08-24\n134.72700\n134.97000\n130.30000\n130.42000\n130.42000\n18680400\n\n\n2023-08-25\n130.14000\n131.39999\n128.03999\n130.69000\n130.69000\n20678100\n\n\n2023-08-28\n132.08000\n133.24000\n130.85001\n131.78999\n131.78999\n16715500\n\n\n2023-08-29\n132.99800\n137.29500\n132.98000\n135.49000\n135.49000\n30803300\n\n\n2023-08-30\n135.57001\n137.25000\n135.02100\n136.92999\n136.92999\n21773400\n\n\n2023-08-31\n137.05000\n138.39999\n136.82001\n137.35001\n137.35001\n28147900\n\n\n2023-09-01\n138.42999\n138.58000\n135.94000\n136.80000\n136.80000\n16665700\n\n\n2023-09-05\n136.44000\n137.36999\n135.56000\n136.71001\n136.71001\n17730200\n\n\n2023-09-06\n137.01500\n137.48000\n134.69000\n135.36999\n135.36999\n15814300\n\n\n2023-09-07\n134.60001\n136.58000\n133.96001\n136.20000\n136.20000\n16976000\n\n\n2023-09-08\n135.86999\n137.51500\n135.86999\n137.20000\n137.20000\n17810700\n\n\n2023-09-11\n137.38001\n138.26401\n136.55000\n137.74000\n137.74000\n17180800\n\n\n2023-09-12\n137.13001\n137.64000\n135.92999\n136.07001\n136.07001\n15212900\n\n\n2023-09-13\n135.89999\n137.70000\n134.92999\n137.50000\n137.50000\n16394900\n\n\n2023-09-14\n138.39000\n139.55000\n137.06000\n138.99000\n138.99000\n19064600\n\n\n2023-09-15\n138.80000\n139.36000\n137.17999\n138.30000\n138.30000\n48947600\n\n\n2023-09-18\n137.63001\n139.92999\n137.63001\n138.96001\n138.96001\n16233600\n\n\n2023-09-19\n138.25000\n139.17500\n137.50000\n138.83000\n138.83000\n15479100\n\n\n2023-09-20\n138.83000\n138.84000\n134.52000\n134.59000\n134.59000\n21473500\n\n\n2023-09-21\n132.39000\n133.19000\n131.09000\n131.36000\n131.36000\n22042700\n\n\n2023-09-22\n131.67999\n133.01000\n130.51000\n131.25000\n131.25000\n17348700\n\n\n2023-09-25\n130.77000\n132.22000\n130.03000\n132.17000\n132.17000\n14650000\n\n\n2023-09-26\n130.91400\n131.40500\n128.19000\n129.45000\n129.45000\n20378800\n\n\n2023-09-27\n129.44000\n131.72000\n129.38001\n131.46001\n131.46001\n18764200\n\n\n2023-09-28\n130.69000\n134.17999\n130.69000\n133.13001\n133.13001\n18201400\n\n\n2023-09-29\n134.08000\n134.89000\n131.32001\n131.85001\n131.85001\n23224200\n\n\n2023-10-02\n132.15500\n135.36000\n132.06500\n135.17000\n135.17000\n19210400\n\n\n2023-10-03\n134.92999\n135.24000\n132.81500\n133.30000\n133.30000\n19628700\n\n\n2023-10-04\n133.66000\n136.57001\n133.42999\n136.27000\n136.27000\n22848000\n\n\n2023-10-05\n136.13001\n136.50000\n134.45500\n135.99000\n135.99000\n15922900\n\n\n2023-10-06\n134.94000\n139.18600\n134.94000\n138.73000\n138.73000\n20819300\n\n\n2023-10-09\n137.99000\n139.97000\n136.70000\n139.50000\n139.50000\n16599100\n\n\n2023-10-10\n139.51000\n140.74000\n138.42999\n139.20000\n139.20000\n19554900\n\n\n2023-10-11\n139.85001\n142.22000\n139.84000\n141.70000\n141.70000\n20146300\n\n\n2023-10-12\n142.16000\n142.38001\n139.45000\n140.28999\n140.28999\n18173100\n\n\n2023-10-13\n140.64999\n141.34000\n137.97000\n138.58000\n138.58000\n19438700\n\n\n2023-10-16\n139.73000\n140.90500\n139.32001\n140.49000\n140.49000\n17345600\n\n\n2023-10-17\n140.03000\n141.25000\n138.52901\n140.99000\n140.99000\n17424000\n\n\n2023-10-18\n140.75000\n141.99000\n138.70500\n139.28000\n139.28000\n18304900\n\n\n2023-10-19\n139.80000\n141.00501\n138.60001\n138.98000\n138.98000\n21831200\n\n\n2023-10-20\n138.59000\n139.03999\n136.24499\n136.74000\n136.74000\n24953900\n\n\n2023-10-23\n136.23000\n139.02000\n135.11000\n137.89999\n137.89999\n20780700\n\n\n2023-10-24\n139.16000\n140.71001\n138.75000\n140.11999\n140.11999\n26535200\n\n\n2023-10-25\n129.77000\n130.10001\n126.09000\n126.67000\n126.67000\n58796100\n\n\n2023-10-26\n124.47000\n125.46000\n122.32000\n123.44000\n123.44000\n33907400\n\n\n2023-10-27\n124.03000\n124.44000\n121.46000\n123.40000\n123.40000\n37367700\n\n\n2023-10-30\n124.46000\n126.55000\n123.88000\n125.75000\n125.75000\n24165600\n\n\n2023-10-31\n126.27000\n126.56000\n123.92500\n125.30000\n125.30000\n21123400\n\n\n2023-11-01\n125.34000\n127.74000\n124.92500\n127.57000\n127.57000\n26536600\n\n\n2023-11-02\n129.56000\n130.09000\n128.11000\n128.58000\n128.58000\n24091700\n\n\n2023-11-03\n129.09000\n130.73000\n129.01000\n130.36999\n130.36999\n19517900\n\n\n2023-11-06\n130.22000\n131.56000\n129.92999\n131.45000\n131.45000\n15360400\n\n\n2023-11-07\n131.98000\n133.28000\n131.14000\n132.39999\n132.39999\n19223800\n\n\n2023-11-08\n132.36000\n133.53999\n132.16000\n133.26000\n133.26000\n15093600\n\n\n2023-11-09\n133.36000\n133.96001\n131.51000\n131.69000\n131.69000\n17976500\n\n\n2023-11-10\n131.53000\n134.27000\n130.86999\n134.06000\n134.06000\n20872900\n\n\n2023-11-13\n133.36000\n134.11000\n132.77000\n133.64000\n133.64000\n16409900\n\n\n2023-11-14\n135.64999\n137.24000\n135.10001\n135.42999\n135.42999\n22317300\n\n\n2023-11-15\n136.64000\n136.84000\n135.33000\n136.38001\n136.38001\n15840900\n\n\n2023-11-16\n136.96001\n138.88001\n136.08000\n138.70000\n138.70000\n17615100\n\n\n2023-11-17\n137.82001\n138.00000\n135.48000\n136.94000\n136.94000\n25565300\n\n\n2023-11-20\n135.50000\n138.42500\n135.49000\n137.92000\n137.92000\n19569400\n\n\n2023-11-21\n137.94000\n138.96500\n137.70500\n138.61999\n138.61999\n17648100\n\n\n2023-11-22\n139.10001\n141.10001\n139.00000\n140.02000\n140.02000\n17306400\n\n\n2023-11-24\n139.53999\n139.67700\n137.47000\n138.22000\n138.22000\n8828600\n\n\n2023-11-27\n137.57001\n139.63001\n137.53999\n138.05000\n138.05000\n17886400\n\n\n2023-11-28\n137.63001\n138.66000\n137.03999\n138.61999\n138.61999\n17046900\n\n\n2023-11-29\n138.98500\n139.67000\n136.29500\n136.39999\n136.39999\n21014700\n\n\n2023-11-30\n136.39999\n136.96001\n132.78999\n133.92000\n133.92000\n29913500\n\n\n2023-12-01\n133.32001\n133.50000\n132.15199\n133.32001\n133.32001\n24258400\n\n\n2023-12-04\n131.29401\n131.45000\n129.39999\n130.63001\n130.63001\n24117100\n\n\n2023-12-05\n130.36999\n133.53999\n129.73000\n132.39000\n132.39000\n19235100\n\n\n2023-12-06\n132.89999\n133.31000\n131.31000\n131.42999\n131.42999\n16360600\n\n\n2023-12-07\n136.60001\n140.00000\n136.23000\n138.45000\n138.45000\n38419400\n\n\n2023-12-08\n135.66000\n137.99000\n135.57001\n136.64000\n136.64000\n22990900\n\n\n2023-12-11\n133.82001\n134.78999\n132.89000\n134.70000\n134.70000\n24502900\n\n\n2023-12-12\n133.27000\n134.53999\n132.83000\n133.64000\n133.64000\n26584000\n\n\n2023-12-13\n134.54500\n134.78000\n132.95000\n133.97000\n133.97000\n25414500\n\n\n2023-12-14\n134.77000\n135.03500\n131.06000\n133.20000\n133.20000\n29619100\n\n\n2023-12-15\n132.92000\n134.83000\n132.63001\n133.84000\n133.84000\n58569400\n\n\n2023-12-18\n133.86000\n138.38001\n133.77000\n137.19000\n137.19000\n25699800\n\n\n2023-12-19\n138.00000\n138.77000\n137.45000\n138.10001\n138.10001\n20661000\n\n\n2023-12-20\n140.33000\n143.07800\n139.41000\n139.66000\n139.66000\n33507300\n\n\n2023-12-21\n140.77000\n142.03000\n140.47301\n141.80000\n141.80000\n18101500\n\n\n2023-12-22\n142.13001\n143.25000\n142.05499\n142.72000\n142.72000\n18494700\n\n\n2023-12-26\n142.98000\n143.94501\n142.50000\n142.82001\n142.82001\n11170100\n\n\n2023-12-27\n142.83000\n143.32001\n141.05100\n141.44000\n141.44000\n17288400\n\n\n2023-12-28\n141.85001\n142.27000\n140.82800\n141.28000\n141.28000\n12192500\n\n\n2023-12-29\n140.67999\n141.43500\n139.89999\n140.92999\n140.92999\n14872700",
    "crumbs": [
      "Datasets",
      "Google"
    ]
  },
  {
    "objectID": "activity31.html",
    "href": "activity31.html",
    "title": "Activity31",
    "section": "",
    "text": "Today, we’ll review the structure of ETS models, derive the forecasting equations, and then explore specific variants including ETS(M,N,A), ETS(M,A,A), ETS(A,Ad,A), and ETS(A,A,M).\nLast class, we calculated the equations,\n\\[\n\\begin{align}\ny_t &= (\\ell_{t-1}+s_{t-m})(1+\\varepsilon_t),\\\\[1mm]\n\\ell_t &= \\ell_{t-1}+\\alpha\\,(\\ell_{t-1}+s_{t-m})\\,\\varepsilon_t,\\\\[1mm]\ns_t &= s_{t-m}+\\gamma\\,(\\ell_{t-1}+s_{t-m})\\,\\varepsilon_t,\n\\end{align}\n\\]\nfor the ETS(M,N,A) case and\n\\[\n\\begin{align}\ny_t &= (\\ell_{t-1}+b_{t-1}+s_{t-m})(1+\\varepsilon_t),\\\\[1mm]\n\\ell_t &= \\ell_{t-1}+b_{t-1}+\\alpha\\,(\\ell_{t-1}+b_{t-1}+s_{t-m})\\,\\varepsilon_t,\\\\[1mm]\nb_t &= b_{t-1}+\\beta\\,(\\ell_{t-1}+b_{t-1}+s_{t-m})\\,\\varepsilon_t,\\\\[1mm]\ns_t &= s_{t-m}+\\gamma\\,(\\ell_{t-1}+b_{t-1}+s_{t-m})\\,\\varepsilon_t,\n\\end{align}\n\\]\nfor the ETS(M,A,A) case.",
    "crumbs": [
      "Week 8",
      "Activity 31"
    ]
  },
  {
    "objectID": "activity31.html#recursive-updating-equations-summary",
    "href": "activity31.html#recursive-updating-equations-summary",
    "title": "Activity31",
    "section": "1. Recursive Updating Equations Summary",
    "text": "1. Recursive Updating Equations Summary\n\nStep 1: Compute the forecast \\(\\mu_t\\) based on current state components.\nStep 2: Calculate the error \\(\\varepsilon_t\\) (multiplicative or additive).\nStep 3: Update state components (level, trend, seasonal) using the error and smoothing parameters.\nStep 4: Repeat for each new observation.",
    "crumbs": [
      "Week 8",
      "Activity 31"
    ]
  },
  {
    "objectID": "activity31.html#deriving-the-etsmna-model",
    "href": "activity31.html#deriving-the-etsmna-model",
    "title": "Activity31",
    "section": "2. Deriving the ETS(M,N,A) Model",
    "text": "2. Deriving the ETS(M,N,A) Model\n\n2.1 Model Structure\nFor ETS(M,N,A) (multiplicative error, no trend, additive seasonality), the equations are:\n\\[\n\\begin{align}\ny_t &= (\\ell_{t-1}+s_{t-m})(1+\\varepsilon_t),\\\\[1mm]\n\\ell_t &= \\ell_{t-1}+\\alpha\\,(\\ell_{t-1}+s_{t-m})\\,\\varepsilon_t,\\\\[1mm]\ns_t &= s_{t-m}+\\gamma\\,(\\ell_{t-1}+s_{t-m})\\,\\varepsilon_t,\n\\end{align}\n\\]\nwhere:\n\n\\(y_t\\) is the observed value at time \\(t\\).\n\\(\\ell_{t-1}\\) is the level component.\n\\(s_{t-m}\\) is the seasonal component (with period \\(m\\)).\n\\(\\varepsilon_t\\) is the multiplicative error, defined as \\[\n\\varepsilon_t = \\frac{y_t}{\\ell_{t-1}+s_{t-m}}-1.\n\\]\n\\(\\alpha\\) and \\(\\gamma\\) are the smoothing parameters.\n\n\n\n2.2 Interpretation\n\nObservation Equation: The forecast is given by the sum of the level and seasonal component, scaled by the multiplicative error.\nLevel Update: The level is adjusted by the error scaled by the current forecast.\nSeasonal Update: The seasonal component is updated similarly, ensuring seasonal effects propagate correctly.",
    "crumbs": [
      "Week 8",
      "Activity 31"
    ]
  },
  {
    "objectID": "activity31.html#etsmaa-model",
    "href": "activity31.html#etsmaa-model",
    "title": "Activity31",
    "section": "3. ETS(M,A,A) Model",
    "text": "3. ETS(M,A,A) Model\n\n3.1 Model Structure\nFor ETS(M,A,A) (multiplicative error, additive trend, additive seasonality), an additional trend component \\(b_{t-1}\\) is introduced:\n\\[\n\\begin{align}\ny_t &= (\\ell_{t-1}+b_{t-1}+s_{t-m})(1+\\varepsilon_t),\\\\[1mm]\n\\ell_t &= \\ell_{t-1}+b_{t-1}+\\alpha\\,(\\ell_{t-1}+b_{t-1}+s_{t-m})\\,\\varepsilon_t,\\\\[1mm]\nb_t &= b_{t-1}+\\beta\\,(\\ell_{t-1}+b_{t-1}+s_{t-m})\\,\\varepsilon_t,\\\\[1mm]\ns_t &= s_{t-m}+\\gamma\\,(\\ell_{t-1}+b_{t-1}+s_{t-m})\\,\\varepsilon_t.\n\\end{align}\n\\]\n\n\n3.2 Interpretation\n\nTrend Component: The term \\(b_{t-1}\\) represents the additive trend.\nTrend Update: The trend is adjusted with its own smoothing parameter \\(\\beta\\).\nThe observation and seasonal equations are similar to ETS(M,N,A) but include the trend in the forecast.",
    "crumbs": [
      "Week 8",
      "Activity 31"
    ]
  },
  {
    "objectID": "activity31.html#exploring-other-combinations",
    "href": "activity31.html#exploring-other-combinations",
    "title": "Activity31",
    "section": "4. Exploring Other Combinations",
    "text": "4. Exploring Other Combinations\n\n4.1 ETS(A,Ad,A) Model\nThe ETS(A,Ad,A) model uses an additive error, an additive damped trend, and additive seasonality. Its equations can be written as:\n\\[\n\\begin{align}\ny_t &= \\ell_{t-1}+ \\phi d_{t-1}+ s_{t-m}+\\varepsilon_t,\\\\[1mm]\n\\ell_t &= \\ell_{t-1} + \\phi d_{t-1} + \\alpha \\varepsilon_t, \\\\\nd_t &= \\phi d_{t-1} + \\beta \\varepsilon_t, \\\\\ns_t &= s_{t-m} + \\gamma \\varepsilon_t,\n\\end{align}\n\\]\nwhere:\n\n\\(\\varepsilon_t\\) is an additive error.\n\\(d_{t-1}\\) represents the trend component.\n\\(\\phi\\) is the damping parameter, \\(0 &lt; \\phi &lt; 1\\), that reduces the trend effect over time.\n\n\n\n4.2 ETS(A,A,M) Model\nFor the ETS(A,A,M) model, the seasonal component enters multiplicatively while both the error and trend are additive:\n\\[\n\\begin{align}\ny_t &= (\\ell_{t-1}+b_{t-1}+s_{t-m})(1+\\varepsilon_t),\\\\[1mm]\n\\ell_t &= \\ell_{t-1}+b_{t-1}+\\alpha\\,\\varepsilon_t,\\\\[1mm]\nb_t &= b_{t-1}+\\beta\\,\\varepsilon_t,\\\\[1mm]\ns_t &= s_{t-m}\\,(1+\\gamma\\,\\varepsilon_t).\n\\end{align}\n\\]\n\n\n4.3 Key Points in Model Variations\n\nError Type: Whether the error is additive or multiplicative will affect how deviations are incorporated into the state updates.\nTrend Component: The presence (or absence) of a trend (additive or damped) changes the update equations. In a damped trend, the damping parameter \\(\\phi\\) moderates the trend’s influence.\nSeasonal Component: Additive seasonality adds or subtracts a fixed amount, while multiplicative seasonality scales the forecast.",
    "crumbs": [
      "Week 8",
      "Activity 31"
    ]
  },
  {
    "objectID": "activity31.html#fitting-ets-models-with-tidyverts",
    "href": "activity31.html#fitting-ets-models-with-tidyverts",
    "title": "Activity31",
    "section": "5. Fitting ETS Models with Tidyverts",
    "text": "5. Fitting ETS Models with Tidyverts\nThe Tidyverts framework in R (which includes packages such as fable and tsibble) provides a streamlined approach to fitting ETS models. Below is an example workflow.\n\nSunspot Activity Dataset (1749-2023)\n\n\n\n\n\n\n\n\n\n\nETS Analysis Code\n\n\n\n\n\n\n\n\n\n\nDiagnostics & Forecasting",
    "crumbs": [
      "Week 8",
      "Activity 31"
    ]
  },
  {
    "objectID": "activity18.html",
    "href": "activity18.html",
    "title": "Activity18",
    "section": "",
    "text": "📌 No Activity\n\n📝 In-Class Quiz 1",
    "crumbs": [
      "Week 5",
      "Activity 18"
    ]
  },
  {
    "objectID": "activity24.html",
    "href": "activity24.html",
    "title": "Activity24",
    "section": "",
    "text": "Theoretical Background: Volatility Clustering\nVolatility clustering (periods of high/low variability persisting) violates ARIMA’s constant variance assumption. Common in COVID-19 cases (waves) and stock returns (crashes). ARCH (Autoregressive Conditional Heteroskedasticity) tests help detect this phenomenon.\n\nMathematical Foundation\n\nARCH Model:\nThe ARCH(q) model for volatility is defined as:\n\\[  \n\\begin{align*}  \nr_t &= \\mu_t + \\epsilon_t, \\quad \\epsilon_t = \\sigma_t z_t, \\\\  \n\\sigma_t^2 &= \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\dots + \\alpha_q \\epsilon_{t-q}^2  \n\\end{align*}  \n\\]\nwhere \\(z_t \\sim N(0,1)\\), and \\(\\alpha_0 &gt; 0\\), \\(\\alpha_i \\geq 0\\).\nKey Idea:\nVariance \\(\\sigma_t^2\\) depends on past squared residuals (\\(\\epsilon_{t-1}^2, \\dots, \\epsilon_{t-q}^2\\)), capturing volatility clustering.\n\n\n\n\nTesting Steps\n\nFit ARIMA Model:\nModel the mean structure (e.g., log returns).\nExtract Residuals:\nCompute residuals \\(\\epsilon_t\\) and squared residuals \\(\\epsilon_t^2\\).\nVisual Diagnostics:\n\nPlot ACF/PACF of squared residuals.\n\nLook for significant autocorrelation (sign of ARCH effects).\n\nStatistical Tests:\n\nLjung-Box Test: Tests autocorrelation in \\(\\epsilon_t^2\\).\n\nEngle’s ARCH Test: Formal Lagrange Multiplier (LM) test for ARCH effects.\n\n\n\n\nExample 1: COVID-19 Cases\n\nData Preparation\nUse the COVID19 package to fetch daily confirmed cases in Italy (clear volatility during waves):\n\nlibrary(COVID19)\nlibrary(fpp3)\n\n# Get data\ncovid_data &lt;- covid19(country = \"Italy\", level = 1, verbose = FALSE) %&gt;%\n  as_tsibble(index = date) %&gt;%\n  mutate(new_cases = difference(confirmed)) %&gt;%\n  mutate(log_new_cases = difference(log(confirmed))) %&gt;%\n  drop_na(log_new_cases, new_cases) %&gt;% \n  mutate(scaled_cases = scale(log_new_cases))\n\n\n\nModel Fitting & Volatility Check\nFit an ARIMA model to new_cases and inspect residuals:\n\nfit_covid &lt;- covid_data %&gt;%\n  model(ARIMA(scaled_cases ~ pdq(2,1,1) + PDQ(0,0,0)))\n\n# Check residuals for volatility clustering\nfit_covid %&gt;% gg_tsresiduals()\n\n\n\n\n\n\n\naugment(fit_covid) %&gt;%\n  ggplot(aes(x = date, y = .resid^2)) +\n  geom_line() + labs(title = \"Squared Residuals (COVID-19 Cases)\")\n\n\n\n\n\n\n\nfit_covid %&gt;% \n  augment() %&gt;% \n  features(.innov,~ljung_box(.x, lag=24, dof=4)\n  )\n\n# A tibble: 1 × 3\n  .model                                            lb_stat lb_pvalue\n  &lt;chr&gt;                                               &lt;dbl&gt;     &lt;dbl&gt;\n1 ARIMA(scaled_cases ~ pdq(2, 1, 1) + PDQ(0, 0, 0))    920.         0\n\n\nKey Observation: Squared residuals cluster many times between January 2020 and January 2023 (COVID waves), indicating volatility clustering.\n\naugment(fit_covid) %&gt;%\n  ggplot(aes(x = date, y = .resid^2)) +\n  geom_line() + labs(title = \"Squared Residuals (COVID-19 Cases)\")\n\n\n\n\n\n\n\n\n\n\n\nExample 2: Stock Returns\n\nData Preparation\nFetch S&P 500 returns using tq_get() (more realistic volatility with irregular trading days):\n\n# Get S&P 500 data and calculate daily returns\nsp500 &lt;- tq_get(\"^GSPC\", get = \"stock.prices\", from = \"2000-01-01\") %&gt;%\n  as_tsibble(index = date) %&gt;%  \n  fill_gaps() %&gt;% \n  fill(close, .direction = \"down\") %&gt;% \n  mutate(return = difference(log(close))) %&gt;%  \n  drop_na(return)           \n\n\n\nModel Fitting & Volatility Check\n\nfit_sp500 &lt;- sp500 %&gt;%\n  model(ARIMA(return ~ pdq(1,0,1)))  \n\n# Visualize squared residuals for volatility clustering\naugment(fit_sp500) %&gt;%\n  ggplot(aes(x = date, y = .resid^2)) +\n  geom_line(alpha = 0.6) +\n  labs(title = \"Squared Residuals (S&P 500 Returns)\")\n\n\n\n\n\n\n\n\nKey Observations:\n\nSquared residuals cluster during crises (e.g., 2008 financial crisis, 2020 COVID crash).\n\nARIMA assumes constant variance — clustered residuals violate this, signaling volatility clustering.\n\n\n\n\nFix: COVID-19 Cases\n\n1. Extract Residuals\nFirst, extract residuals from the ARIMA model:\n\n# For COVID-19 example:\ncovid_resid &lt;- augment(fit_covid) %&gt;% \n  select(date, .resid)\n\n\n\n2. Visual ARCH Test (ACF/PACF of Squared Residuals)\nPlot autocorrelation in squared residuals:\n\ncovid_resid %&gt;%\n  mutate(resid_sq = .resid^2) %&gt;%\n  gg_tsdisplay(resid_sq, plot_type = \"partial\") +\n  labs(title = \"ACF/PACF of Squared Residuals (COVID-19)\")\n\n\n\n\n\n\n\n\nInterpretation:\n- Significant spikes at lag 1 or higher → Evidence of ARCH effects.\n\n\n3. Engle’s ARCH Test \nUse the FinTS package to test ARCH effects on residuals:\n\nlibrary(FinTS)  # install.packages(\"FinTS\")\n\n# Extract residual vector\nresid_vector &lt;- covid_resid %&gt;% pull(.resid)\n\n# Engle’s ARCH test (lag = 5)\nArchTest(resid_vector, lags = 5, demean = TRUE)\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  resid_vector\nChi-squared = 273.72, df = 5, p-value &lt; 2.2e-16\n\n\nConclusion: If \\(p &lt; 0.05\\), reject the null (ARCH effects exist).\n\n\n\nNext Steps: GARCH Modeling\nIf ARCH effects are detected, use the rugarch package to model volatility:\n\nlibrary(rugarch)  # install.packages(\"rugarch\")\n\n# Fit ARIMA(1,0,1)-GARCH(1,1)\nspec &lt;- ugarchspec(\n  mean.model = list(armaOrder = c(1,1)), \n  variance.model = list(model = \"sGARCH\")\n)\nfit_garch &lt;- ugarchfit(spec, data = resid_vector)\nplot(fit_garch, which = 10)\n\n\n\n\n\n\n\n\n\nParameter Stability\nCheck rolling estimates to assess model stability over time:\n\nroll_fit &lt;- ugarchroll(spec, data = resid_vector, n.ahead = 1, n.start = 1000)\nplot(roll_fit, which = 4)  # Rolling parameter estimates\n\n\n\n\n\n\n\n\n\n\n\nLab activity: S&P 500 Returns\nRepeat the above steps for S&P Returns\n\n# Extract residuals from ARIMA\nsp500_resid &lt;- augment(fit_sp500) %&gt;% select(date, .resid)\n\n# Engle’s test\nresid_sp500 &lt;- sp500_resid %&gt;% pull(.resid)\nArchTest(resid_sp500, lags = 5, demean = TRUE)\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  resid_sp500\nChi-squared = 1049.2, df = 5, p-value &lt; 2.2e-16\n\n# ACF/PACF of squared residuals\nsp500_resid %&gt;%\n  mutate(resid_sq = .resid^2) %&gt;%\n  gg_tsdisplay(resid_sq, plot_type = \"partial\")",
    "crumbs": [
      "Week 6",
      "Activity 24"
    ]
  },
  {
    "objectID": "activity26.html",
    "href": "activity26.html",
    "title": "Activity26",
    "section": "",
    "text": "Theoretical Background: Volatility Clustering\nIn many financial and real‐world time series (e.g., stock returns or COVID‐19 cases), periods of high and low volatility tend to cluster. This violates the constant variance assumption of simple models. ARCH and its generalization, GARCH, allow the conditional variance to change over time, capturing this volatility clustering.\nKey Ideas:\n- ARCH Models: Variance depends on past squared errors.\n- GARCH Models: Extend ARCH by also letting past variances influence today’s volatility.",
    "crumbs": [
      "Week 7",
      "Activity 26"
    ]
  },
  {
    "objectID": "activity26.html#sequential-build-up-of-garch-models",
    "href": "activity26.html#sequential-build-up-of-garch-models",
    "title": "Activity26",
    "section": "Sequential Build-Up of GARCH Models",
    "text": "Sequential Build-Up of GARCH Models\n\n1. Simple ARCH(1) Model\nStart with modeling the return \\(r_t\\) (or residual) as having a constant mean \\(\\mu\\) and a time-varying volatility:\n\\[\n\\begin{aligned}\nr_t &= \\mu + \\epsilon_t, \\quad \\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0,1), \\\\\n\\sigma_t^2 &= \\alpha_0 + \\alpha_1\\,\\epsilon_{t-1}^2.\n\\end{aligned}\n\\]\nInterpretation:\n\nThe current volatility \\(\\sigma_t^2\\) is a function of a constant \\(\\alpha_0\\) plus the influence of the previous period’s squared error, \\(\\alpha_1\\,\\epsilon_{t-1}^2\\).\n\n\n\n\n2. GARCH(1,1) Model\nA natural extension is to include persistence in volatility. The GARCH(1,1) model adds the last period’s variance:\n\\[\n\\begin{aligned}\nr_t &= \\mu + \\epsilon_t, \\quad \\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0,1), \\\\\n\\sigma_t^2 &= \\alpha_0 + \\alpha_1\\,\\epsilon_{t-1}^2 + \\beta_1\\,\\sigma_{t-1}^2.\n\\end{aligned}\n\\]\nInterpretation:\n\n\\(\\alpha_0\\) is the baseline variance.\n\n\\(\\alpha_1\\,\\epsilon_{t-1}^2\\) captures the immediate impact of shocks (news effect).\n\n\\(\\beta_1\\,\\sigma_{t-1}^2\\) shows how past volatility persists into the current period.\n\n\n\n\n3. GARCH(1,1) Model with Drift in the Mean\nOften, we also include a drift (dynamic) in the returns. Here the mean equation and the variance equation are modeled jointly:\n\\[\n\\begin{aligned}\nr_t &= \\mu + \\phi r_{t-1} + \\epsilon_t, \\quad \\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0,1), \\\\\n\\sigma_t^2 &= \\alpha_0 + \\alpha_1\\,\\epsilon_{t-1}^2 + \\beta_1\\,\\sigma_{t-1}^2.\n\\end{aligned}\n\\]\nInterpretation:\n\nThe first line models the return with a constant mean \\(\\mu\\) plus a noise term scaled by the conditional standard deviation \\(\\sigma_t\\).\n\nThe second line builds the volatility equation just as in the GARCH(1,1) model, indicating that today’s variance depends on both the last period’s shock and the previous variance.",
    "crumbs": [
      "Week 7",
      "Activity 26"
    ]
  },
  {
    "objectID": "activity26.html#practice-configurations",
    "href": "activity26.html#practice-configurations",
    "title": "Activity26",
    "section": "Practice Configurations",
    "text": "Practice Configurations\n\nPractice Problem 1: ARCH(1) Model\nProblem:\nWrite the sequential equations for an ARCH(1) model where returns \\(r_t\\) have a constant mean and conditional variance that depends on the previous squared error.\nAnswer:\n\\[\n\\begin{aligned}\nr_t &= \\mu + \\epsilon_t, \\quad \\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0,1),\\\\[1mm]\n\\sigma_t^2 &= \\alpha_0 + \\alpha_1\\,\\epsilon_{t-1}^2.\n\\end{aligned}\n\\]\n\n\nPractice Problem 2: GARCH(1,1) Model\nProblem:\nWrite the sequential equations for a GARCH(1,1) model for returns \\(r_t\\).\nAnswer:\n\\[\n\\begin{aligned}\nr_t &= \\mu + \\epsilon_t, \\quad \\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0,1),\\\\[1mm]\n\\sigma_t^2 &= \\alpha_0 + \\alpha_1\\,\\epsilon_{t-1}^2 + \\beta_1\\,\\sigma_{t-1}^2.\n\\end{aligned}\n\\]\n\n\nPractice Problem 3: GARCH(1,1) Model with Drift\nProblem:\nWrite the sequential equation for a GARCH(1,1) model that includes a drift in the mean of the returns.\nAnswer:\n\\[\n\\begin{aligned}\nr_t &= \\mu + \\epsilon_t, \\quad \\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0,1),\\\\[1mm]\n\\sigma_t^2 &= \\alpha_0 + \\alpha_1\\,\\epsilon_{t-1}^2 + \\beta_1\\,\\sigma_{t-1}^2.\n\\end{aligned}\n\\]\n\\[\nE(\\sigma_t^2) = \\alpha_0 + (\\alpha_1+\\beta_1) E(\\sigma_t^2),\n\\]\nsolving for $E(\\sigma_t^2)$ gives\n\\[\nE(\\sigma_t^2) = \\frac{\\alpha_0}{1-\\alpha_1-\\beta_1}.\n\\]\nPlugging in \\(\\alpha_0=0.02\\), \\(\\alpha_1=0.12\\), and \\(\\beta_1=0.85\\), we have\n\\[\nE(\\sigma_t^2)=\\frac{0.02}{1-0.12-0.85}=\\frac{0.02}{0.03}\\approx0.6667.\n\\]",
    "crumbs": [
      "Week 7",
      "Activity 26"
    ]
  },
  {
    "objectID": "activity33.html",
    "href": "activity33.html",
    "title": "Activity33",
    "section": "",
    "text": "Ensemble forecasting combines predictions from multiple models to average out individual biases and reduce variance. This approach is particularly useful when data may exhibit regime uncertainty or when different models capture complementary features of the series.\nKey Equation:\n\\[\n\\begin{align}\n\\hat{y}_t^{Ensemble} &= \\frac{1}{m}\\sum_{i=1}^m \\hat{y}_t^{(i)}\n\\end{align}\n\\]\nwhere \\(m\\) is the number of models.",
    "crumbs": [
      "Week 9",
      "Activity 33"
    ]
  },
  {
    "objectID": "activity33.html#ensemble-forecasting",
    "href": "activity33.html#ensemble-forecasting",
    "title": "Activity33",
    "section": "",
    "text": "Ensemble forecasting combines predictions from multiple models to average out individual biases and reduce variance. This approach is particularly useful when data may exhibit regime uncertainty or when different models capture complementary features of the series.\nKey Equation:\n\\[\n\\begin{align}\n\\hat{y}_t^{Ensemble} &= \\frac{1}{m}\\sum_{i=1}^m \\hat{y}_t^{(i)}\n\\end{align}\n\\]\nwhere \\(m\\) is the number of models.",
    "crumbs": [
      "Week 9",
      "Activity 33"
    ]
  },
  {
    "objectID": "activity33.html#australian-cement-production-example",
    "href": "activity33.html#australian-cement-production-example",
    "title": "Activity33",
    "section": "Australian Cement Production Example",
    "text": "Australian Cement Production Example\nWe use the aus_production dataset focusing on Australian cement production. The code below loads the data, builds two models (ETS and ARIMA), and forms a simple average ensemble.",
    "crumbs": [
      "Week 9",
      "Activity 33"
    ]
  },
  {
    "objectID": "activity33.html#exploring-ensemble-forecasts-with-simulations",
    "href": "activity33.html#exploring-ensemble-forecasts-with-simulations",
    "title": "Activity33",
    "section": "Exploring Ensemble Forecasts with Simulations",
    "text": "Exploring Ensemble Forecasts with Simulations\nThe generate() function is useful for simulating multiple future scenarios (Monte Carlo simulations). This can help explore forecast uncertainty and the range of possible outcomes.\n\n\n\n\n\n\n\n\nFurther Exploration Prompts:\n\nExperiment with different ensemble weights (e.g., weighted average instead of simple average).\n\nCompare forecast accuracy using measures like MAE or CRPS by splitting your data into training and test sets.\n\nUse residual analysis (e.g., with gg_tsresiduals()) to check for model adequacy.",
    "crumbs": [
      "Week 9",
      "Activity 33"
    ]
  },
  {
    "objectID": "activity33.html#lab-activity-sydney-tourism-ensemble-models",
    "href": "activity33.html#lab-activity-sydney-tourism-ensemble-models",
    "title": "Activity33",
    "section": "Lab Activity: Sydney Tourism Ensemble Models",
    "text": "Lab Activity: Sydney Tourism Ensemble Models\nThis guided activity explores forecasting for Sydney tourism using the tourism dataset. Follow these steps and review the prompts to deepen your understanding:\n\nData Preparation & Model Building:\nFilter the tourism data for Sydney (holiday purpose) and build three models: two ETS variants and an ARIMA model.\n\n\n\n\n\n\n\n\n\n\nModel Comparison:\nCompare the information criteria (e.g., AICc, BIC) and residual variance to decide on the best model.\n\n\n\n\n\n\n\n\n\n\nSimulating Future Scenarios:\nUse the generate() function to simulate 100 future paths over 12 months. Examine the range of forecasts.\n\n\n\n\n\n\n\n\n\nFurther Exploration:\n\nPrompt 1: How do the different model specifications (ETS_ANA vs. ETS_AAA) impact the forecast uncertainty?\n\n\n\n\n\n\n\n\n\nThe ETS_AAA model (additive trend) shows wider forecast intervals than ETS_ANA (no trend) because trend uncertainty compounds over time. The ensemble averages these uncertainties, resulting in low variance.\n\nPrompt 2: Try adjusting the forecast horizon or the number of simulation paths; what changes do you observe in the distribution of outcomes?\n\n\n\n\n\n\n\n\n\nIncreasing the horizon amplifies divergence in simulated paths due to accumulating errors. More paths (e.g., times = 500) better approximate the forecast distribution but don’t fundamentally alter its spread.\n\nPrompt 3: Experiment with combining models using different weights and compare the ensemble’s performance with that of individual models.\n\n\n\n\n\n\n\n\n\n\nPrompt 4: Perform a residual analysis on each model to detect potential structural breaks or model inadequacies.",
    "crumbs": [
      "Week 9",
      "Activity 33"
    ]
  },
  {
    "objectID": "activity33.html#need-to-check-for",
    "href": "activity33.html#need-to-check-for",
    "title": "Activity33",
    "section": "Need to check for:",
    "text": "Need to check for:\n\nAutocorrelation (significant spikes in ACF → poor fit)\n\nNon-normality (histogram skew → invalid prediction intervals)\n\nHeteroscedasticity (changing variance → consider transformations)",
    "crumbs": [
      "Week 9",
      "Activity 33"
    ]
  },
  {
    "objectID": "template/ActivityTemplateRMarkdown.html",
    "href": "template/ActivityTemplateRMarkdown.html",
    "title": "Activity",
    "section": "",
    "text": "# Time Series Essentials, install if needed!\nlibrary(feasts)       # Feature extraction & decomposition\nlibrary(fable)        # Forecasting models (ARIMA, ETS, etc.)\nlibrary(fpp3)         # Tidy time series dataseta\nlibrary(astsa)        # Applied statistical TS methods from textbook\nlibrary(tseries)      # Unit root tests & TS diagnostics\nlibrary(tsibbledata)  # Curated TS datasets\nlibrary(quantmod)     # Financial data retrieval\nlibrary(tidyquant)    # Financial analysis in tidyverse\nlibrary(purrr)        # Functional programming for TS pipelines\nlibrary(readr)        # Efficient data import\n\n\n\n# Your R-code\n\nYour textual explanations .."
  },
  {
    "objectID": "activity22.html",
    "href": "activity22.html",
    "title": "Activity22",
    "section": "",
    "text": "Theoretical Foundations of MA Processes",
    "crumbs": [
      "Week 6",
      "Activity 22"
    ]
  },
  {
    "objectID": "activity22.html#time-series-nature-invertibility",
    "href": "activity22.html#time-series-nature-invertibility",
    "title": "Activity22",
    "section": "Time Series Nature & Invertibility",
    "text": "Time Series Nature & Invertibility\nDefinition:\nMA(\\(q\\)) model:\n\\(Y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q}\\)\nwhere \\(\\epsilon_t \\sim WN(0,\\sigma^2)\\)\nKey Properties:\n\nModels shock persistence through lagged errors\n\nACF cuts off after lag \\(q\\) (distinct signature)\n\nPACF tails off gradually\n\nRequires invertibility (roots of \\(1 + \\theta_1 z + \\cdots + \\theta_q z^q = 0\\) lie outside unit circle)",
    "crumbs": [
      "Week 6",
      "Activity 22"
    ]
  },
  {
    "objectID": "activity22.html#simulating-diagnosing-ma-processes",
    "href": "activity22.html#simulating-diagnosing-ma-processes",
    "title": "Activity22",
    "section": "Simulating & Diagnosing MA Processes",
    "text": "Simulating & Diagnosing MA Processes\n\nSimulated MA(2) Process\n\nlibrary(fable)\nset.seed(123)\nma_data &lt;- tibble(\n  time = 1:100,\n  y = arima.sim(model = list(ma = c(0.5, -0.3)), n = 100) # θ₁=0.5, θ₂=-0.3\n) %&gt;% as_tsibble(index = time)\n\nma_data %&gt;% \n  gg_tsdisplay(y, plot_type = \"scatter\") + # Observe ACF cutoff at lag 2\n  labs(title = \"Simulated MA(2): θ₁=0.5, θ₂=-0.3\")\n\n\n\n\n\n\n\n\n\n\nModel Estimation & Diagnostics\n\nfit_ma &lt;- ma_data %&gt;% \n  model(ARIMA(y ~ pdq(0,0,2))) # Explicit MA(2) specification\n\nreport(fit_ma) # Check θ estimates vs true values (0.5, -0.3)\n\nSeries: y \nModel: ARIMA(0,0,2) \n\nCoefficients:\n         ma1      ma2\n      0.5477  -0.3881\ns.e.  0.0911   0.0908\n\nsigma^2 estimated as 0.8128:  log likelihood=-131.49\nAIC=268.98   AICc=269.23   BIC=276.8\n\nfit_ma %&gt;% \n  residuals() %&gt;% \n  gg_tsdisplay(plot_type = \"scatter\") + \n  labs(title = \"MA(2) Residual Diagnostics\")",
    "crumbs": [
      "Week 6",
      "Activity 22"
    ]
  },
  {
    "objectID": "activity22.html#real-world-case-study-us-consumption",
    "href": "activity22.html#real-world-case-study-us-consumption",
    "title": "Activity22",
    "section": "Real-World Case Study: US Consumption",
    "text": "Real-World Case Study: US Consumption\n\nLab Activity A: Modeling Consumption with MA\n1. Data Preparation\n\n\n\n\n\n\n\n\n2. Exploratory Analysis\n\n\n\n\n\n\n\n\n3. MA Order Identification\n\n\n\n\n\n\n\n\n4. Model Fitting\n\n\n\n\n\n\n\n\n5. Residual Diagnostics\n\n\n\n\n\n\n\n\n6. Forecasting",
    "crumbs": [
      "Week 6",
      "Activity 22"
    ]
  },
  {
    "objectID": "activity22.html#lab-activity-b-modeling-us-production",
    "href": "activity22.html#lab-activity-b-modeling-us-production",
    "title": "Activity22",
    "section": "Lab Activity B: Modeling US Production",
    "text": "Lab Activity B: Modeling US Production\n1. Data Preparation\n\n\n\n\n\n\n\n\n2. Visualize Series\n\n\n\n\n\n\n\n\n3. MA Order Selection\nDetermine appropriate MA order through ACF:\n\n\n\n\n\n\n\n\n4. Model Comparison\nFit competing specifications and evaluate:\n\n\n\n\n\n\n\n\n5. Policy-Relevant Forecasting\nUsing the best model, generate forecasts up-to 10 time points into the future and interpret economic meaning:",
    "crumbs": [
      "Week 6",
      "Activity 22"
    ]
  },
  {
    "objectID": "activity22.html#key-concepts-cheat-sheet",
    "href": "activity22.html#key-concepts-cheat-sheet",
    "title": "Activity22",
    "section": "Key Concepts Cheat Sheet",
    "text": "Key Concepts Cheat Sheet\n\n\n\n\n\n\n\n\nFeature\nMA(\\(q\\))\nContrast with AR(\\(p\\))\n\n\n\n\nACF\nCuts off at lag \\(q\\)\nTails off gradually\n\n\nPACF\nTails off gradually\nCuts off at lag \\(p\\)\n\n\nCondition\nInvertibility (roots &gt; 1)\nStationarity (roots &lt; 1)\n\n\nUse Case\nShort-lived shock effects\nLong-term dependencies",
    "crumbs": [
      "Week 6",
      "Activity 22"
    ]
  },
  {
    "objectID": "activity20.html",
    "href": "activity20.html",
    "title": "Activity20",
    "section": "",
    "text": "Cross-Correlation & Multiserial Dynamics\nObjective: Analyze lead-lag relationships between series\nThe cross-correlation function at lag k is given by:\n\\[\n\\rho_{XY}(k) = \\frac{\\gamma_{XY}(k)}{\\sqrt{\\gamma_{XX}(0)\\,\\gamma_{YY}(0)}}\n\\]\nwhere:\n\n\\(\\gamma_{XY}(k)\\) is the cross-covariance at lag k between series X and Y\n\\(\\gamma_{XX}(0)\\) and \\(\\gamma_{YY}(0)\\) are the variances (covariance at lag 0) of X and Y, respectively\n\\(\\rho_{XY}(k)\\) thus ranges between -1 and +1\n\n\nHow to interpret the CCF?\nThe CCF at lag k measures how strongly the current values of one series correlate with the values of the other series shifted by k time steps.\n\nPositive lag (k &gt; 0) indicates how much the first series LEADS the second.\nNegative lag (k &lt; 0) indicates how much the second series LEADS the first.\nA high absolute value of CCF at a particular lag suggests a strong lead-lag relationship at that time shift.\n\n\n\n1.1 CCF Analysis with pelt\n\npelt |&gt; \n  as_tsibble() |&gt;\n  CCF(Lynx, Hare) |&gt; \n  autoplot() +\n  labs(title = \"Hare peaks lead Lynx by 1-2 years (predator-prey dynamics)\")\n\n\n\n\n\n\n\n\n\nA single cross-correlation plot is sufficient for understanding how two time series move relative to each other over different time shifts.\nPeaks at positive lags suggest that the first series’ changes come before changes in the second series (leading). Peaks at negative lags suggest the second series leads the first.\nIn ecological contexts (like hare vs. lynx), the classic Hudson Bay hare–lynx data show that the hare population tends to peak first, with the lynx population lagging by roughly 1–2 years.\n\n\n\n1.2 Spurious Correlation Caveat\n\n# Simulate independent series\nset.seed(123)\nfake_data &lt;- tsibble(t=1:100, x=rnorm(100), y=rnorm(100), index=t)\nfake_data |&gt; \n  CCF(x,y) |&gt; autoplot()\n\n\n\n\n\n\n\n\nCritical Thinking: Even independent series may show “significant” correlations by chance. Always validate with domain knowledge.\nSimulation & Model Identification\nHands-on ARMA process experimentation\n\n\n1.3 AR(2) Simulation & Diagnostics\n\nsim_ar2 &lt;- arima.sim(n=200, list(ar=c(0.6, -0.3))) \n\nsim_ar2 |&gt; \n  as_tsibble() |&gt;\n  ACF() |&gt; \n  autoplot() # Decaying oscillations\n\n\n\n\n\n\n\nsim_ar2 |&gt; \n  as_tsibble() |&gt;\n  PACF() |&gt; \n  autoplot() # Spikes at lags 1-2\n\n\n\n\n\n\n\n\n\n\n1.4 MA(1) Characteristics\n\nsim_ma1 &lt;- arima.sim(n=200, list(ma=0.8))\n\nsim_ma1 |&gt; \n  as_tsibble() |&gt;\n  ACF() |&gt; \n  autoplot() # Cutoff after lag 1\n\n\n\n\n\n\n\nsim_ma1 |&gt; \n  as_tsibble() |&gt;\n  PACF() |&gt; \n  autoplot() # Exponential decay\n\n\n\n\n\n\n\n\nGolden Rule:\n\nAR(p): PACF significant for first p lags\n\nMA(q): ACF significant for first q lags\n\nARMA: Both decay gradually\n\n\n\nLab Activity 1: Electricity demand and Temperature\nThe vic_elec dataset contains half-hourly electricity demand in Victoria, Australia, alongside temperature readings.\n\nAggregate the data to daily resolution (summing demand and averaging temperature).\nPlot the cross-correlation function (CCF) between daily demand and temperature.\nInterpret the results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CCF values are consistently near or below zero across all lags; there isn’t a pronounced peak or trough at any specific lag. Since the correlations remain broadly negative (and small in magnitude) for both positive and negative lags, there’s no clear evidence that temperature systematically leads or lags demand in this dataset. The correlation is negative, suggesting that higher daily temperatures coincide with slightly lower daily electricity demand (or vice versa). One possible explanation is that the observed period or region may use more electricity for heating rather than cooling, so warmer days reduce demand.",
    "crumbs": [
      "Week 5",
      "Activity 20"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 493: Spring 2025",
    "section": "",
    "text": "Welcome to the world of applied time series!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "activity9.html",
    "href": "activity9.html",
    "title": "Activity9",
    "section": "",
    "text": "We often model a response \\(Y_t\\) as a linear function of a predictor \\(X_t\\):\n\\[\n\\begin{align}\nY_t &= \\beta_0 + \\beta_1 X_t + \\varepsilon_t, \\\\\n\\hat{\\beta}_1 &= \\frac{\\sum (Y_t - \\bar{Y})(X_t - \\bar{X})}{\\sum (X_t - \\bar{X})^2}.\n\\end{align}\n\\]\nWhere:\n\n\\(Y_t\\) is the response at time \\(t\\).\n\\(X_t\\) is the predictor at time \\(t\\).\n\\(\\beta_0\\) is the intercept.\n\\(\\beta_1\\) is the slope.\n\\(\\varepsilon_t\\) is the error term.\n\n\n\nDataset: gafa_stock (Amazon vs Market Index)\n\n# 1) Get S&P 500 data (adjust as needed for start dates):\nsp500 &lt;- tq_get(\"^GSPC\", from = \"2018-01-01\", to = \"2019-12-31\") %&gt;%\n  # Rename for consistency\n  select(date, sp500_close = adjusted)\n\n# 2) Use Amazon data from gafa_stock in fpp3:\namzn &lt;- gafa_stock %&gt;%\n filter(Symbol == \"AMZN\", Date &gt;= \"2018-01-01\", Date &lt;= \"2018-12-31\") %&gt;%\n select(Date, amzn_close = Adj_Close)\n\n# 3) Join by date\namzn_sp &lt;- amzn %&gt;%\n  inner_join(sp500, by = c(\"Date\" = \"date\")) \n\n# Regression coefficients\namzn_sp %&gt;% \n  model(TSLM(amzn_close ~ sp500_close)) %&gt;% \n  tidy() %&gt;% \n  knitr::kable(caption = \"Coefficients table\")\n\n\nCoefficients table\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nTSLM(amzn_close ~ sp500_close)\n(Intercept)\n-1940.961149\n255.8360580\n-7.586738\n0\n\n\nTSLM(amzn_close ~ sp500_close)\nsp500_close\n1.304591\n0.0930976\n14.013160\n0\n\n\n\n\n# 4) Quick visualization\n amzn_sp %&gt;%\n  ggplot(aes(x = sp500_close, y = amzn_close)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Amazon Stock vs S&P 500\",\n    x = \"S&P 500 Adjusted Close\",\n    y = \"Amazon Adjusted Close\"\n  )\n\n\n\n\n\n\n\n\nCOVID Example (Mobility):\nDataset: oxcgrt (Oxford COVID Policy Tracker)\n\nlibrary(readr)\nurl &lt;- \"https://github.com/OxCGRT/covid-policy-dataset/raw/main/data/OxCGRT_compact_national_v1.csv\"\noxcgrt &lt;- read_csv(url)\n\n\nModel: \\(\\log(\\text{ConfirmedCases}_t) = \\beta_0 + \\beta_1\\text{ConfirmedDeaths}_t + \\varepsilon_t\\)\n\nActivity: Test \\(H_0: \\beta_1=0\\) using \\(t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}\\)\n\n\noxcgrt %&gt;% \n  filter(CountryName == \"United States\") %&gt;% \n  mutate(Date = lubridate::ymd(Date)) %&gt;% \n  as_tsibble(index = Date) %&gt;% \n  tidyr::drop_na(ConfirmedCases,ConfirmedDeaths ) %&gt;% \n  model(TSLM(ConfirmedCases ~ ConfirmedDeaths)) %&gt;% \n  tidy()  %&gt;% \n  knitr::kable(caption = \"Coefficients table\")\n\n\nCoefficients table\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nTSLM(ConfirmedCases ~ ConfirmedDeaths)\n(Intercept)\n-1.054628e+07\n4.288403e+05\n-24.59255\n0\n\n\nTSLM(ConfirmedCases ~ ConfirmedDeaths)\nConfirmedDeaths\n9.148213e+01\n6.202599e-01\n147.48998\n0\n\n\n\n\n\n\noxdata &lt;- oxcgrt %&gt;% \n  filter(CountryName == \"United States\") %&gt;% \n  mutate(Date = lubridate::ymd(Date)) %&gt;% \n  as_tsibble(index = Date) \n  \n\noxdata %&gt;% \n  ggplot(aes(x = ConfirmedCases, y = ConfirmedDeaths)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Confirmed Cases vs Confirmed Deaths\",\n    x = \"Confirmed Cases\",\n    y = \"Confirmed Deaths\"\n  )",
    "crumbs": [
      "Week 3",
      "Activity 9"
    ]
  },
  {
    "objectID": "activity9.html#finance-example-amazon-vs.-sp-500",
    "href": "activity9.html#finance-example-amazon-vs.-sp-500",
    "title": "Activity9",
    "section": "",
    "text": "Dataset: gafa_stock (Amazon vs Market Index)\n\n# 1) Get S&P 500 data (adjust as needed for start dates):\nsp500 &lt;- tq_get(\"^GSPC\", from = \"2018-01-01\", to = \"2019-12-31\") %&gt;%\n  # Rename for consistency\n  select(date, sp500_close = adjusted)\n\n# 2) Use Amazon data from gafa_stock in fpp3:\namzn &lt;- gafa_stock %&gt;%\n filter(Symbol == \"AMZN\", Date &gt;= \"2018-01-01\", Date &lt;= \"2018-12-31\") %&gt;%\n select(Date, amzn_close = Adj_Close)\n\n# 3) Join by date\namzn_sp &lt;- amzn %&gt;%\n  inner_join(sp500, by = c(\"Date\" = \"date\")) \n\n# Regression coefficients\namzn_sp %&gt;% \n  model(TSLM(amzn_close ~ sp500_close)) %&gt;% \n  tidy() %&gt;% \n  knitr::kable(caption = \"Coefficients table\")\n\n\nCoefficients table\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nTSLM(amzn_close ~ sp500_close)\n(Intercept)\n-1940.961149\n255.8360580\n-7.586738\n0\n\n\nTSLM(amzn_close ~ sp500_close)\nsp500_close\n1.304591\n0.0930976\n14.013160\n0\n\n\n\n\n# 4) Quick visualization\n amzn_sp %&gt;%\n  ggplot(aes(x = sp500_close, y = amzn_close)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Amazon Stock vs S&P 500\",\n    x = \"S&P 500 Adjusted Close\",\n    y = \"Amazon Adjusted Close\"\n  )\n\n\n\n\n\n\n\n\nCOVID Example (Mobility):\nDataset: oxcgrt (Oxford COVID Policy Tracker)\n\nlibrary(readr)\nurl &lt;- \"https://github.com/OxCGRT/covid-policy-dataset/raw/main/data/OxCGRT_compact_national_v1.csv\"\noxcgrt &lt;- read_csv(url)\n\n\nModel: \\(\\log(\\text{ConfirmedCases}_t) = \\beta_0 + \\beta_1\\text{ConfirmedDeaths}_t + \\varepsilon_t\\)\n\nActivity: Test \\(H_0: \\beta_1=0\\) using \\(t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}\\)\n\n\noxcgrt %&gt;% \n  filter(CountryName == \"United States\") %&gt;% \n  mutate(Date = lubridate::ymd(Date)) %&gt;% \n  as_tsibble(index = Date) %&gt;% \n  tidyr::drop_na(ConfirmedCases,ConfirmedDeaths ) %&gt;% \n  model(TSLM(ConfirmedCases ~ ConfirmedDeaths)) %&gt;% \n  tidy()  %&gt;% \n  knitr::kable(caption = \"Coefficients table\")\n\n\nCoefficients table\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nTSLM(ConfirmedCases ~ ConfirmedDeaths)\n(Intercept)\n-1.054628e+07\n4.288403e+05\n-24.59255\n0\n\n\nTSLM(ConfirmedCases ~ ConfirmedDeaths)\nConfirmedDeaths\n9.148213e+01\n6.202599e-01\n147.48998\n0\n\n\n\n\n\n\noxdata &lt;- oxcgrt %&gt;% \n  filter(CountryName == \"United States\") %&gt;% \n  mutate(Date = lubridate::ymd(Date)) %&gt;% \n  as_tsibble(index = Date) \n  \n\noxdata %&gt;% \n  ggplot(aes(x = ConfirmedCases, y = ConfirmedDeaths)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Confirmed Cases vs Confirmed Deaths\",\n    x = \"Confirmed Cases\",\n    y = \"Confirmed Deaths\"\n  )",
    "crumbs": [
      "Week 3",
      "Activity 9"
    ]
  },
  {
    "objectID": "activity9.html#lab-activity-1",
    "href": "activity9.html#lab-activity-1",
    "title": "Activity9",
    "section": "Lab Activity 1",
    "text": "Lab Activity 1\n\nFilter the data to one country (e.g., ‘United States’).\nCreate a new variable, log_cases, defined as \\(\\\\(\\log(1 + \\text{ConfirmedCases})\\\\)\\). (Adding 1 helps avoid \\(\\\\(\\log(0)\\\\).)\\)\nFit a TSLM model relating log_cases to C6M_Stay at home requirements.\nInterpret the slope coefficient.\n\n\nSolution\n\n# Filter & transform\nexercise1_data &lt;- oxcgrt %&gt;%\n  filter(CountryName == 'United States') %&gt;%\n  mutate(\n    Date = ymd(Date),\n    log_cases = log(1 + ConfirmedCases)\n  ) %&gt;%\n  as_tsibble(index = Date) %&gt;%\n  drop_na(log_cases, `C6M_Stay at home requirements`)\n\n# Fit TSLM\nex1_model &lt;- exercise1_data %&gt;%\n  model(tslm_ex1 = TSLM(log_cases ~ `C6M_Stay at home requirements`))\n\n# Summaries\nex1_model %&gt;% tidy(tslm_ex1) %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntslm_ex1\n(Intercept)\n14.976905\n0.2302570\n65.044305\n0e+00\n\n\ntslm_ex1\nC6M_Stay at home requirements\n0.917216\n0.1722268\n5.325628\n1e-07\n\n\n\n\nex1_model %&gt;% glance(tslm_ex1) %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\ntslm_ex1\n0.0252702\n0.0243792\n15.75292\n28.36231\n1e-07\n2\n-3065.006\n3025.699\n3025.721\n3040.697\n15.81785\n17233.7\n1094\n2",
    "crumbs": [
      "Week 3",
      "Activity 9"
    ]
  },
  {
    "objectID": "activity9.html#lab-activity-2",
    "href": "activity9.html#lab-activity-2",
    "title": "Activity9",
    "section": "Lab Activity 2",
    "text": "Lab Activity 2\n\nFilter to the same (or another) country.\nDefine a new variable, log_deaths = \\(\\\\(\\log(1 + \\text{ConfirmedDeaths})\\\\)\\).\nRegress log_deaths on StringencyIndex_Average using TSLM.\nCheck if increased stringency is correlated with reduced deaths (i.e., a negative slope).\n\n\nSolution\n\n# Filter & transform\nexercise2_data &lt;- oxcgrt %&gt;%\n  filter(CountryName == 'United States') %&gt;%\n  mutate(\n    Date = ymd(Date),\n    log_deaths = log(1 + ConfirmedDeaths)\n  ) %&gt;%\n  as_tsibble(index = Date) %&gt;%\n  drop_na(log_deaths, StringencyIndex_Average)\n\n# Fit TSLM\nex2_model &lt;- exercise2_data %&gt;%\n  model(tslm_ex2 = TSLM(log_deaths ~ StringencyIndex_Average))\n\n# Summaries\nex2_model %&gt;% tidy(tslm_ex2)  %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntslm_ex2\n(Intercept)\n9.3193169\n0.2484414\n37.51112\n0\n\n\ntslm_ex2\nStringencyIndex_Average\n0.0585671\n0.0047442\n12.34504\n0\n\n\n\n\nex2_model %&gt;% glance(tslm_ex2)  %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\ntslm_ex2\n0.1222721\n0.1214698\n9.854598\n152.3999\n0\n2\n-2807.946\n2511.578\n2511.6\n2526.577\n9.911232\n10780.93\n1094\n2",
    "crumbs": [
      "Week 3",
      "Activity 9"
    ]
  },
  {
    "objectID": "activity10.html",
    "href": "activity10.html",
    "title": "Activity10",
    "section": "",
    "text": "Multiple regression models the relationship between a dependent variable and multiple independent variables. In many real‐world scenarios, predictors can be correlated—this is known as multicollinearity. High multicollinearity inflates the variance of the coefficient estimates, making them unstable and difficult to interpret. A common diagnostic measure is the Variance Inflation Factor (VIF). For a regression model:\n\\[\n\\begin{align}\nY_t &= \\beta_0 + \\beta_1 X_{1,t} + \\beta_2 X_{2,t} + \\varepsilon_t,\n\\end{align}\n\\]\nthe VIF for predictor \\(\\\\(X_i\\\\)\\) is computed as:\n\\[\n\\text{VIF}(X_i) = \\frac{1}{1 - R_i^2},\n\\]\nwhere \\(\\\\(R_i^2\\\\)\\) is the coefficient of determination when \\(\\\\(X_i\\\\)\\) is regressed on the other predictors. VIF values greater than 5 (or 10, depending on context) indicate potentially problematic collinearity.\n\n\n\n\nDataset: us_change (available in the fpp3 package)\nModel:\n\\[\n\\text{Consumption}_t = \\beta_0 + \\beta_1 \\text{Income}_t + \\beta_2 \\text{Production}_t + \\varepsilon_t\n\\]\nIn this example, we explore how Income and Production drive Consumption. We then use VIF to diagnose multicollinearity.\n\nlibrary(car)\n\n# Fit the TSLM model on us_change data\nmodel_macro &lt;- us_change %&gt;%\n  model(tslm_macro = TSLM(Consumption ~ Income + Production))\n\n# Display model coefficients\nmodel_macro %&gt;%\n  tidy(tslm_macro) %&gt;%\n  knitr::kable(caption = 'US Macro Model Coefficients')\n\n\nUS Macro Model Coefficients\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntslm_macro\n(Intercept)\n0.5106790\n0.0477604\n10.692528\n0.00e+00\n\n\ntslm_macro\nIncome\n0.1842811\n0.0427041\n4.315301\n2.53e-05\n\n\ntslm_macro\nProduction\n0.1925038\n0.0252739\n7.616697\n0.00e+00\n\n\n\n\n# Compute VIF values using a standard lm object\nmacro_lm &lt;- lm(Consumption ~ Income + Production, data = us_change)\nvif_macro &lt;- car::vif(macro_lm)\nvif_macro\n\n    Income Production \n  1.078113   1.078113 \n\n\n\n\n\nDataset: oxcgrt (Oxford COVID Policy Tracker)\nModel:\n\\[\n\\log(\\text{ConfirmedCases}_t) = \\beta_0 + \\beta_1 \\text{ConfirmedDeaths}_t + \\beta_2 \\text{StringencyIndexAverage}_t + \\varepsilon_t\n\\]\nHere, we investigate the effect of COVID-19 deaths and policy stringency on the growth of confirmed cases, using a log-transformation to account for exponential growth. VIF is computed to check for multicollinearity between the predictors.\n\nlibrary(readr)\nurl &lt;- \"https://github.com/OxCGRT/covid-policy-dataset/raw/main/data/OxCGRT_compact_national_v1.csv\"\noxcgrt &lt;- read_csv(url)\n\n# Filter for the United States and create a log-transformed 'ConfirmedCases' variable\noxcgrt_us &lt;- oxcgrt %&gt;% \n  filter(CountryName == 'United States') %&gt;% \n  mutate(Date = ymd(Date),\n         log_ConfirmedCases = log(1 + ConfirmedCases)) %&gt;% \n  as_tsibble(index = Date) %&gt;% \n  drop_na(log_ConfirmedCases, ConfirmedDeaths, StringencyIndex_Average)\n\n# Fit the TSLM model\nmodel_covid &lt;- oxcgrt_us %&gt;%\n  model(tslm_covid = TSLM(log_ConfirmedCases ~ ConfirmedDeaths + StringencyIndex_Average))\n\n# Display model coefficients\nmodel_covid %&gt;% \n  tidy(tslm_covid) %&gt;%\n  knitr::kable(caption = 'COVID Model Coefficients')\n\n\nCOVID Model Coefficients\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntslm_covid\n(Intercept)\n1.7724524\n0.0956172\n18.53696\n0\n\n\ntslm_covid\nConfirmedDeaths\n0.0000113\n0.0000001\n156.84617\n0\n\n\ntslm_covid\nStringencyIndex_Average\n0.1578871\n0.0013481\n117.12148\n0\n\n\n\n\n# Compute VIF values using a standard lm object (car::vif works with lm)\ncovid_lm &lt;- lm(log_ConfirmedCases ~ ConfirmedDeaths + StringencyIndex_Average, data = as_tibble(oxcgrt_us))\nvif_covid &lt;- car::vif(covid_lm)\nvif_covid\n\n        ConfirmedDeaths StringencyIndex_Average \n               1.270702                1.270702 \n\n\n\n\n\nPrompt:\n\nUsing the us_change dataset, fit a TSLM model with Consumption as the response and all of the remaining variables as predictors.\n\nCompute the VIF for each predictor using the car::vif() function.\n\nInterpret the VIF values and discuss whether there is evidence of multicollinearity.\n\nSolution:\n\n# Fit the TSLM model on us_change data\nmodel_macro_lab &lt;- us_change %&gt;%\n  model(tslm_macro_lab = TSLM(Consumption ~ .))\n\n# Convert to an lm object for VIF computation\nmacro_lm_lab &lt;- lm(Consumption ~ ., data = us_change)\nvif_macro_lab &lt;- car::vif(macro_lm_lab)\nvif_macro_lab\n\n     Quarter       Income   Production      Savings Unemployment \n    1.118319     2.713012     2.709416     2.533910     2.784650 \n\n\n\n\n\nPrompt:\n\nUsing the oxcgrt dataset for the United States, create a new variable log_ConfirmedDeaths as \\(\\\\(\\\\log(1 + \\text{ConfirmedDeaths})\\\\)\\).\n\nFit a TSLM model with ConfirmedCases, StringencyIndex_Average, C6M_Stay at home requirements as predictors for log_ConfirmedDeaths.\n\nCompute the VIF for the predictors and interpret the results in terms of potential collinearity.\n\nSolution:\n\n# Filter and transform the data\noxcgrt_lab &lt;- oxcgrt %&gt;% \n  filter(CountryName == 'United States') %&gt;% \n  mutate(Date = ymd(Date),\n         log_ConfirmedDeaths = log(1 + ConfirmedDeaths)) %&gt;% \n  as_tsibble(index = Date) %&gt;% \n  drop_na(ConfirmedCases, log_ConfirmedDeaths, StringencyIndex_Average)\n\n# Fit the TSLM model\nmodel_covid_lab &lt;- oxcgrt_lab %&gt;%\n  model(tslm_covid_lab = TSLM(log_ConfirmedDeaths ~ ConfirmedCases + StringencyIndex_Average + `C6M_Stay at home requirements`))\n\n# Convert to an lm object for VIF computation\ncovid_lm_lab &lt;- lm(log_ConfirmedDeaths ~ ConfirmedCases + StringencyIndex_Average + `C6M_Stay at home requirements`, data = as_tibble(oxcgrt_lab))\nvif_covid_lab &lt;- car::vif(covid_lm_lab)\nvif_covid_lab\n\n                 ConfirmedCases         StringencyIndex_Average \n                       1.466983                        4.000390 \n`C6M_Stay at home requirements` \n                       3.786874",
    "crumbs": [
      "Week 3",
      "Activity 10"
    ]
  },
  {
    "objectID": "activity10.html#real-world-examples",
    "href": "activity10.html#real-world-examples",
    "title": "Activity10",
    "section": "",
    "text": "Dataset: us_change (available in the fpp3 package)\nModel:\n\\[\n\\text{Consumption}_t = \\beta_0 + \\beta_1 \\text{Income}_t + \\beta_2 \\text{Production}_t + \\varepsilon_t\n\\]\nIn this example, we explore how Income and Production drive Consumption. We then use VIF to diagnose multicollinearity.\n\nlibrary(car)\n\n# Fit the TSLM model on us_change data\nmodel_macro &lt;- us_change %&gt;%\n  model(tslm_macro = TSLM(Consumption ~ Income + Production))\n\n# Display model coefficients\nmodel_macro %&gt;%\n  tidy(tslm_macro) %&gt;%\n  knitr::kable(caption = 'US Macro Model Coefficients')\n\n\nUS Macro Model Coefficients\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntslm_macro\n(Intercept)\n0.5106790\n0.0477604\n10.692528\n0.00e+00\n\n\ntslm_macro\nIncome\n0.1842811\n0.0427041\n4.315301\n2.53e-05\n\n\ntslm_macro\nProduction\n0.1925038\n0.0252739\n7.616697\n0.00e+00\n\n\n\n\n# Compute VIF values using a standard lm object\nmacro_lm &lt;- lm(Consumption ~ Income + Production, data = us_change)\nvif_macro &lt;- car::vif(macro_lm)\nvif_macro\n\n    Income Production \n  1.078113   1.078113 \n\n\n\n\n\nDataset: oxcgrt (Oxford COVID Policy Tracker)\nModel:\n\\[\n\\log(\\text{ConfirmedCases}_t) = \\beta_0 + \\beta_1 \\text{ConfirmedDeaths}_t + \\beta_2 \\text{StringencyIndexAverage}_t + \\varepsilon_t\n\\]\nHere, we investigate the effect of COVID-19 deaths and policy stringency on the growth of confirmed cases, using a log-transformation to account for exponential growth. VIF is computed to check for multicollinearity between the predictors.\n\nlibrary(readr)\nurl &lt;- \"https://github.com/OxCGRT/covid-policy-dataset/raw/main/data/OxCGRT_compact_national_v1.csv\"\noxcgrt &lt;- read_csv(url)\n\n# Filter for the United States and create a log-transformed 'ConfirmedCases' variable\noxcgrt_us &lt;- oxcgrt %&gt;% \n  filter(CountryName == 'United States') %&gt;% \n  mutate(Date = ymd(Date),\n         log_ConfirmedCases = log(1 + ConfirmedCases)) %&gt;% \n  as_tsibble(index = Date) %&gt;% \n  drop_na(log_ConfirmedCases, ConfirmedDeaths, StringencyIndex_Average)\n\n# Fit the TSLM model\nmodel_covid &lt;- oxcgrt_us %&gt;%\n  model(tslm_covid = TSLM(log_ConfirmedCases ~ ConfirmedDeaths + StringencyIndex_Average))\n\n# Display model coefficients\nmodel_covid %&gt;% \n  tidy(tslm_covid) %&gt;%\n  knitr::kable(caption = 'COVID Model Coefficients')\n\n\nCOVID Model Coefficients\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntslm_covid\n(Intercept)\n1.7724524\n0.0956172\n18.53696\n0\n\n\ntslm_covid\nConfirmedDeaths\n0.0000113\n0.0000001\n156.84617\n0\n\n\ntslm_covid\nStringencyIndex_Average\n0.1578871\n0.0013481\n117.12148\n0\n\n\n\n\n# Compute VIF values using a standard lm object (car::vif works with lm)\ncovid_lm &lt;- lm(log_ConfirmedCases ~ ConfirmedDeaths + StringencyIndex_Average, data = as_tibble(oxcgrt_us))\nvif_covid &lt;- car::vif(covid_lm)\nvif_covid\n\n        ConfirmedDeaths StringencyIndex_Average \n               1.270702                1.270702 \n\n\n\n\n\nPrompt:\n\nUsing the us_change dataset, fit a TSLM model with Consumption as the response and all of the remaining variables as predictors.\n\nCompute the VIF for each predictor using the car::vif() function.\n\nInterpret the VIF values and discuss whether there is evidence of multicollinearity.\n\nSolution:\n\n# Fit the TSLM model on us_change data\nmodel_macro_lab &lt;- us_change %&gt;%\n  model(tslm_macro_lab = TSLM(Consumption ~ .))\n\n# Convert to an lm object for VIF computation\nmacro_lm_lab &lt;- lm(Consumption ~ ., data = us_change)\nvif_macro_lab &lt;- car::vif(macro_lm_lab)\nvif_macro_lab\n\n     Quarter       Income   Production      Savings Unemployment \n    1.118319     2.713012     2.709416     2.533910     2.784650 \n\n\n\n\n\nPrompt:\n\nUsing the oxcgrt dataset for the United States, create a new variable log_ConfirmedDeaths as \\(\\\\(\\\\log(1 + \\text{ConfirmedDeaths})\\\\)\\).\n\nFit a TSLM model with ConfirmedCases, StringencyIndex_Average, C6M_Stay at home requirements as predictors for log_ConfirmedDeaths.\n\nCompute the VIF for the predictors and interpret the results in terms of potential collinearity.\n\nSolution:\n\n# Filter and transform the data\noxcgrt_lab &lt;- oxcgrt %&gt;% \n  filter(CountryName == 'United States') %&gt;% \n  mutate(Date = ymd(Date),\n         log_ConfirmedDeaths = log(1 + ConfirmedDeaths)) %&gt;% \n  as_tsibble(index = Date) %&gt;% \n  drop_na(ConfirmedCases, log_ConfirmedDeaths, StringencyIndex_Average)\n\n# Fit the TSLM model\nmodel_covid_lab &lt;- oxcgrt_lab %&gt;%\n  model(tslm_covid_lab = TSLM(log_ConfirmedDeaths ~ ConfirmedCases + StringencyIndex_Average + `C6M_Stay at home requirements`))\n\n# Convert to an lm object for VIF computation\ncovid_lm_lab &lt;- lm(log_ConfirmedDeaths ~ ConfirmedCases + StringencyIndex_Average + `C6M_Stay at home requirements`, data = as_tibble(oxcgrt_lab))\nvif_covid_lab &lt;- car::vif(covid_lm_lab)\nvif_covid_lab\n\n                 ConfirmedCases         StringencyIndex_Average \n                       1.466983                        4.000390 \n`C6M_Stay at home requirements` \n                       3.786874",
    "crumbs": [
      "Week 3",
      "Activity 10"
    ]
  },
  {
    "objectID": "activity11.html",
    "href": "activity11.html",
    "title": "Activity11",
    "section": "",
    "text": "Core Concept\nResidual analysis is crucial for verifying model adequacy in time series. Residuals \\(e_t = y_t - \\hat{y}_t\\) reveal violations of model assumptions (independence, homoscedasticity, normality). We demonstrate this through three domains, using the glance() function for model diagnostics, and discuss error modeling strategies when residuals aren’t white noise.\nResiduals are defined as the differences between the observed values and the corresponding fitted values:\n\\[\n\\begin{align}\ne_t &= y_t - \\hat{y}_t,\n\\end{align}\n\\]\nwhere \\(y_t\\) is the observed value and \\(\\hat{y}_t\\) is the predicted value at time \\(t\\). Examining residuals helps assess model assumptions including independence, homoscedasticity, and normality.",
    "crumbs": [
      "Week 3",
      "Activity 11"
    ]
  },
  {
    "objectID": "activity11.html#residual-analysis-in-time-series-modeling",
    "href": "activity11.html#residual-analysis-in-time-series-modeling",
    "title": "Activity11",
    "section": "",
    "text": "Core Concept\nResidual analysis is crucial for verifying model adequacy in time series. Residuals \\(e_t = y_t - \\hat{y}_t\\) reveal violations of model assumptions (independence, homoscedasticity, normality). We demonstrate this through three domains, using the glance() function for model diagnostics, and discuss error modeling strategies when residuals aren’t white noise.\nResiduals are defined as the differences between the observed values and the corresponding fitted values:\n\\[\n\\begin{align}\ne_t &= y_t - \\hat{y}_t,\n\\end{align}\n\\]\nwhere \\(y_t\\) is the observed value and \\(\\hat{y}_t\\) is the predicted value at time \\(t\\). Examining residuals helps assess model assumptions including independence, homoscedasticity, and normality.",
    "crumbs": [
      "Week 3",
      "Activity 11"
    ]
  },
  {
    "objectID": "activity11.html#finance-example-european-stock-markets",
    "href": "activity11.html#finance-example-european-stock-markets",
    "title": "Activity11",
    "section": "1. Finance Example: European Stock Markets",
    "text": "1. Finance Example: European Stock Markets\nWe utilize the built-in EuStockMarkets dataset to illustrate a Box-Cox transformation and residual analysis on financial data.\n\n# Convert the EuStockMarkets dataset to a tsibble\nstock_tsibble &lt;- as_tsibble(EuStockMarkets)\n\n# Use the DAX index for analysis and determine the optimal Box-Cox lambda\nstock_DAX &lt;- stock_tsibble %&gt;% \n  filter(key == \"DAX\") \n  \n\nlambda &lt;- stock_DAX |&gt;\n  features(value, features = guerrero) |&gt;\n  pull(lambda_guerrero)\n\nstock_DAX_BC &lt;- stock_DAX |&gt;\n  mutate(BoxCoxValue = box_cox(value, lambda))\n\n\n# Fit a simple model with trend\nfit_finance &lt;- stock_DAX_BC %&gt;% \n  model(lm = TSLM(BoxCoxValue ~ trend()))\n\nfit_finance %&gt;% tidy() %&gt;% knitr::kable()\n\n\n\n\nkey\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nDAX\nlm\n(Intercept)\n2.677081\n0.0003964\n6753.6546\n0\n\n\nDAX\nlm\ntrend()\n0.000000\n0.0000000\n114.5636\n0\n\n\n\n\nfit_finance %&gt;% glance() %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkey\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\nDAX\nlm\n0.8759912\n0.8759245\n7.31e-05\n13124.81\n0\n2\n6218.707\n-17709.87\n-17709.85\n-17693.28\n7.32e-05\n0.1358332\n1858\n2\n\n\n\n\n# Residual analysis: Plot fitted vs residuals\nfit_finance %&gt;% augment() %&gt;%\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", col = \"blue\", se = FALSE) +\n  labs(title = \"Residuals of DAX after Box-Cox Transformation\")\n\n\n\n\n\n\n\n\nInsight: If residuals show unexplained structure, consider adding seasonal components or switching to ARIMA.",
    "crumbs": [
      "Week 3",
      "Activity 11"
    ]
  },
  {
    "objectID": "activity11.html#health-example-electricity-demand-in-victoria",
    "href": "activity11.html#health-example-electricity-demand-in-victoria",
    "title": "Activity11",
    "section": "2. Health Example: Electricity Demand in Victoria",
    "text": "2. Health Example: Electricity Demand in Victoria\nUsing the vic_elec dataset from the Tidyverts ecosystem, we fit a log-linear model to study the effect of temperature on electricity demand.\n\n# Fit a log-linear model: log(Demand) as a function of Temperature\nfit_demand &lt;- vic_elec %&gt;% model(TSLM(log(Demand) ~ Temperature))\n\nfit_demand %&gt;%  gg_tsresiduals() +\n  labs(title = \"Model Residuals\")\n\n\n\n\n\n\n\nfit_demand %&gt;% tidy() %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nTSLM(log(Demand) ~ Temperature)\n(Intercept)\n8.3034435\n0.0023852\n3481.30664\n0\n\n\nTSLM(log(Demand) ~ Temperature)\nTemperature\n0.0078263\n0.0001385\n56.50756\n0\n\n\n\n\nfit_demand %&gt;% glance() %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\nTSLM(log(Demand) ~ Temperature)\n0.057225\n0.0572071\n0.0323145\n3193.104\n0\n2\n15635.07\n-180559.2\n-180559.2\n-180532.6\n0.0323162\n1699.938\n52606\n2\n\n\n\n\n# Plot residuals against fitted values\nfit_demand %&gt;% augment() %&gt;%\n  ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", col = \"blue\", se = FALSE) +\n  labs(title = \"Residuals of Log-Linear Model for Electricity Demand\")\n\n\n\n\n\n\n\n\n\n# Fit a refined log-linear model with a quadratic term\nfit_demand_quad &lt;- vic_elec %&gt;% \n  model(TSLM(log(Demand) ~ Temperature + I(Temperature^2)))\n\nfit_demand_quad %&gt;% gg_tsresiduals() +\n  labs(title = \"Model Residuals\")\n\n\n\n\n\n\n\nfit_demand_quad %&gt;% tidy() %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nTSLM(log(Demand) ~ Temperature + I(Temperature^2))\n(Intercept)\n8.6290530\n0.0052048\n1657.91105\n0\n\n\nTSLM(log(Demand) ~ Temperature + I(Temperature^2))\nTemperature\n-0.0312701\n0.0005771\n-54.18703\n0\n\n\nTSLM(log(Demand) ~ Temperature + I(Temperature^2))\nI(Temperature^2)\n0.0010463\n0.0000150\n69.60956\n0\n\n\n\n\nfit_demand_quad %&gt;% glance() %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\nTSLM(log(Demand) ~ Temperature + I(Temperature^2))\n0.1367406\n0.1367077\n0.0295896\n4166.324\n0\n3\n17952.78\n-185192.6\n-185192.6\n-185157.1\n0.0295909\n1556.562\n52605\n3\n\n\n\n\n# Residual analysis: Plot fitted vs residuals\nfit_demand_quad %&gt;% augment() %&gt;%\n  ggplot(aes(.fitted, .resid)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", col = \"blue\", se = FALSE) +\n  labs(title = \"Residuals of Log-Linear Model for Electricity Demand\",\n       x = \"Fitted Values\", y = \"Residuals\")\n\n\n\n\n\n\n\n\nInsight: Even with better metrics, residual autocorrelation persists – this signals the need for SARIMA models.",
    "crumbs": [
      "Week 3",
      "Activity 11"
    ]
  },
  {
    "objectID": "activity11.html#environment-example-air-quality-analysis",
    "href": "activity11.html#environment-example-air-quality-analysis",
    "title": "Activity11",
    "section": "3. Environment Example: Air Quality Analysis",
    "text": "3. Environment Example: Air Quality Analysis\nThe built-in airquality dataset is used to analyze the relationship between temperature and ozone levels, followed by residual diagnostics.\n\n# Prepare the 'airquality' data with a proper date variable\nlibrary(stringr)\nairquality &lt;- as_tibble(airquality) %&gt;% \n  mutate(Date = lubridate::ymd(stringr::str_c(1973, Month, Day, sep = \"-\"))) %&gt;%\n  arrange(Date) %&gt;% \n  as_tsibble(index = Date)\n\n# Fit a linear model: Ozone as a function of Temperature\nfit_env &lt;- airquality %&gt;%  model(TSLM(Ozone ~ Temp))\n\nfit_env %&gt;% tidy() %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nTSLM(Ozone ~ Temp)\n(Intercept)\n-146.995491\n18.2871736\n-8.038174\n0\n\n\nTSLM(Ozone ~ Temp)\nTemp\n2.428703\n0.2331318\n10.417724\n0\n\n\n\n\nfit_env %&gt;% glance() %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.model\nr_squared\nadj_r_squared\nsigma2\nstatistic\np_value\ndf\nlog_lik\nAIC\nAICc\nBIC\nCV\ndeviance\ndf.residual\nrank\n\n\n\n\nTSLM(Ozone ~ Temp)\n0.4877072\n0.4832134\n562.3675\n108.529\n0\n2\n-530.8532\n738.5126\n738.7269\n746.7734\n568.4843\n64109.89\n114\n2\n\n\n\n\n# Compute fitted values and residuals\nfit_env %&gt;% augment() %&gt;% \n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", col = \"blue\", se = FALSE) +\n  labs(title = \"Residuals of Ozone vs Temperature Model\")\n\n\n\n\n\n\n\nfit_env %&gt;%   gg_tsresiduals() +\n  labs(title = \"Model Residuals\")",
    "crumbs": [
      "Week 3",
      "Activity 11"
    ]
  },
  {
    "objectID": "activity11.html#activity-1-retail-sales-seasonality-analysis",
    "href": "activity11.html#activity-1-retail-sales-seasonality-analysis",
    "title": "Activity11",
    "section": "Activity 1: Retail Sales Seasonality Analysis",
    "text": "Activity 1: Retail Sales Seasonality Analysis\nPrompt\nUsing the aus_retail dataset (Tidyverts), analyze the “Takeaway food services” sector in Victoria.\n\nFit a model incorporating both trend and monthly seasonality\n\nPerform residual analysis to assess model adequacy\n\nDiscuss whether seasonality improves the model\n\nSolution\n\n\n\n\n\n\n\n\nKey Insight\nThe season() term automatically creates 11 monthly dummy variables. Compare residuals before/after adding seasonality using glance() metrics like AIC.",
    "crumbs": [
      "Week 3",
      "Activity 11"
    ]
  },
  {
    "objectID": "activity11.html#activity-2-gasoline-production-transformation-study",
    "href": "activity11.html#activity-2-gasoline-production-transformation-study",
    "title": "Activity11",
    "section": "Activity 2: Gasoline Production Transformation Study",
    "text": "Activity 2: Gasoline Production Transformation Study\nPrompt\nUsing the gas dataset (astsa):\n\nApply Box-Cox transformation with Guerrero’s optimal \\(\\lambda\\)\n\nCompare residuals against a square root transformation (\\(\\lambda=0.5\\))\n\nIdentify which transformation better satisfies homoscedasticity\n\nSolution\n\n\n\n\n\n\n\n\nCritical Check\nUse glance(fit_opt) vs glance(fit_sqrt) to compare sigma (residual SD) - lower values indicate better variance stabilization.",
    "crumbs": [
      "Week 3",
      "Activity 11"
    ]
  },
  {
    "objectID": "activity7.html",
    "href": "activity7.html",
    "title": "Activity7",
    "section": "",
    "text": "Time series data often exhibit patterns that evolve over time. Three key components to recognize in many series are trend, seasonality, and noise. Understanding these components is crucial for forecasting and interpretation.\n\nTrend: Refers to the long-term progression of the series — an overall increase, decrease, or stable behavior over a period.\nSeasonality: Captures repeating patterns or cycles, such as higher sales every December or temperature variations across seasons.\nNoise: Represents random fluctuations or irregularities in the data that are not explained by trend or seasonality.\n\nMathematically, a time series \\(y_t\\) can be represented as a combination of these components:\n\nAdditive Model:\n\\[\ny_t = T_t + S_t + R_t\n\\] where:\n\n\\(T_t\\) is the trend component,\n\\(S_t\\) is the seasonal component,\n\\(R_t\\) is the remainder or residual (noise) component.\n\nMultiplicative Model:\n\\[\ny_t = T_t \\times S_t \\times R_t\n\\]\nThis form is useful when seasonal fluctuations change proportionally to the series level.\n\n\n\nTo analyze these components, we can use decomposition methods such as classical decomposition or STL (Seasonal-Trend decomposition using Loess). These methods separate a time series into its trend, seasonal, and remainder components, providing insights into underlying patterns.\nSteps in Decomposition:\n\nFit an STL decomposition model to the series, specifying a seasonal window to capture recurring patterns.\nExtract the trend, seasonal, and remainder components.\nPlot the decomposed components:\n\nThe observed series.\nThe extracted trend component.\nThe seasonal component, revealing periodic patterns.\nThe remainder (residuals) after removing trend and seasonality.\n\n\nThese plots allow us to visually inspect how much of the variation in the data is due to trend, seasonal effects, or noise.\n\n\n\nSmoothing helps to highlight the trend by reducing short-term fluctuations.\n\nMoving Average Smoothing:\n\nEach point is replaced by the average of its neighbors over a specified window.\nThis technique smooths out short-term variations, making long-term trends more visible.\n\nExponential Smoothing:\n\nApplies decreasing weights to older observations, making it responsive to recent changes.\n\n\n\n\n\n\n# Step 1: Retrieve stock data using tq_get()\nstock_data &lt;- tq_get(\"AAPL\", get = \"stock.prices\", from = \"2018-01-01\", to = \"2023-01-01\") %&gt;%\n  select(date, adjusted) %&gt;%\n  rename(Adjusted_Close = adjusted)\n\n# Convert to a tsibble object\nstock_ts &lt;- as_tsibble(stock_data, index = date) %&gt;% \n  fill_gaps() %&gt;% \n  mutate(Adjusted_Close = na.approx(Adjusted_Close))\n  \n\n# Step 2: STL Decomposition\ndecomp &lt;- stock_ts %&gt;%\n  model(STL(Adjusted_Close ~ trend(window = 365) + season(window = \"periodic\"))) %&gt;%\n  components()\n\n# Plot the decomposed components\nautoplot(decomp) +\n  labs(title = \"STL Decomposition of Apple Inc. Adjusted Closing Price\",\n       subtitle = \"Trend, Seasonal, and Remainder Components\")\n\n\n\n\n\n\n\n# Step 3: Smoothing Techniques\n# Moving Average Smoothing (30-day window)\nstock_ts &lt;- stock_ts %&gt;%\n  mutate(Moving_Avg = slider::slide_dbl(Adjusted_Close, mean, .before = 29, .after = 0))\n\n# Fit Holt's method using fable\nfit_holt &lt;- stock_ts %&gt;%\n  model(Holt = ETS(Adjusted_Close ~ error(\"A\") + trend(\"A\") + season(\"N\")))\n\n# Extract fitted values\nholt_fc &lt;- fit_holt %&gt;% \n  augment() %&gt;% \n  select(date, .fitted)\n\n# Plot the original series, moving average\nstock_ts %&gt;%\n  autoplot(Adjusted_Close, color = \"black\") +\n  autolayer(stock_ts, Moving_Avg, color = \"blue\") +\n  autolayer(holt_fc, .fitted, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Smoothing Techniques Applied to Apple Inc. Adjusted Closing Price\",\n       subtitle = \"Original Series (Black), 30-day MA (Blue), Holt's ES (Red)\",\n       y = \"Adjusted Close Price (USD)\") +\n  theme_tq()\n\n\n\n\n\n\n\n\n\n\n\nApply the concepts of time series decomposition and smoothing using the Tidyverts ecosystem in R. Use the stock data for Microsoft Corporation (MSFT) obtained via the tidyquant package. Perform the following tasks:\n\nDecompose the time series for Microsoft’s adjusted closing price using STL decomposition. Visualize the trend, seasonal, and remainder components.\nApply smoothing techniques (e.g., moving average or exponential smoothing) to the same series and compare the results.\nInterpret the components and smoothed series to identify trends, seasonality, and noise.\n\n\n# Step 1: Retrieve Microsoft stock data using tq_get()\nmsft_data &lt;- tq_get(\"MSFT\", get = \"stock.prices\", from = \"2018-01-01\", to = \"2023-01-01\") %&gt;%\n  select(date, adjusted) %&gt;%\n  rename(Adjusted_Close = adjusted)\n\n# Convert to a tsibble object\nmsft_ts &lt;- as_tsibble(msft_data, index = date) %&gt;% \n  fill_gaps() %&gt;% \n  mutate(Adjusted_Close = na.approx(Adjusted_Close))\n\n# Step 2: STL Decomposition\ndecomp &lt;- msft_ts %&gt;%\n  model(STL(Adjusted_Close ~ trend(window = 365) + season(window = \"periodic\"))) %&gt;%\n  components()\n\n# Plot the decomposed components\nautoplot(decomp) +\n  labs(title = \"STL Decomposition of Microsoft Corporation Adjusted Closing Price\",\n       subtitle = \"Trend, Seasonal, and Remainder Components\",\n       y = \"Adjusted Close Price (USD)\")\n\n\n\n\n\n\n\n# Step 3: Smoothing Techniques\n# Moving Average Smoothing (30-day window)\nmsft_ts &lt;- msft_ts %&gt;%\n  mutate(Moving_Avg = slider::slide_dbl(Adjusted_Close, mean, .before = 29, .after = 0))\n\n# Fit Holt's method using fable\nfit_holt &lt;- msft_ts %&gt;%\n  model(Holt = ETS(Adjusted_Close ~ error(\"A\") + trend(\"A\") + season(\"N\")))\n\n# Extract fitted values\nholt_fc &lt;- fit_holt %&gt;% \n  augment() %&gt;% \n  select(date, .fitted)\n\n# Plot with Holt smoothing\nmsft_ts %&gt;%\n  autoplot(Adjusted_Close, color = \"black\") +\n  autolayer(msft_ts, Moving_Avg, color = \"blue\") +\n  autolayer(holt_fc, .fitted, color = \"red\") +\n  labs(title = \"Smoothing Techniques Applied to Microsoft Corporation Adjusted Closing Price\",\n       subtitle = \"Original Series (Black), 30-day MA (Blue), Holt's ES (Red)\",\n       y = \"Adjusted Close Price (USD)\")",
    "crumbs": [
      "Week 2",
      "Activity 7"
    ]
  },
  {
    "objectID": "activity7.html#understanding-components-decomposition-and-smoothing",
    "href": "activity7.html#understanding-components-decomposition-and-smoothing",
    "title": "Activity7",
    "section": "",
    "text": "Time series data often exhibit patterns that evolve over time. Three key components to recognize in many series are trend, seasonality, and noise. Understanding these components is crucial for forecasting and interpretation.\n\nTrend: Refers to the long-term progression of the series — an overall increase, decrease, or stable behavior over a period.\nSeasonality: Captures repeating patterns or cycles, such as higher sales every December or temperature variations across seasons.\nNoise: Represents random fluctuations or irregularities in the data that are not explained by trend or seasonality.\n\nMathematically, a time series \\(y_t\\) can be represented as a combination of these components:\n\nAdditive Model:\n\\[\ny_t = T_t + S_t + R_t\n\\] where:\n\n\\(T_t\\) is the trend component,\n\\(S_t\\) is the seasonal component,\n\\(R_t\\) is the remainder or residual (noise) component.\n\nMultiplicative Model:\n\\[\ny_t = T_t \\times S_t \\times R_t\n\\]\nThis form is useful when seasonal fluctuations change proportionally to the series level.\n\n\n\nTo analyze these components, we can use decomposition methods such as classical decomposition or STL (Seasonal-Trend decomposition using Loess). These methods separate a time series into its trend, seasonal, and remainder components, providing insights into underlying patterns.\nSteps in Decomposition:\n\nFit an STL decomposition model to the series, specifying a seasonal window to capture recurring patterns.\nExtract the trend, seasonal, and remainder components.\nPlot the decomposed components:\n\nThe observed series.\nThe extracted trend component.\nThe seasonal component, revealing periodic patterns.\nThe remainder (residuals) after removing trend and seasonality.\n\n\nThese plots allow us to visually inspect how much of the variation in the data is due to trend, seasonal effects, or noise.\n\n\n\nSmoothing helps to highlight the trend by reducing short-term fluctuations.\n\nMoving Average Smoothing:\n\nEach point is replaced by the average of its neighbors over a specified window.\nThis technique smooths out short-term variations, making long-term trends more visible.\n\nExponential Smoothing:\n\nApplies decreasing weights to older observations, making it responsive to recent changes.\n\n\n\n\n\n\n# Step 1: Retrieve stock data using tq_get()\nstock_data &lt;- tq_get(\"AAPL\", get = \"stock.prices\", from = \"2018-01-01\", to = \"2023-01-01\") %&gt;%\n  select(date, adjusted) %&gt;%\n  rename(Adjusted_Close = adjusted)\n\n# Convert to a tsibble object\nstock_ts &lt;- as_tsibble(stock_data, index = date) %&gt;% \n  fill_gaps() %&gt;% \n  mutate(Adjusted_Close = na.approx(Adjusted_Close))\n  \n\n# Step 2: STL Decomposition\ndecomp &lt;- stock_ts %&gt;%\n  model(STL(Adjusted_Close ~ trend(window = 365) + season(window = \"periodic\"))) %&gt;%\n  components()\n\n# Plot the decomposed components\nautoplot(decomp) +\n  labs(title = \"STL Decomposition of Apple Inc. Adjusted Closing Price\",\n       subtitle = \"Trend, Seasonal, and Remainder Components\")\n\n\n\n\n\n\n\n# Step 3: Smoothing Techniques\n# Moving Average Smoothing (30-day window)\nstock_ts &lt;- stock_ts %&gt;%\n  mutate(Moving_Avg = slider::slide_dbl(Adjusted_Close, mean, .before = 29, .after = 0))\n\n# Fit Holt's method using fable\nfit_holt &lt;- stock_ts %&gt;%\n  model(Holt = ETS(Adjusted_Close ~ error(\"A\") + trend(\"A\") + season(\"N\")))\n\n# Extract fitted values\nholt_fc &lt;- fit_holt %&gt;% \n  augment() %&gt;% \n  select(date, .fitted)\n\n# Plot the original series, moving average\nstock_ts %&gt;%\n  autoplot(Adjusted_Close, color = \"black\") +\n  autolayer(stock_ts, Moving_Avg, color = \"blue\") +\n  autolayer(holt_fc, .fitted, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Smoothing Techniques Applied to Apple Inc. Adjusted Closing Price\",\n       subtitle = \"Original Series (Black), 30-day MA (Blue), Holt's ES (Red)\",\n       y = \"Adjusted Close Price (USD)\") +\n  theme_tq()\n\n\n\n\n\n\n\n\n\n\n\nApply the concepts of time series decomposition and smoothing using the Tidyverts ecosystem in R. Use the stock data for Microsoft Corporation (MSFT) obtained via the tidyquant package. Perform the following tasks:\n\nDecompose the time series for Microsoft’s adjusted closing price using STL decomposition. Visualize the trend, seasonal, and remainder components.\nApply smoothing techniques (e.g., moving average or exponential smoothing) to the same series and compare the results.\nInterpret the components and smoothed series to identify trends, seasonality, and noise.\n\n\n# Step 1: Retrieve Microsoft stock data using tq_get()\nmsft_data &lt;- tq_get(\"MSFT\", get = \"stock.prices\", from = \"2018-01-01\", to = \"2023-01-01\") %&gt;%\n  select(date, adjusted) %&gt;%\n  rename(Adjusted_Close = adjusted)\n\n# Convert to a tsibble object\nmsft_ts &lt;- as_tsibble(msft_data, index = date) %&gt;% \n  fill_gaps() %&gt;% \n  mutate(Adjusted_Close = na.approx(Adjusted_Close))\n\n# Step 2: STL Decomposition\ndecomp &lt;- msft_ts %&gt;%\n  model(STL(Adjusted_Close ~ trend(window = 365) + season(window = \"periodic\"))) %&gt;%\n  components()\n\n# Plot the decomposed components\nautoplot(decomp) +\n  labs(title = \"STL Decomposition of Microsoft Corporation Adjusted Closing Price\",\n       subtitle = \"Trend, Seasonal, and Remainder Components\",\n       y = \"Adjusted Close Price (USD)\")\n\n\n\n\n\n\n\n# Step 3: Smoothing Techniques\n# Moving Average Smoothing (30-day window)\nmsft_ts &lt;- msft_ts %&gt;%\n  mutate(Moving_Avg = slider::slide_dbl(Adjusted_Close, mean, .before = 29, .after = 0))\n\n# Fit Holt's method using fable\nfit_holt &lt;- msft_ts %&gt;%\n  model(Holt = ETS(Adjusted_Close ~ error(\"A\") + trend(\"A\") + season(\"N\")))\n\n# Extract fitted values\nholt_fc &lt;- fit_holt %&gt;% \n  augment() %&gt;% \n  select(date, .fitted)\n\n# Plot with Holt smoothing\nmsft_ts %&gt;%\n  autoplot(Adjusted_Close, color = \"black\") +\n  autolayer(msft_ts, Moving_Avg, color = \"blue\") +\n  autolayer(holt_fc, .fitted, color = \"red\") +\n  labs(title = \"Smoothing Techniques Applied to Microsoft Corporation Adjusted Closing Price\",\n       subtitle = \"Original Series (Black), 30-day MA (Blue), Holt's ES (Red)\",\n       y = \"Adjusted Close Price (USD)\")",
    "crumbs": [
      "Week 2",
      "Activity 7"
    ]
  },
  {
    "objectID": "activity12.html",
    "href": "activity12.html",
    "title": "Activity12",
    "section": "",
    "text": "Today, we will walk through a complete model selection exercise using VIX data. Our goals are to:\n\nPerform Exploratory Data Analysis (EDA) with visual diagnostics.\nFit built-in models (ARIMA and ETS) using the Tidyverts framework.\nEvaluate models based on five metrics: AIC, AICc, BIC, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\nConclude on the optimal model by comparing these metrics along with residual diagnostics and forecast performance.\n\nFor a model with \\(n\\) observations and \\(k\\) parameters (including the intercept), the evaluation criteria are defined as:\n\\[\n\\begin{align}\n\\text{AIC} &= n\\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 2k,\\\\[1mm]\n\\text{BIC} &= n\\ln\\left(\\frac{\\text{RSS}}{n}\\right) + k\\ln(n),\\\\[1mm]\n\\text{AICc} &= \\text{AIC} + \\frac{2k(k+1)}{n-k-1},\n\\end{align}\n\\]\nand the error metrics:\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{RSS}}{n},\\\\[1mm]\n\\text{RMSE} &= \\sqrt{\\text{MSE}}.\n\\end{align}\n\\]",
    "crumbs": [
      "Week 3",
      "Activity 12"
    ]
  },
  {
    "objectID": "activity12.html#model-selection-diagnostic-criteria",
    "href": "activity12.html#model-selection-diagnostic-criteria",
    "title": "Activity12",
    "section": "",
    "text": "Today, we will walk through a complete model selection exercise using VIX data. Our goals are to:\n\nPerform Exploratory Data Analysis (EDA) with visual diagnostics.\nFit built-in models (ARIMA and ETS) using the Tidyverts framework.\nEvaluate models based on five metrics: AIC, AICc, BIC, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\nConclude on the optimal model by comparing these metrics along with residual diagnostics and forecast performance.\n\nFor a model with \\(n\\) observations and \\(k\\) parameters (including the intercept), the evaluation criteria are defined as:\n\\[\n\\begin{align}\n\\text{AIC} &= n\\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 2k,\\\\[1mm]\n\\text{BIC} &= n\\ln\\left(\\frac{\\text{RSS}}{n}\\right) + k\\ln(n),\\\\[1mm]\n\\text{AICc} &= \\text{AIC} + \\frac{2k(k+1)}{n-k-1},\n\\end{align}\n\\]\nand the error metrics:\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{RSS}}{n},\\\\[1mm]\n\\text{RMSE} &= \\sqrt{\\text{MSE}}.\n\\end{align}\n\\]",
    "crumbs": [
      "Week 3",
      "Activity 12"
    ]
  },
  {
    "objectID": "activity12.html#time-series-plot",
    "href": "activity12.html#time-series-plot",
    "title": "Activity12",
    "section": "Time Series Plot",
    "text": "Time Series Plot\n\nvix_data %&gt;% \n  autoplot(adjusted) +\n  labs(title = \"VIX Adjusted Prices\", y = \"Adjusted\", x = \"Date\")",
    "crumbs": [
      "Week 3",
      "Activity 12"
    ]
  },
  {
    "objectID": "activity12.html#acf-and-pacf-analysis",
    "href": "activity12.html#acf-and-pacf-analysis",
    "title": "Activity12",
    "section": "ACF and PACF Analysis",
    "text": "ACF and PACF Analysis\n\nvix_data %&gt;% \n  ACF(adjusted) %&gt;% \n  autoplot() +\n  labs(title = \"ACF of VIX Data\")\n\n\n\n\n\n\n\nvix_data %&gt;% \n  PACF(adjusted) %&gt;% \n  autoplot() +\n  labs(title = \"PACF of VIX Data\")\n\n\n\n\n\n\n\n\n\nObservation:\nThe persistence in ACF suggests potential need for differencing, but we’ll let ARIMA automatically handle any required transformations through its \\(d\\) parameter.",
    "crumbs": [
      "Week 3",
      "Activity 12"
    ]
  },
  {
    "objectID": "activity12.html#arima-model-original-series",
    "href": "activity12.html#arima-model-original-series",
    "title": "Activity12",
    "section": "1. ARIMA Model (Original Series)",
    "text": "1. ARIMA Model (Original Series)\n\n# Fit an ARIMA model on original series\nmodel_arima &lt;- vix_data %&gt;% \n  model(ARIMA(adjusted))\n\n# Display summary report\nreport(model_arima)\n\nSeries: adjusted \nModel: ARIMA(3,1,1)(0,0,2)[7] \n\nCoefficients:\n         ar1     ar2      ar3      ma1    sma1    sma2\n      0.9615  0.0082  -0.0179  -0.9811  0.0286  0.0553\ns.e.  0.0168  0.0145   0.0122   0.0133  0.0115  0.0107\n\nsigma^2 estimated as 1.728:  log likelihood=-15446.52\nAIC=30907.05   AICc=30907.06   BIC=30956.88\n\n# Display accuracy metrics\nmodel_arima %&gt;% accuracy()\n\n# A tibble: 1 × 10\n  .model          .type         ME  RMSE   MAE    MPE  MAPE  MASE RMSSE     ACF1\n  &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ARIMA(adjusted) Traini… -0.00181  1.31 0.731 -0.220  3.42 0.356 0.403 -2.97e-4",
    "crumbs": [
      "Week 3",
      "Activity 12"
    ]
  },
  {
    "objectID": "activity12.html#ets-model",
    "href": "activity12.html#ets-model",
    "title": "Activity12",
    "section": "2. ETS Model",
    "text": "2. ETS Model\n\n# Fit ETS model on original series\nmodel_ets &lt;- vix_data %&gt;% \n  model(ETS(adjusted))\n\n# Display summary report\nreport(model_ets)\n\nSeries: adjusted \nModel: ETS(M,Ad,M) \n  Smoothing parameters:\n    alpha = 0.9998863 \n    beta  = 0.0449181 \n    gamma = 0.0001019892 \n    phi   = 0.8178318 \n\n  Initial states:\n     l[0]       b[0]    s[0]     s[-1]     s[-2]    s[-3]     s[-4]     s[-5]\n 25.49738 -0.7829361 1.00114 0.9990668 0.9956332 1.002539 0.9994978 0.9996608\n    s[-6]\n 1.002462\n\n  sigma^2:  0.003\n\n     AIC     AICc      BIC \n83569.41 83569.45 83661.96 \n\n# Display accuracy metrics\nmodel_ets %&gt;% accuracy()\n\n# A tibble: 1 × 10\n  .model        .type           ME  RMSE   MAE    MPE  MAPE  MASE RMSSE    ACF1\n  &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 ETS(adjusted) Training -0.000342  1.32 0.732 -0.120  3.43 0.356 0.405 -0.0392",
    "crumbs": [
      "Week 3",
      "Activity 12"
    ]
  },
  {
    "objectID": "activity12.html#lab-activity-modeling-stock-returns-with-arima",
    "href": "activity12.html#lab-activity-modeling-stock-returns-with-arima",
    "title": "Activity12",
    "section": "Lab Activity: Modeling Stock Returns with ARIMA",
    "text": "Lab Activity: Modeling Stock Returns with ARIMA\nModel daily SPY returns using ARIMA, analyze residuals, and compare with ETS.\nTasks\n\nRetrieve SPY data (2015-01-01 to present) and compute daily log returns\n\nPerform EDA (time series plot, ACF/PACF)\n\nFit ARIMA(2,0,1) and ETS models to returns\n\nCompare models using AIC and RMSE\n\nForecast 1-month ahead returns\n\nSolution\n\n# Task 1: Data Preparation\nspy_data &lt;- tq_get(\"SPY\", from = \"2015-01-01\") %&gt;% \n  as_tsibble(index = date) %&gt;% \n  tsibble::fill_gaps() %&gt;% \n  mutate(adjusted = na.approx(adjusted),\n         log_return = difference(log(adjusted))) %&gt;% \n  select(date, adjusted, log_return) %&gt;% \n  tidyr::drop_na()\n\n\n# Task 2: EDA\nspy_data %&gt;% autoplot(log_return) +\n  labs(title = \"SPY Daily Log Returns\", y = \"Return\")\n\n\n\n\n\n\n\nspy_data %&gt;% ACF(log_return) %&gt;% autoplot()\n\n\n\n\n\n\n\nspy_data %&gt;% PACF(log_return) %&gt;% autoplot()\n\n\n\n\n\n\n\n# Task 3: Model Fitting\nfit_arima &lt;- spy_data %&gt;% \n  model(ARIMA(log_return ~ pdq(2,0,1)))\n\nfit_ets &lt;- spy_data %&gt;% \n  model(ETS(log_return))\n\n# Task 4: Model Comparison\nglance(fit_arima) %&gt;% select(AIC, AICc, BIC)\n\n# A tibble: 1 × 3\n      AIC    AICc     BIC\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 -25033. -25033. -24983.\n\nglance(fit_ets) %&gt;% select(AIC, AICc, BIC)\n\n# A tibble: 1 × 3\n     AIC   AICc    BIC\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 -4947. -4947. -4928.\n\naccuracy(fit_arima)\n\n# A tibble: 1 × 10\n  .model         .type       ME    RMSE     MAE   MPE  MAPE  MASE RMSSE     ACF1\n  &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ARIMA(log_ret… Trai… -1.52e-6 0.00838 0.00504   NaN   Inf 0.694 0.710 -4.44e-4\n\naccuracy(fit_ets)\n\n# A tibble: 1 × 10\n  .model          .type       ME    RMSE     MAE   MPE  MAPE  MASE RMSSE    ACF1\n  &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 ETS(log_return) Train… 3.35e-6 0.00843 0.00503  -Inf   Inf 0.692 0.714 -0.0176\n\n# Task 5: Forecasting\nfit_arima %&gt;% \n  forecast(h = 30) %&gt;% \n  autoplot(spy_data) +\n  labs(title = \"30-Day SPY Return Forecast\", y = \"Log Return\")",
    "crumbs": [
      "Week 3",
      "Activity 12"
    ]
  },
  {
    "objectID": "activity2.html",
    "href": "activity2.html",
    "title": "Activity2",
    "section": "",
    "text": "In time series analysis, understanding relationships between different variables is crucial. Three fundamental statistical concepts that help capture these relationships are covariance, correlation, and partial correlation.\n\n\nCovariance measures how two variables move together. For two time series \\(X_t\\) and \\(Y_t\\), the sample covariance over \\(n\\) periods is given by:\n\\[\n\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{t=1}^{n} (X_t - \\bar{X})(Y_t - \\bar{Y})\n\\]\nwhere \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means. A positive covariance indicates that when \\(X_t\\) is above its mean, \\(Y_t\\) tends to be above its mean as well, and vice versa.\n\n\n\nCorrelation standardizes covariance, providing a dimensionless measure between -1 and 1:\n\\[\n\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nwhere \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X_t\\) and \\(Y_t\\). A correlation of 1 means perfect positive linear relationship, -1 means perfect negative linear relationship, and 0 suggests no linear relationship.\n\n\n\nPartial correlation measures the relationship between two variables while controlling for the effect of one or more additional variables. For three variables \\(X\\), \\(Y\\), and \\(Z\\), the partial correlation between \\(X\\) and \\(Y\\) controlling for \\(Z\\) is defined as:\n\\[\n\\rho_{XY \\cdot Z} = \\frac{\\rho_{XY} - \\rho_{XZ}\\rho_{YZ}}{\\sqrt{(1-\\rho_{XZ}^2)(1-\\rho_{YZ}^2)}}\n\\]\nThis formula removes the influence of \\(Z\\) on both \\(X\\) and \\(Y\\), providing a clearer picture of their direct relationship.",
    "crumbs": [
      "Week 1",
      "Activity 2"
    ]
  },
  {
    "objectID": "activity2.html#correlation-covariance-and-partial-correlation-in-time-series",
    "href": "activity2.html#correlation-covariance-and-partial-correlation-in-time-series",
    "title": "Activity2",
    "section": "",
    "text": "In time series analysis, understanding relationships between different variables is crucial. Three fundamental statistical concepts that help capture these relationships are covariance, correlation, and partial correlation.\n\n\nCovariance measures how two variables move together. For two time series \\(X_t\\) and \\(Y_t\\), the sample covariance over \\(n\\) periods is given by:\n\\[\n\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{t=1}^{n} (X_t - \\bar{X})(Y_t - \\bar{Y})\n\\]\nwhere \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means. A positive covariance indicates that when \\(X_t\\) is above its mean, \\(Y_t\\) tends to be above its mean as well, and vice versa.\n\n\n\nCorrelation standardizes covariance, providing a dimensionless measure between -1 and 1:\n\\[\n\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nwhere \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X_t\\) and \\(Y_t\\). A correlation of 1 means perfect positive linear relationship, -1 means perfect negative linear relationship, and 0 suggests no linear relationship.\n\n\n\nPartial correlation measures the relationship between two variables while controlling for the effect of one or more additional variables. For three variables \\(X\\), \\(Y\\), and \\(Z\\), the partial correlation between \\(X\\) and \\(Y\\) controlling for \\(Z\\) is defined as:\n\\[\n\\rho_{XY \\cdot Z} = \\frac{\\rho_{XY} - \\rho_{XZ}\\rho_{YZ}}{\\sqrt{(1-\\rho_{XZ}^2)(1-\\rho_{YZ}^2)}}\n\\]\nThis formula removes the influence of \\(Z\\) on both \\(X\\) and \\(Y\\), providing a clearer picture of their direct relationship.",
    "crumbs": [
      "Week 1",
      "Activity 2"
    ]
  },
  {
    "objectID": "activity2.html#applied-example-using-fpp3-data",
    "href": "activity2.html#applied-example-using-fpp3-data",
    "title": "Activity2",
    "section": "Applied Example Using fpp3 Data",
    "text": "Applied Example Using fpp3 Data\nConsider the global_economy dataset from the fpp3 package, which contains economic indicators for various countries over time. We can explore relationships such as:\n\nCovariance and correlation between Growth and CPI for different countries.\nPartial correlation to understand the relationship between Imports and Exports while controlling for GDP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nCode\nYear\nGDP\nGrowth\nCPI\nImports\nExports\nPopulation\n\n\n\n\nAfghanistan\nAFG\n1988\nNA\nNA\nNA\nNA\nNA\n11540888\n\n\nPalau\nPLW\n1979\nNA\nNA\nNA\nNA\nNA\n12124\n\n\nGreece\nGRC\n1981\n52346507380\n-1.553721\n6.517618\n25.75087\n21.38946\n9729350\n\n\nAlbania\nALB\n1960\nNA\nNA\nNA\nNA\nNA\n1608800\n\n\nHong Kong SAR, China\nHKG\n1964\n2206466461\n8.627709\nNA\n80.58546\n72.92227\n3504600\n\n\nNew Caledonia\nNCL\n1969\n263108835\n15.716313\nNA\nNA\nNA\n104000\n\n\nUganda\nUGA\n1972\n1491596639\nNA\nNA\n16.77934\n19.38967\n9988380\n\n\nNiger\nNER\n2003\n2731416346\n5.300000\n81.907782\n24.78620\n15.19787\n12656870\n\n\nIceland\nISL\n1975\n1386032921\nNA\nNA\n41.62935\n33.28791\n217979\n\n\nLithuania\nLTU\n1981\nNA\nNA\nNA\nNA\nNA\n3432947\n\n\n\n\n\n\n# Filter data for Australia and select relevant variables\naus_data &lt;- global_economy %&gt;%\n1  filter(Country == \"Australia\") %&gt;%\n2  dplyr::select(Year, GDP, Growth, Population, CPI, Imports, Exports)\n\n\n1\n\nFilter the global_economy dataset to include only rows where the country is Australia.\n\n2\n\nSelect the variables of interest: Year, Growth, Population, and CPI\n\n\n\n\n\n# Calculate covariance between GDP and Population for Australia\n1cov_gdp_pop &lt;- cov(aus_data$Growth, aus_data$CPI, use = \"complete.obs\")\ncov_gdp_pop  \n\n\n1\n\nCompute the covariance between Growth and CPI for the Australian subset, using complete observations.\n\n\n\n\n[1] -17.36472\n\n\n\n# Calculate correlation between GDP and Population for Australia\n1cor_gdp_pop &lt;- cor(aus_data$Growth, aus_data$CPI, use = \"complete.obs\")\ncor_gdp_pop  \n\n\n1\n\nCompute the Pearson correlation coefficient between GDP and Population.\n\n\n\n\n[1] -0.2779003\n\n\n\n# Compute correlation matrix for GDP, Population, and Life Expectancy\ncor_matrix &lt;- aus_data %&gt;%\n1  dplyr::select(GDP, Growth, Population, CPI, Imports, Exports) %&gt;%\n2  cor(use = \"complete.obs\")\ncor_matrix\n\n\n1\n\nSelect columns for GDP, Growth, Population, CPI, Imports, and Exports from the Australian data.\n\n2\n\nCalculate the correlation matrix for these variables using complete observations.\n\n\n\n\n                  GDP     Growth Population        CPI    Imports    Exports\nGDP         1.0000000 -0.2494090  0.9073355  0.9003771  0.7831384  0.8128970\nGrowth     -0.2494090  1.0000000 -0.2950084 -0.2779003 -0.1753012 -0.2251961\nPopulation  0.9073355 -0.2950084  1.0000000  0.9899235  0.9054921  0.9114850\nCPI         0.9003771 -0.2779003  0.9899235  1.0000000  0.9313180  0.9275880\nImports     0.7831384 -0.1753012  0.9054921  0.9313180  1.0000000  0.9238266\nExports     0.8128970 -0.2251961  0.9114850  0.9275880  0.9238266  1.0000000\nYear        0.8801466 -0.2863494  0.9966214  0.9906993  0.9224200  0.9195759\n                 Year\nGDP         0.8801466\nGrowth     -0.2863494\nPopulation  0.9966214\nCPI         0.9906993\nImports     0.9224200\nExports     0.9195759\nYear        1.0000000\n\n\n\nPartial correlation to understand the relationship between Imports and Exports while controlling for GDP.\n\n\n# Partial correlation between Imports and Exports controlling for GDP manually\n# Extract relevant correlation coefficients from the matrix\n1rho_Imports_Exports &lt;- cor_matrix[\"Imports\", \"Exports\"]\n2rho_Imports_GDP &lt;- cor_matrix[\"Imports\", \"GDP\"]\n3rho_Exports_GDP &lt;- cor_matrix[\"Exports\", \"GDP\"]\n\n\n1\n\nExtracts the direct correlation between Imports and Exports.\n\n2\n\nExtracts the correlation between Imports and GDP.\n\n3\n\nExtracts the correlation between Exports and GDP.\n\n\n\n\n\n# Compute partial correlation between Imports and Exports controlling for GDP\npcorr_Imp_Exp_GDP &lt;- (rho_Imports_Exports - rho_Imports_Exports * rho_Exports_GDP) /\n  sqrt((1 - rho_Imports_GDP^2) * (1 - rho_Exports_GDP^2))\npcorr_Imp_Exp_GDP  # Display partial correlation result\n\n[1] 0.4772659",
    "crumbs": [
      "Week 1",
      "Activity 2"
    ]
  },
  {
    "objectID": "activity2.html#lab-activity-recreate-the-above-analysis-using-the-class-activity-template-and-answer-the-following-question",
    "href": "activity2.html#lab-activity-recreate-the-above-analysis-using-the-class-activity-template-and-answer-the-following-question",
    "title": "Activity2",
    "section": "Lab Activity: Recreate the above analysis using the class activity template and answer the following question:",
    "text": "Lab Activity: Recreate the above analysis using the class activity template and answer the following question:\n\nApply the partial correlation formula to compute the relationship between GDP and Exports, controlling for Population.",
    "crumbs": [
      "Week 1",
      "Activity 2"
    ]
  },
  {
    "objectID": "activity17.html",
    "href": "activity17.html",
    "title": "Activity17",
    "section": "",
    "text": "Exploratory Data Analysis and Preprocessing\n\n\nVisualization: Plot the raw time‐series data to inspect for trends, seasonality, and irregular fluctuations (including rolling means/variances).\nTransformations: Apply appropriate transformations (e.g. logarithmic or Box–Cox) to stabilize variance.\n\n\n\n\n\n\n\n\n\n\nDecomposition & Stationarity Testing\n\n\nDecomposition: Use methods like STL to break the series into trend, seasonal, and remainder components.\nDetrending/Deseasonalizing: Remove or model the trend and seasonal components so that the residual approximates white noise. Stationarity Tests: Apply tests (e.g., ADF, KPSS) and difference the series as needed to achieve stationarity.\n\n\n\n\n\n\n\n\n\n\nModel Identification & Fitting\n\n\nLag Selection: Determine the optimal lag length (using criteria such as AIC or BIC) for an autoregressive or VAR model.\nDynamic Modeling: Fit an autoregressive model (or a VAR for multivariate data) on the stationary series. Granger Causality Testing: Within the VAR framework, test whether past values of one variable significantly improve the prediction of another.\n\n\n\n\n\n\n\n\n\n\nImpulse Response Analysis\n\n\nIRF Computation: After estimating the VAR, compute impulse response functions to trace how shocks to one variable affect the system over time.\nInterpretation: Use IRFs to quantify the duration and magnitude of shock effects, complementing the Granger causality findings.\n\n\n\n\n\n\n\n\n\n\nDiagnostics & Forecasting\n\n\nResidual Analysis: Check that the model’s residuals resemble white noise (e.g., via Ljung–Box tests) to validate the model fit. Forecasting: Employ the fitted model to forecast future values, reintroducing trend and seasonal components as needed.",
    "crumbs": [
      "Week 5",
      "Activity 17"
    ]
  },
  {
    "objectID": "activity17.html#review-of-concepts-and-applications-covered-so-far",
    "href": "activity17.html#review-of-concepts-and-applications-covered-so-far",
    "title": "Activity17",
    "section": "",
    "text": "Exploratory Data Analysis and Preprocessing\n\n\nVisualization: Plot the raw time‐series data to inspect for trends, seasonality, and irregular fluctuations (including rolling means/variances).\nTransformations: Apply appropriate transformations (e.g. logarithmic or Box–Cox) to stabilize variance.\n\n\n\n\n\n\n\n\n\n\nDecomposition & Stationarity Testing\n\n\nDecomposition: Use methods like STL to break the series into trend, seasonal, and remainder components.\nDetrending/Deseasonalizing: Remove or model the trend and seasonal components so that the residual approximates white noise. Stationarity Tests: Apply tests (e.g., ADF, KPSS) and difference the series as needed to achieve stationarity.\n\n\n\n\n\n\n\n\n\n\nModel Identification & Fitting\n\n\nLag Selection: Determine the optimal lag length (using criteria such as AIC or BIC) for an autoregressive or VAR model.\nDynamic Modeling: Fit an autoregressive model (or a VAR for multivariate data) on the stationary series. Granger Causality Testing: Within the VAR framework, test whether past values of one variable significantly improve the prediction of another.\n\n\n\n\n\n\n\n\n\n\nImpulse Response Analysis\n\n\nIRF Computation: After estimating the VAR, compute impulse response functions to trace how shocks to one variable affect the system over time.\nInterpretation: Use IRFs to quantify the duration and magnitude of shock effects, complementing the Granger causality findings.\n\n\n\n\n\n\n\n\n\n\nDiagnostics & Forecasting\n\n\nResidual Analysis: Check that the model’s residuals resemble white noise (e.g., via Ljung–Box tests) to validate the model fit. Forecasting: Employ the fitted model to forecast future values, reintroducing trend and seasonal components as needed.",
    "crumbs": [
      "Week 5",
      "Activity 17"
    ]
  },
  {
    "objectID": "activity1.html",
    "href": "activity1.html",
    "title": "Activity1",
    "section": "",
    "text": "Time series analysis involves studying data points collected or recorded over time. Its applications range from economics to biology, where understanding patterns, trends, and predicting future events is crucial.\nMathematically, a time series \\(\\left\\{X_t\\right\\}\\) is a sequence of random variables indexed by time \\(t\\). A central concept is stationarity. A time series is (weakly) stationary if its statistical properties do not change over time.\nFormally, \\(\\left\\{X_t\\right\\}\\) is weakly stationary if: \\(\\mathbb{E}\\left[X_t\\right]=\\mu, \\quad \\operatorname{Var}\\left(X_t\\right)=\\sigma^2, \\quad\\) and \\(\\quad \\operatorname{Cov}\\left(X_t, X_{t+k}\\right)=\\gamma_k\\) for all \\(t\\). Stationarity is crucial for reliable modeling and forecasting, as many models assume constant mean and variance over time.",
    "crumbs": [
      "Week 1",
      "Activity 1"
    ]
  },
  {
    "objectID": "activity1.html#introduction-to-applied-time-series",
    "href": "activity1.html#introduction-to-applied-time-series",
    "title": "Activity1",
    "section": "",
    "text": "Time series analysis involves studying data points collected or recorded over time. Its applications range from economics to biology, where understanding patterns, trends, and predicting future events is crucial.\nMathematically, a time series \\(\\left\\{X_t\\right\\}\\) is a sequence of random variables indexed by time \\(t\\). A central concept is stationarity. A time series is (weakly) stationary if its statistical properties do not change over time.\nFormally, \\(\\left\\{X_t\\right\\}\\) is weakly stationary if: \\(\\mathbb{E}\\left[X_t\\right]=\\mu, \\quad \\operatorname{Var}\\left(X_t\\right)=\\sigma^2, \\quad\\) and \\(\\quad \\operatorname{Cov}\\left(X_t, X_{t+k}\\right)=\\gamma_k\\) for all \\(t\\). Stationarity is crucial for reliable modeling and forecasting, as many models assume constant mean and variance over time.",
    "crumbs": [
      "Week 1",
      "Activity 1"
    ]
  },
  {
    "objectID": "activity1.html#recap-multiple-linear-regression",
    "href": "activity1.html#recap-multiple-linear-regression",
    "title": "Activity1",
    "section": "Recap: Multiple Linear Regression",
    "text": "Recap: Multiple Linear Regression\nMultiple linear regression models a response variable \\(Y\\) as a linear function of predictors \\(X_1, X_2, \\ldots, X_p:\\)\n\\[\nY=\\beta_0+\\beta_1 X_1+\\beta_2 X_2+\\cdots+\\beta_p X_p+\\varepsilon\n\\]\nwhere \\(\\varepsilon\\) is the error term. Regression analysis serves as a foundation for understanding relationships between variables, which extends into time series when considering trends and seasonality.",
    "crumbs": [
      "Week 1",
      "Activity 1"
    ]
  },
  {
    "objectID": "activity1.html#using-tidyverts-in-the-tidyverse-ecosystem",
    "href": "activity1.html#using-tidyverts-in-the-tidyverse-ecosystem",
    "title": "Activity1",
    "section": "Using Tidyverts in the Tidyverse Ecosystem",
    "text": "Using Tidyverts in the Tidyverse Ecosystem\nThe Tidyverts collection integrates time series analysis into the tidyverse framework, allowing seamless data manipulation and visualization with familiar syntax.",
    "crumbs": [
      "Week 1",
      "Activity 1"
    ]
  },
  {
    "objectID": "activity1.html#understanding-time-series-data",
    "href": "activity1.html#understanding-time-series-data",
    "title": "Activity1",
    "section": "Understanding Time Series Data",
    "text": "Understanding Time Series Data\nTime series data for a single entity over time can be represented as a tsibble, which is a tidy data structure that inherently understands time.\n\nexample1 &lt;- tsibble(\n1   year = 2015:2019,\n2   y = c(123, 39, 78, 52, 110),\n3   index = year\n)\n\n4str(example1)\n\n\n1\n\nDefine a time index from 2015 to 2019.\n\n2\n\nCreate corresponding observations for each year.\n\n3\n\nConstruct a tsibble with year as the index.\n\n4\n\nUse str() to inspect the structure of the tsibble.\n\n\n\n\ntbl_ts [5 × 2] (S3: tbl_ts/tbl_df/tbl/data.frame)\n $ year: int [1:5] 2015 2016 2017 2018 2019\n $ y   : num [1:5] 123 39 78 52 110\n - attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ .rows: list&lt;int&gt; [1:1] \n  .. ..$ : int [1:5] 1 2 3 4 5\n  .. ..@ ptype: int(0) \n - attr(*, \"index\")= chr \"year\"\n  ..- attr(*, \"ordered\")= logi TRUE\n - attr(*, \"index2\")= chr \"year\"\n - attr(*, \"interval\")= interval [1:1] 1Y\n  ..@ .regular: logi TRUE",
    "crumbs": [
      "Week 1",
      "Activity 1"
    ]
  },
  {
    "objectID": "activity1.html#working-with-real-datasets",
    "href": "activity1.html#working-with-real-datasets",
    "title": "Activity1",
    "section": "Working with Real Datasets",
    "text": "Working with Real Datasets\n\nExample: Google Stock Data\n\n1google &lt;- read_csv(\"data/google.csv\")\ngoogle %&gt;%  \n2  as_tsibble() %&gt;%\n3  autoplot(Close) +\n4  ylab(\"USD\")\n\n\n1\n\nRead Google’s stock data from a CSV file.\n\n2\n\nConvert the data frame into a tsibble.\n\n3\n\nUse autoplot() to plot the closing prices over time.\n\n4\n\nLabel the y-axis as “USD” for clarity.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Daily closing stock prices of Google (in USD)\n\n\n\n\n\n\ngoogle %&gt;%  \n1  as_tsibble() %&gt;%\n2  autoplot(Volume)\n\n\n1\n\nConvert the Google dataset into a tsibble.\n\n2\n\nPlot the trading volume over time using autoplot().\n\n\n\n\n\n\n\n\n\n\nFigure 2: Daily trading volume of Google shares\n\n\n\n\n\n\n\nExample: Global Temperature Data\n\n1astsa::gtemp_both %&gt;%\n2  as_tsibble() %&gt;%\n3  autoplot() +\n4  labs(x = \"Year\", y = \"Global Temperature Deviation (in Celsius)\")\n\n\n1\n\nUse gtemp_both dataset containing historical temperature data.\n\n2\n\nConvert it into a tsibble format.\n\n3\n\nCreate a time series plot with autoplot().\n\n4\n\nLabel the axes to contextualize the data.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Global mean land–ocean temperature from 1850 to 2023, measured as deviations from the 1991-2020 average.",
    "crumbs": [
      "Week 1",
      "Activity 1"
    ]
  },
  {
    "objectID": "activity15.html",
    "href": "activity15.html",
    "title": "Activity15",
    "section": "",
    "text": "Retrieve Brent and WTI crude oil prices, compute daily returns, and explore a basic VAR model.\n\n# Get Brent (BZ=F) and WTI (CL=F) crude oil prices and convert to tsibble\nbrent &lt;- tq_get(\"BZ=F\", from = \"2021-01-01\", to = Sys.Date(), get = \"stock.prices\") %&gt;% \n  as_tsibble(index = date)\n\nwti &lt;- tq_get(\"CL=F\", from = \"2021-01-01\", to = Sys.Date(), get = \"stock.prices\") %&gt;% \n  as_tsibble(index = date)\n\n# Merge Brent and WTI by date and compute daily returns\noil_data &lt;- left_join(brent %&gt;% select(date, adjusted) %&gt;% rename(Brent = adjusted),\n                      wti %&gt;% select(date, adjusted) %&gt;% rename(WTI = adjusted),\n                      by = \"date\") %&gt;% \n  mutate(dBrent = difference(Brent),\n         dWTI = difference(WTI)) %&gt;% \n  drop_na() %&gt;% \n  as_tsibble(index = date)\n\n# Plot daily returns for Brent and WTI\noil_data %&gt;% \n  autoplot(vars(dBrent, dWTI)) +\n  labs(title = \"Daily Returns: Brent & WTI\", x = \"Date\", y = \"Return\")\n\n\n\n\n\n\n\n\n\n\n\nA VAR (Vector Autoregression) model captures the dynamic relationships between multiple time series. In our case, each variable (dBrent and dWTI) is modeled as a function of its own past values and those of the other.\nImpulse response functions (IRFs) trace the effect of a one-time shock to one variable (e.g., Brent returns) on the future values of another variable (e.g., WTI returns). The IRF output typically displays point estimates and confidence intervals for the response over a series of future periods.\n\n\n\n\n# Convert to tibble for VAR analysis\noil_data_df &lt;- oil_data %&gt;% \n  as_tibble() %&gt;%\n  select(dBrent, dWTI)\n\n# Select optimal lag order for the VAR model\nvar_selection &lt;- VARselect(oil_data_df, lag.max = 20, type = \"const\")\nprint(var_selection$selection)\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    18      2      1     18 \n\n\n\n\n\nHere we fit a VAR model with a chosen lag order (e.g., p = 4) and compute its impulse response function.\n\nvar_model_1 &lt;- VAR(oil_data_df, p = 4, type = \"const\")\n\n\n# Compute the IRF: show how a shock to Brent returns (dBrent) affects WTI returns (dWTI)\nirf_1 &lt;- irf(var_model_1, impulse = \"dBrent\", response = \"dWTI\", boot = TRUE)\nplot(irf_1, main = \"IRF Demo: Shock to Brent Returns and Response of WTI Returns\")\n\n\n\n\n\n\n\n\nIn the IRF plot, we see:\n\nThe horizontal axis representing the number of periods after the shock.\nThe vertical axis showing the estimated change in WTI returns due to a unit shock in Brent returns.\nA solid line for the point estimates and dashed lines indicating the confidence intervals.\n\nThis output helps visualize both the magnitude and duration of the shock’s impact.\n\n\n\nTask: Using the oil_data_df, fit VAR models with different lag orders (e.g., VAR(2) and VAR(3)) and analyze the impulse response functions (IRFs) to study how shocks in Brent returns affect WTI returns.\n\n# VAR model with lag order 2\nvar_model_2 &lt;- VAR(oil_data_df, p = 2, type = \"const\")\nirf_2 &lt;- irf(var_model_2, impulse = \"dBrent\", response = \"dWTI\", boot = TRUE)\nplot(irf_2, main = \"IRF: VAR(2) Model (Brent -&gt; WTI)\")\n\n\n\n\n\n\n\n# VAR model with lag order 3\nvar_model_3 &lt;- VAR(oil_data_df, p = 3, type = \"const\")\nirf_3 &lt;- irf(var_model_3, impulse = \"dBrent\", response = \"dWTI\", boot = TRUE)\nplot(irf_3, main = \"IRF: VAR(3) Model (Brent -&gt; WTI)\")",
    "crumbs": [
      "Week 4",
      "Activity 15"
    ]
  },
  {
    "objectID": "activity15.html#oil-prices-analysis-brent-wti-impulse-response-function",
    "href": "activity15.html#oil-prices-analysis-brent-wti-impulse-response-function",
    "title": "Activity15",
    "section": "",
    "text": "Retrieve Brent and WTI crude oil prices, compute daily returns, and explore a basic VAR model.\n\n# Get Brent (BZ=F) and WTI (CL=F) crude oil prices and convert to tsibble\nbrent &lt;- tq_get(\"BZ=F\", from = \"2021-01-01\", to = Sys.Date(), get = \"stock.prices\") %&gt;% \n  as_tsibble(index = date)\n\nwti &lt;- tq_get(\"CL=F\", from = \"2021-01-01\", to = Sys.Date(), get = \"stock.prices\") %&gt;% \n  as_tsibble(index = date)\n\n# Merge Brent and WTI by date and compute daily returns\noil_data &lt;- left_join(brent %&gt;% select(date, adjusted) %&gt;% rename(Brent = adjusted),\n                      wti %&gt;% select(date, adjusted) %&gt;% rename(WTI = adjusted),\n                      by = \"date\") %&gt;% \n  mutate(dBrent = difference(Brent),\n         dWTI = difference(WTI)) %&gt;% \n  drop_na() %&gt;% \n  as_tsibble(index = date)\n\n# Plot daily returns for Brent and WTI\noil_data %&gt;% \n  autoplot(vars(dBrent, dWTI)) +\n  labs(title = \"Daily Returns: Brent & WTI\", x = \"Date\", y = \"Return\")\n\n\n\n\n\n\n\n\n\n\n\nA VAR (Vector Autoregression) model captures the dynamic relationships between multiple time series. In our case, each variable (dBrent and dWTI) is modeled as a function of its own past values and those of the other.\nImpulse response functions (IRFs) trace the effect of a one-time shock to one variable (e.g., Brent returns) on the future values of another variable (e.g., WTI returns). The IRF output typically displays point estimates and confidence intervals for the response over a series of future periods.\n\n\n\n\n# Convert to tibble for VAR analysis\noil_data_df &lt;- oil_data %&gt;% \n  as_tibble() %&gt;%\n  select(dBrent, dWTI)\n\n# Select optimal lag order for the VAR model\nvar_selection &lt;- VARselect(oil_data_df, lag.max = 20, type = \"const\")\nprint(var_selection$selection)\n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    18      2      1     18 \n\n\n\n\n\nHere we fit a VAR model with a chosen lag order (e.g., p = 4) and compute its impulse response function.\n\nvar_model_1 &lt;- VAR(oil_data_df, p = 4, type = \"const\")\n\n\n# Compute the IRF: show how a shock to Brent returns (dBrent) affects WTI returns (dWTI)\nirf_1 &lt;- irf(var_model_1, impulse = \"dBrent\", response = \"dWTI\", boot = TRUE)\nplot(irf_1, main = \"IRF Demo: Shock to Brent Returns and Response of WTI Returns\")\n\n\n\n\n\n\n\n\nIn the IRF plot, we see:\n\nThe horizontal axis representing the number of periods after the shock.\nThe vertical axis showing the estimated change in WTI returns due to a unit shock in Brent returns.\nA solid line for the point estimates and dashed lines indicating the confidence intervals.\n\nThis output helps visualize both the magnitude and duration of the shock’s impact.\n\n\n\nTask: Using the oil_data_df, fit VAR models with different lag orders (e.g., VAR(2) and VAR(3)) and analyze the impulse response functions (IRFs) to study how shocks in Brent returns affect WTI returns.\n\n# VAR model with lag order 2\nvar_model_2 &lt;- VAR(oil_data_df, p = 2, type = \"const\")\nirf_2 &lt;- irf(var_model_2, impulse = \"dBrent\", response = \"dWTI\", boot = TRUE)\nplot(irf_2, main = \"IRF: VAR(2) Model (Brent -&gt; WTI)\")\n\n\n\n\n\n\n\n# VAR model with lag order 3\nvar_model_3 &lt;- VAR(oil_data_df, p = 3, type = \"const\")\nirf_3 &lt;- irf(var_model_3, impulse = \"dBrent\", response = \"dWTI\", boot = TRUE)\nplot(irf_3, main = \"IRF: VAR(3) Model (Brent -&gt; WTI)\")",
    "crumbs": [
      "Week 4",
      "Activity 15"
    ]
  },
  {
    "objectID": "activity28.html",
    "href": "activity28.html",
    "title": "Activity28",
    "section": "",
    "text": "The us_change dataset from provides percentage changes in quarterly personal consumption expenditure, personal disposable income, production, savings, and the unemployment rate in the United States from 1960 to 2016.\ndata(us_change)\nconsumption_ts &lt;- us_change %&gt;%\n  dplyr::select(Quarter, Consumption) %&gt;%\n  as_tsibble(index = Quarter)",
    "crumbs": [
      "Week 7",
      "Activity 28"
    ]
  },
  {
    "objectID": "activity28.html#arima-model-selection-strategy",
    "href": "activity28.html#arima-model-selection-strategy",
    "title": "Activity28",
    "section": "ARIMA Model Selection Strategy:",
    "text": "ARIMA Model Selection Strategy:\nThe grid search explores non-seasonal \\((p,d,q)\\) and seasonal \\((P,D,Q)_m\\) components, testing both integration orders (\\(d\\)) and seasonal differencing (\\(D\\)). Using AICc for selection balances fit and complexity, favoring the ARIMA(2,1,2)(0,0,1)[4] model with \\(\\text{AICc}= -540.9\\). Seasonal PDQ terms use \\(m=4\\) (quarterly data), but only a seasonal MA(1) component remains significant.\n\n# --------------------------\n# 1. Modeling \n# --------------------------\n\n# Create model grid of candidate orders\ngrid &lt;- expand_grid(\n  non_seasonal = list(c(0,1,1), c(1,1,0), c(2,1,2)),\n  seasonal     = list(c(0,0,0), c(1,0,0,4), c(0,0,1,4))\n)\n\n# Define a safe ARIMA function that builds the model from candidate orders\nsafe_arima &lt;- possibly(function(ts, ns, s) {\n  ts %&gt;% \n    model(ARIMA(Consumption ~ pdq(ns[1], ns[2], ns[3]) + PDQ(s[1], s[2], s[3], period = s[4])))\n}, otherwise = NULL)\n\n# Fit the candidate models and collect the results as a mable\nresults &lt;- grid %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    model   = list(safe_arima(consumption_ts, non_seasonal, seasonal)),\n    ns_str  = paste(non_seasonal, collapse = \",\"),\n    s_str   = paste(seasonal, collapse = \",\")\n  ) %&gt;% \n  ungroup() %&gt;% \n  filter(!map_lgl(model, is.null)) %&gt;% \n  mutate(info = map(model, ~ glance(.x))) %&gt;% \n  unnest(info)\n\n# Compare models by AICc \nbest_model &lt;- results %&gt;% arrange(AICc) %&gt;% slice(1)\nbest_model \n\n# A tibble: 1 × 13\n  non_seasonal seasonal  model    ns_str s_str .model sigma2 log_lik   AIC  AICc\n  &lt;list&gt;       &lt;list&gt;    &lt;list&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 &lt;dbl [3]&gt;    &lt;dbl [4]&gt; &lt;mdl_df&gt; 0,1,1  0,0,… \"ARIM…  0.369   -181.  367.  367.\n# ℹ 3 more variables: BIC &lt;dbl&gt;, ar_roots &lt;list&gt;, ma_roots &lt;list&gt;",
    "crumbs": [
      "Week 7",
      "Activity 28"
    ]
  },
  {
    "objectID": "activity28.html#residual-diagnostics",
    "href": "activity28.html#residual-diagnostics",
    "title": "Activity28",
    "section": "Residual Diagnostics:",
    "text": "Residual Diagnostics:\nThe Ljung-Box test (\\(p=0.245\\)) confirms white noise residuals, while the ARCH test (\\(p=0.56\\)) shows no volatility clustering. The seasonal plot reveals no systematic seasonal patterns in residuals, supporting model adequacy.\n\n# --------------------------\n# 2. Diagnostics\n# --------------------------\n\nbest_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]] %&gt;% \n  residuals() %&gt;% \n  features(.resid, ~ljung_box(.x, lag = 20))\n\n# A tibble: 1 × 3\n  .model                                                       lb_stat lb_pvalue\n  &lt;chr&gt;                                                          &lt;dbl&gt;     &lt;dbl&gt;\n1 \"ARIMA(Consumption ~ pdq(ns[1], ns[2], ns[3]) + PDQ(s[1], s…    23.9     0.245\n\nbest_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]] %&gt;% \n  residuals() %&gt;% \n  gg_tsdisplay(.resid, plot_type = \"season\") +\n  labs(title = \"Residual Diagnostics with Seasonal Analysis\")\n\n\n\n\n\n\n\nbest_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]] %&gt;% \n  residuals() %&gt;% \n  features(.resid^2, list(arch_test = ~ FinTS::ArchTest(.x, lags = 5)$p.value))\n\n# A tibble: 1 × 2\n  .model                                                  arch_test_Chi-square…¹\n  &lt;chr&gt;                                                                    &lt;dbl&gt;\n1 \"ARIMA(Consumption ~ pdq(ns[1], ns[2], ns[3]) + PDQ(s[…                  0.560\n# ℹ abbreviated name: ¹​`arch_test_Chi-squared`",
    "crumbs": [
      "Week 7",
      "Activity 28"
    ]
  },
  {
    "objectID": "activity28.html#forecast-uncertainty",
    "href": "activity28.html#forecast-uncertainty",
    "title": "Activity28",
    "section": "Forecast Uncertainty:",
    "text": "Forecast Uncertainty:\nThe fan chart shows widening prediction intervals (\\(\\pm2\\sigma\\)), reflecting increased uncertainty for longer horizons - a hallmark of Box-Jenkins forecasts. The stable trend projection suggests the model captured the consumption series’ momentum.\n\n# --------------------------\n# 3. Forecasting\n# --------------------------\n\n# Generate forecasts for all viable models\nensemble_forecast &lt;- best_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]]  %&gt;% \n  forecast(h = 8) \n\n# Visualize forecast distributions\nensemble_forecast %&gt;% \n  autoplot() +\n  autolayer(consumption_ts, Consumption) +\n  labs(title = \"SARIMA Forecasts\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nLab Activity: Beer Production Analysis\nApply the same methodology to aus_production (quarterly Australian beer production):\n\n# 1. Prepare data\nbeer_ts &lt;- aus_production %&gt;%\n  dplyr::select(Quarter, Beer) %&gt;%\n  as_tsibble(index = Quarter)\n\n# 2. Modify model grid (adapt seasonal PDQ orders)\ngrid_beer &lt;- expand_grid(\n  non_seasonal = list(c(1,1,1), c(0,1,2), c(2,1,2)),\n   seasonal = list(\n    c(0,0,0,4),        # No seasonal terms\n    c(1,1,0,4),        # Seasonal AR(1) + differencing\n    c(0,1,1,4)         # Seasonal MA(1) + differencing\n  )\n)\n\ngrid_beer %&gt;% knitr::kable()\n\n\n\n\nnon_seasonal\nseasonal\n\n\n\n\n1, 1, 1\n0, 0, 0, 4\n\n\n1, 1, 1\n1, 1, 0, 4\n\n\n1, 1, 1\n0, 1, 1, 4\n\n\n0, 1, 2\n0, 0, 0, 4\n\n\n0, 1, 2\n1, 1, 0, 4\n\n\n0, 1, 2\n0, 1, 1, 4\n\n\n2, 1, 2\n0, 0, 0, 4\n\n\n2, 1, 2\n1, 1, 0, 4\n\n\n2, 1, 2\n0, 1, 1, 4\n\n\n\n\n\nKey differences to assess:\n\nDoes beer data require stronger seasonal differencing (D=1)?\nCompare AICc values for models with/without seasonal terms\nCheck if residuals show remaining production cycle patterns\n\n\n# Generic-safe ARIMA function\nsafe_arima &lt;- possibly(function(ts, ns, s) {\n  ts %&gt;% \n    model(ARIMA(Beer ~ pdq(ns[1], ns[2], ns[3]) +  \n                PDQ(s[1], s[2], s[3], period = s[4])))\n}, otherwise = NULL) \n\n\n# Fit the candidate models and collect the results as a mable\nresults_beer &lt;- grid_beer %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    model   = list(safe_arima(beer_ts, non_seasonal, seasonal)),\n    ns_str  = paste(non_seasonal, collapse = \",\"),\n    s_str   = paste(seasonal, collapse = \",\")\n  ) %&gt;% \n  ungroup() %&gt;% \n  filter(!map_lgl(model, is.null)) %&gt;% \n  mutate(info = map(model, ~ glance(.x))) %&gt;% \n  unnest(info)\n\n# Compare models by AICc \nbest_model &lt;- results_beer %&gt;% arrange(AICc) %&gt;% slice(1)\nbest_model \n\n# A tibble: 1 × 13\n  non_seasonal seasonal  model    ns_str s_str .model sigma2 log_lik   AIC  AICc\n  &lt;list&gt;       &lt;list&gt;    &lt;list&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 &lt;dbl [3]&gt;    &lt;dbl [4]&gt; &lt;mdl_df&gt; 0,1,2  0,1,… \"ARIM…   240.   -886. 1781. 1781.\n# ℹ 3 more variables: BIC &lt;dbl&gt;, ar_roots &lt;list&gt;, ma_roots &lt;list&gt;\n\nresults_beer %&gt;% \n  dplyr::select(ns_str, s_str, AICc) %&gt;% \n  arrange(AICc)\n\n# A tibble: 9 × 3\n  ns_str s_str    AICc\n  &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n1 0,1,2  0,1,1,4 1781.\n2 1,1,1  0,1,1,4 1785.\n3 2,1,2  0,1,1,4 1785.\n4 2,1,2  1,1,0,4 1817.\n5 1,1,1  1,1,0,4 1824.\n6 0,1,2  1,1,0,4 1825.\n7 2,1,2  0,0,0,4 2061.\n8 0,1,2  0,0,0,4 2261.\n9 1,1,1  0,0,0,4 2327.\n\n\n\n# --------------------------\n# 2. Diagnostics\n# --------------------------\n\nbest_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]] %&gt;% \n  residuals() %&gt;% \n  features(.resid, ~ljung_box(.x, lag = 20))\n\n# A tibble: 1 × 3\n  .model                                                       lb_stat lb_pvalue\n  &lt;chr&gt;                                                          &lt;dbl&gt;     &lt;dbl&gt;\n1 \"ARIMA(Beer ~ pdq(ns[1], ns[2], ns[3]) + PDQ(s[1], s[2], s[…    14.9     0.780\n\nbest_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]] %&gt;% \n  residuals() %&gt;% \n  gg_tsdisplay(.resid, plot_type = \"season\") +\n  labs(title = \"Residual Diagnostics with Seasonal Analysis\")\n\n\n\n\n\n\n\nbest_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]] %&gt;% \n  residuals() %&gt;% \n  features(.resid^2, list(arch_test = ~ FinTS::ArchTest(.x, lags = 5)$p.value))\n\n# A tibble: 1 × 2\n  .model                                                  arch_test_Chi-square…¹\n  &lt;chr&gt;                                                                    &lt;dbl&gt;\n1 \"ARIMA(Beer ~ pdq(ns[1], ns[2], ns[3]) + PDQ(s[1], s[2…                  0.866\n# ℹ abbreviated name: ¹​`arch_test_Chi-squared`\n\n\n\nbest_model %&gt;% \n  pull(model) %&gt;% \n  .[[1]] %&gt;% \n  forecast(h=12) %&gt;% \n  autoplot(beer_ts) +\n  labs(title = \"Beer Production Forecast with Seasonal Differencing\")",
    "crumbs": [
      "Week 7",
      "Activity 28"
    ]
  }
]